{
  
    
        "post0": {
            "title": "[Python Review] Part V: Packages",
            "content": "Packages . Reference . https://dabeaz-course.github.io/practical-python/Notes | .",
            "url": "http://blog.logancyang.com/note/python/2020/07/05/python-basics-v.html",
            "relUrl": "/note/python/2020/07/05/python-basics-v.html",
            "date": " • Jul 5, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "[Python Review] Part IV: Testing, Logging and Debugging",
            "content": "Testing . The dynamic nature of Python makes testing critically important to most applications. There is no compiler to find your bugs. The only way to find bugs is to run the code and make sure you try out all of its features. . Assertions . The assert statement is an internal check for the program. If an expression is not true, it raises a AssertionError exception. . assert statement syntax: . assert &lt;expression&gt; [, &#39;Diagnostic message&#39;] . For example, . assert isinstance(10, int), &#39;Expected int&#39; . (BTW, the syntax of nested [, arg] in documentation [, arg1 [, arg2 [...]]] is the standard Python way of documenting function signatures. Check this post for why.) . It shouldn’t be used to check the user-input (i.e., data entered on a web form or something). It’s purpose is more for internal checks and invariants (conditions that should always be true). . Contract programming . Also known as Design By Contract, liberal use of assertions is an approach for designing software. It prescribes that software designers should define precise interface specifications for the components of the software. . For example, you might put assertions on all inputs of a function. . def add(x, y): assert isinstance(x, int), &#39;Expected int&#39; assert isinstance(y, int), &#39;Expected int&#39; return x + y &gt;&gt;&gt; add(2, 3) 5 &gt;&gt;&gt; add(&#39;2&#39;, &#39;3&#39;) Traceback (most recent call last): ... AssertionError: Expected int . Checking inputs will immediately catch callers who aren’t using appropriate arguments. . Inline tests . Assertions can also be used for simple tests. . def add(x, y): return x + y assert add(2,2) == 4 . This way you are including the test in the same module as your code. Benefit: If the code is obviously broken, attempts to import the module will crash. . This is not recommended for exhaustive testing. It’s more of a basic “smoke test”. Does the function work on any example at all? If not, then something is definitely broken. . unittest module . Suppose you have a code file simple.py, and you write test_simple.py to test it: . # simple.py def add(x, y): return x + y # test_simple.py import simple import unittest # Notice that it inherits from unittest.TestCase class TestAdd(unittest.TestCase): def test_simple(self): # Test with simple integer arguments r = simple.add(2, 2) self.assertEqual(r, 5) def test_str(self): # Test with strings r = simple.add(&#39;hello&#39;, &#39;world&#39;) self.assertEqual(r, &#39;helloworld&#39;) . The testing class must inherit from unittest.TestCase. . In the testing class, you can define the testing methods. . Important: Each method must start with test. . Using unittest . There are several built in assertions that come with unittest. Each of them asserts a different thing. . # Assert that expr is True self.assertTrue(expr) # Assert that x == y self.assertEqual(x,y) # Assert that x != y self.assertNotEqual(x,y) # Assert that x is near y self.assertAlmostEqual(x,y,places) # Assert that callable(arg1,arg2,...) raises exc self.assertRaises(exc, callable, arg1, arg2, ...) . This is not an exhaustive list. There are other assertions in the module. . Running unittest . To run the tests, turn the code into a script. . # test_simple.py ... if __name__ == &#39;__main__&#39;: unittest.main() . Then run Python on the test file. . bash % python3 test_simple.py F. ======================================================== FAIL: test_simple (__main__.TestAdd) -- Traceback (most recent call last): File &quot;testsimple.py&quot;, line 8, in test_simple self.assertEqual(r, 5) AssertionError: 4 != 5 -- Ran 2 tests in 0.000s FAILED (failures=1) . When you run a test that checks the type of something, you need to check that an exception is raised: . class TestStock(unittest.TestCase): ... def test_bad_shares(self): s = stock.Stock(&#39;GOOG&#39;, 100, 490.1) with self.assertRaises(TypeError): s.shares = &#39;100&#39; . Effective unit testing is an art and it can grow to be quite complicated for large applications. . The unittest module has a huge number of options related to test runners, collection of results and other aspects of testing. Consult the documentation for details. . Third-party test tools: pytest . The built-in unittest module has the advantage of being available everywhere–it’s part of Python. However, many programmers also find it to be quite verbose. A popular alternative is pytest. With pytest, your testing file simplifies to something like the following: . # test_simple.py import simple def test_simple(): assert simple.add(2,2) == 4 def test_str(): assert simple.add(&#39;hello&#39;,&#39;world&#39;) == &#39;helloworld&#39; . To run the tests, you simply type a command such as python -m pytest. It will discover all of the tests and run them. . There’s a lot more to pytest than this example, but it’s usually pretty easy to get started should you decide to try it out. . Logging . The logging module is a standard library module for recording diagnostic information. It’s also a very large module with a lot of sophisticated functionality. We will show a simple example to illustrate its usefulness. . Suppose we have: . # fileparse.py def parse(f, types=None, names=None, delimiter=None): records = [] for line in f: line = line.strip() if not line: continue try: records.append(split(line,types,names,delimiter)) except ValueError as e: print(&quot;Couldn&#39;t parse :&quot;, line) print(&quot;Reason :&quot;, e) return records . We shouldn’t print the message when there’s an exception. Answer from stackoverflow: . The logging package has a lot of useful features: . Easy to see where and when (even what line no.) a logging call is being made from. | You can log to files, sockets, pretty much anything, all at the same time. | You can differentiate your logging based on severity. | . Print doesn’t have any of these. . Also, if your project is meant to be imported by other python tools, it’s bad practice for your package to print things to stdout, since the user likely won’t know where the print messages are coming from. With logging, users of your package can choose whether or not they want to propogate logging messages from your tool or not. . To use logging here: . # fileparse.py import logging # CREATE THE LOGGER OBJECT log = logging.getLogger(__name__) def parse(f,types=None,names=None,delimiter=None): ... try: records.append(split(line,types,names,delimiter)) except ValueError as e: log.warning(&quot;Couldn&#39;t parse : %s&quot;, line) log.debug(&quot;Reason : %s&quot;, e) . The code is modified to issue warning messages via a Logger object. The one created with logging.getLogger(__name__). . Logging basics . Create a logger object: . logger = logging.getLogger(name) # name is a string . Issuing log messages. . logger.critical(message [, args]) logger.error(message [, args]) logger.warning(message [, args]) logger.info(message [, args]) logger.debug(message [, args]) . Each method represents a different level of severity. . All of them create a formatted log message. args is used with the % operator to create the message. . logmsg = message % args # Written to the log . Logging configuration . Typically, this is a one-time configuration at program startup. The configuration is separate from the code that makes the logging calls. . # main.py ... if __name__ == &#39;__main__&#39;: import logging logging.basicConfig( filename = &#39;app.log&#39;, # Log output file level = logging.INFO, # Output level ) . Logging is highly configurable. You can adjust every aspect of it: output files, levels, message formats, etc. However, the code that uses logging doesn’t have to worry about that. . # Set level to DEBUG &gt;&gt;&gt; logging.getLogger(&#39;fileparse&#39;).level = logging.DEBUG &gt;&gt;&gt; a = report.read_portfolio(&#39;Data/missing.csv&#39;) WARNING:fileparse:Row 4: Bad row: [&#39;MSFT&#39;, &#39;&#39;, &#39;51.23&#39;] DEBUG:fileparse:Row 4: Reason: invalid literal for int() with base 10: &#39;&#39; WARNING:fileparse:Row 7: Bad row: [&#39;IBM&#39;, &#39;&#39;, &#39;70.44&#39;] DEBUG:fileparse:Row 7: Reason: invalid literal for int() with base 10: &#39;&#39; &gt;&gt;&gt; # Turn it off, set level to CRITICAL &gt;&gt;&gt; logging.getLogger(&#39;fileparse&#39;).level=logging.CRITICAL &gt;&gt;&gt; a = report.read_portfolio(&#39;Data/missing.csv&#39;) &gt;&gt;&gt; . Adding logging to a program . To add logging to an application, you need to have some mechanism to initialize the logging module in the main module. One way to do this is to include some setup code that looks like this: . # This file sets up basic configuration of the logging module. # Change settings here to adjust logging output as needed. import logging logging.basicConfig( # Name of the log file (omit to use stderr) filename = &#39;app.log&#39;, # File mode (use &#39;a&#39; to append) filemode = &#39;w&#39;, # Logging level (DEBUG, INFO, WARNING, ERROR, or CRITICAL) level = logging.WARNING, ) . You’d need to put this someplace in the startup steps of your program. . Debugging . So, your program has crashed… Now what? Read the tracebacks! . bash % python3 blah.py Traceback (most recent call last): File &quot;blah.py&quot;, line 13, in ? foo() File &quot;blah.py&quot;, line 10, in foo bar() File &quot;blah.py&quot;, line 7, in bar spam() File &quot;blah.py&quot;, 4, in spam line x.append(3) # Cause of the crash AttributeError: &#39;int&#39; object has no attribute &#39;append&#39; . PRO TIP: Paste the whole traceback to Google! . You can use -i to keep Python alive to Python REPL when executing in the shell: . bash % python3 -i blah.py Traceback (most recent call last): File &quot;blah.py&quot;, line 13, in ? foo() File &quot;blah.py&quot;, line 10, in foo bar() File &quot;blah.py&quot;, line 7, in bar spam() File &quot;blah.py&quot;, 4, in spam line x.append(3) AttributeError: &#39;int&#39; object has no attribute &#39;append&#39; &gt;&gt;&gt; . It preserves the interpreter state. That means that you can go poking around after the crash. Checking variable values and other state. . Debugging with print() . Tip: use repr(). It shows details. . def spam(x): print(&#39;DEBUG:&#39;, repr(x)) ... &gt;&gt;&gt; from decimal import Decimal &gt;&gt;&gt; x = Decimal(&#39;3.4&#39;) # NO `repr` &gt;&gt;&gt; print(x) 3.4 # WITH `repr` &gt;&gt;&gt; print(repr(x)) Decimal(&#39;3.4&#39;) &gt;&gt;&gt; . Debugging with the Python debugger pdb . You can manually launch the debugger inside a program. . def some_function(): ... breakpoint() # Enter the debugger (Python 3.7+) ... . This starts the debugger at the breakpoint() call. . In earlier Python versions, you did this, . import pdb ... pdb.set_trace() # Instead of `breakpoint()` ... . You can also run the entire program under debugger: . bash % python3 -m pdb someprogram.py . It will automatically enter the debugger before the first statement. Allowing you to set breakpoints and change the configuration. . Common debugger commands: . (Pdb) help # Get help (Pdb) w(here) # Print stack trace (Pdb) d(own) # Move down one stack level (Pdb) u(p) # Move up one stack level (Pdb) b(reak) loc # Set a breakpoint (Pdb) s(tep) # Execute one instruction (Pdb) c(ontinue) # Continue execution (Pdb) l(ist) # List source code (Pdb) a(rgs) # Print args of current function (Pdb) !statement # Execute statement . Example for setting breakpoints: . (Pdb) b 45 # Line 45 in current file (Pdb) b file.py:45 # Line 34 in file.py (Pdb) b foo # Function foo() in current file (Pdb) b module.foo # Function foo() in a module . Develop using Jupyter Notebook and nbdev . Using Jupyter Notebook and nbdev by fast.ai is good for efficient debugging. Jupyter Notebook is a great tool for experimenting with code. . Debugging in VSCode . You can create a py file and use # %% to specify a cell which behaves like a Jupyter Notebook cell. Click at the beginning of any line in the source code to set a breakpoint, and run the cell with Debug cell. . . Reference . https://dabeaz-course.github.io/practical-python/Notes | https://stackoverflow.com/questions/2120507/why-do-python-function-docs-include-the-comma-after-the-bracket-for-optional-arg | https://stackoverflow.com/questions/6918493/in-python-why-use-logging-instead-of-print | .",
            "url": "http://blog.logancyang.com/note/python/2020/07/04/python-basics-iv.html",
            "relUrl": "/note/python/2020/07/04/python-basics-iv.html",
            "date": " • Jul 4, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "[From NAND to TETRIS] Computer Architecture 101 Part III: Computer Architecture and Assembler",
            "content": "Continuing the discussion through abstraction level 4 in the last post, we look at level 5 and level 6 in this post. . Level 5: Computer Architecture . We are going to build the Von Neumann Architecture that is capable of executing any program. . . Von Neumann Architecture . . The CPU has ALUs and registers. The ALUs do the arithmetic and logical operations. . The memory holds the data and the program. . To move data around, there are 3 kinds of buses: the data bus, address bus, and control bus (or control wires). . The ALU loads information from the Data bus and manipulates it using the Control bits. . We need a two-way connection between the CPU registers and the data bus for intermediate results. We also need to connect CPU registers to the address bus (one way) because we access the memory by the memory address in the registers. So the CPU registers have both data and addresses. . For the memory, we need to connect the data memory to the data bus, and the program memory to the control bus. . The Fetch-Execute Cycle . All that the CPU does is fetch an instruction and execute it, and repeat. This is the CPU loop. . Fetch: . put the location of the next instruction (available from the program counter) into the address of the program memory | and then get the instruction code itself by reading the memory content at that location. | . . In the execute cycle, we use the bits from the control bus and access the registers and/or the data memory to manipulate the data. . There is clash between fetch and execute because they access the program and data memory which are in the same physical memory. Usually, we use a multiplexor to do the fetch cycle first and remember the content of the instruction in the register, and then access the data memory. There’s a variant of the Von Neumann Architecture which is the Harvard Architecture. It physically separate the program and data memory to avoid this complication. . The Hack CPU . The hack CPU is the brain of the computer, it decides which instruction to execute next and how. . It has 3 inputs: . Data value | Instruction | Reset bit | . And 4 outputs: . Data value | Write to memory (Y/N) | Memory address | Address of the next instruction | . Here’s the Hack CPU architecture: . . All the chips used are the ones we have built before. We just need to assemble them together in this way. The C bits are control bits. . Here is how the CPU recognizes and handles an A-instruction: . . Here is how the CPU recognizes and handles a C-instruction: . . Looking at ALU: . . . Reset bit: program starts/restarts when reset is 1. . Here is the control implementation and program counter logic: . . . Data memory . Recall the overall Hack architecture: . . Now that we have the CPU implementation, we look at the memory. It has 3 parts: the RAM part that is the data memory, plus the Screen Memory Map, and the kernel memory map. . . The fist part, RAM16, is already implemented in our previous sections. . We also went through Screen Memory Map and Keyboard Memory Map in detail before. One special feature of the Screen chip is that it can refresh the display at a frequency. . ROM32K: Instruction memory . . To put a program into ROM, there are 2 ways . Burn the program onto a ROM chip, such as a CD, and run the program with the CD. | Load the program which is written in a text file using hardware simulation by the built-in ROM chip. | . Putting it all together . . Comments . We can have a chip that tracks the state the computer is in to make sure it performs the right set of instructions given its state. We use the concept of Finite State Machine to organize the states and their transitions. | To add more peripheral input/output devices, we can allocate more segments in the memory for them. | . Level 6. Assembler . For the first time in this series, this part is software. The Assembler translates the machine language to binary code that can be executed. It is the first software layer above the hardware. . But where do we run the Assembler if it is not already in 0s and 1s itself? . Since we are not building the first computer in the world, we can write the Assembler on another computer, compile it down to binary, transfer the binary to the Hack computer we are building, and use the binary directly. . . The Assembler reads a line of Hack code into an character array. It ignores comments and whitespaces when breaking down the line into fields. . The Assember also supports symbols (variable names). The way it supports that is to store the symbol name at an address. . . Essentially, a symbol is just an alias for an address location. . It is possible in the Assembly (Hack) machine language that a “jump” symbol is used before it is declared using the @ operator. One solution is to do 2 passes for the code, the first pass builds the symbol table, and the second pass uses it to correctly executes the program. . . The translator perspective . We have already seen the tables that map A and C instructions to binary code. Along with user symbols that are constructed by the declarations, we have all we need to do the translation. . We take care of . White spaces, empty line, indentation | Comments, inline comments | A, C instructions | Symbols | . Step 1: Handling whitespaces . Let’s focus on them one by one. First we handle whitespaces for a simplified Hack program with no symbols. . This is the easiest one. We simply strip all whitespaces, so the program after this step has no whitespace. . Step 2: Translating A-instructions . An A-instruction is just @value. In binary, we prepend the op code 0 for A-instruction, and convert the value in decimal to binary, add 0 paddings accordingly. . . Step 3: Translating C-instructions . Consider MD=D+1. This C-instruction has a dest: MD and comp: D+1, with no jump field. . The binary syntax for C-instructions is . 1 1 1 a c1 c2 c3 c4 c5 c6 d1 d2 d3 j1 j2 j3 . 3 1s at the beginning, and fill in the a, c, d, j bits according to the table below . . Step 4: Handling symbols . . For pre-defined symbols, we can simply replace them with their values based on the table below . . For the label symbols, they are used for jump commands. . The program uses the label to “goto” another instruction | Declared with () | A label symbol points to the memory location of its next instruction. label -&gt; mem_index_of_next_instruction | . When translating a label symbol, simply replace it with the value mem_index_of_next_instruction. . For the variable symbols, they are assigned to memory locations starting from address 16 (a designer’s decision). . We need a symbol table to keep track of the new and old variable symbols. It has key-value pairs of symbol name and symbol address. . The symbol table is initialized with all the predefined symbols and their addresses. . The Assembler performs 2 passes. In the first pass adds the label symbols to the symbol table. The second pass adds the variable symbols to the symbol table. . The symbol table is an auxilary data structure that the Assembler needs to process the program. Once the processing completes, the symbol table can be tossed away. . Overall Assembler logic . Keep in mind, the Assembler is just a text processor that takes in the Assembly/Hack language and returns a file containing only two types of characters, 0 and 1. These 0s and 1s are still ASCII characters. When the computer loads it into memory it then becomes real 0s and 1s. . Although we say we are not building the world’s first computer and resort to a high level language to write this Assembler text processor, all that matters is the resulting translation directive text file. One can write it from scratch with hardware logic, but it’s cumbersome. It’s not that we are not able to produce this text processor without the help of a high level language. We already have all the logic we need. . . The overall Assembler process: . . To implement the Assembler in a high level language like Python or Java, here’s the pseudocode: . . Macro assembler and macro commands . We can make the Hack language more user friendly by adding macro commands. For example, we can convert . @100 D=M . to . D=M[100] . To achieve this, simply add the translation logic into the Assembler. . How was the first Assembler created in history without the assistance of high level languages? . The first Assembler was built by hand. People write the binary code for the translation process. However, we only needed to do it once. Once it’s created, people can build on it and create new Assemblers by programming. . From NAND to TETRIS Part I: Last words . Now we have built the entire Hack computer from NAND gates! . Outside the classroom, it is rare that people need to work with the Assemly language. People only work with high level languages in programming unless they really need to optimize a piece of code for a specialized device. . The most important concept introduced here is that why computers are programmable and can do all kinds of tasks. . We bridged the gap from hardware to software. This is the key idea we learned! . In From NAND to TETRIS Part II, the material will cover the software layers such as the virtual machine, the compiler, operating system, etc. . Reference . Coursera From NAND To TETRIS | The amazing lecturers: Shimon Schocken | Noam Nisan | . | .",
            "url": "http://blog.logancyang.com/note/compsci/2020/07/04/computer-architecture-101-part-iii.html",
            "relUrl": "/note/compsci/2020/07/04/computer-architecture-101-part-iii.html",
            "date": " • Jul 4, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "[Python Review] Part III: Generators and Advanced Topics",
            "content": "Generators . Iteration protocol . Many objects support iteration: . a = &#39;hello&#39; for c in a: # Loop over characters in a ... b = { &#39;name&#39;: &#39;Dave&#39;, &#39;password&#39;:&#39;foo&#39;} for k in b: # Loop over keys in dictionary ... c = [1,2,3,4] for i in c: # Loop over items in a list/tuple ... f = open(&#39;foo.txt&#39;) for x in f: # Loop over lines in a file ... . What happens under the hood of a for loop? . for x in obj: ... # Is equivalent to _iter = obj.__iter__() # Get iterator object while True: try: x = _iter.__next__() # Get next item except StopIteration: # No more items break # statements ... . All objects that support for loop implement this low level iteration protocol. This is a manual iteration through a list: . &gt;&gt;&gt; x = [1,2,3] &gt;&gt;&gt; it = x.__iter__() &gt;&gt;&gt; it &lt;listiterator object at 0x590b0&gt; &gt;&gt;&gt; it.__next__() 1 &gt;&gt;&gt; it.__next__() 2 &gt;&gt;&gt; it.__next__() 3 &gt;&gt;&gt; it.__next__() Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in ? StopIteration . Support iteration in your custom object . Knowing about iteration is useful if you want to add it to your own objects. For example, making a custom container. . class Portfolio: def __init__(self): self.holdings = [] def __iter__(self): return self.holdings.__iter__() ... port = Portfolio() for s in port: ... . For container objects, supporting iteration, indexing, containment, and other kinds of operators is an important part of being Pythonic. . Side note: __contains__() is a function for the in check, for example: . def __contains__(self, name): return any([s.name == name for s in self._holdings]) . next() built-in function . The next() built-in function is a shortcut for calling the __next__() method of an iterator. Try using it on a file: . &gt;&gt;&gt; f = open(&#39;Data/portfolio.csv&#39;) &gt;&gt;&gt; f.__iter__() # Note: This returns the file itself &lt;_io.TextIOWrapper name=&#39;Data/portfolio.csv&#39; mode=&#39;r&#39; encoding=&#39;UTF-8&#39;&gt; &gt;&gt;&gt; next(f) &#39;name,shares,price n&#39; &gt;&gt;&gt; next(f) &#39;&quot;AA&quot;,100,32.20 n&#39; &gt;&gt;&gt; next(f) &#39;&quot;IBM&quot;,50,91.10 n&#39; . Customizing iteration . Now we look at how we can generalize iteration using a generator function. . Suppose you wanted to create your own custom iteration pattern. . For example, a countdown. . &gt;&gt;&gt; for x in countdown(10): ... print(x, end=&#39; &#39;) ... 10 9 8 7 6 5 4 3 2 1 . There is an easy way to do this. . Generator . A generator is a function that defines iteration. . def countdown(n): while n &gt; 0: yield n n -= 1 &gt;&gt;&gt; for x in countdown(10): ... print(x, end=&#39; &#39;) ... 10 9 8 7 6 5 4 3 2 1 . Definition: A generator is any function that uses the yield statement. . The behavior of generators is different than a normal function. Calling a generator function creates a generator object. It does not immediately execute the function. . def countdown(n): # Added a print statement print(&#39;Counting down from&#39;, n) while n &gt; 0: yield n n -= 1 &gt;&gt;&gt; x = countdown(10) # There is NO PRINT STATEMENT &gt;&gt;&gt; x # x is a generator object &lt;generator object at 0x58490&gt; . The function only executes on __next__() call. . &gt;&gt;&gt; x = countdown(10) &gt;&gt;&gt; x &lt;generator object at 0x58490&gt; &gt;&gt;&gt; x.__next__() Counting down from 10 10 &gt;&gt;&gt; . yield produces a value, but suspends the function execution. The function resumes on next call to __next__(). . &gt;&gt;&gt; x.__next__() 9 &gt;&gt;&gt; x.__next__() 8 . When the generator finally returns, the iteration raises an error. . &gt;&gt;&gt; x.__next__() 1 &gt;&gt;&gt; x.__next__() Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in ? StopIteration . This means a generator function implements the same low-level protocol that the for statements uses on lists, tuples, dicts, files, etc. . Generator example: find matching substring from lines in file . &gt;&gt;&gt; def filematch(filename, substr): with open(filename, &#39;r&#39;) as f: for line in f: if substr in line: yield line &gt;&gt;&gt; for line in open(&#39;Data/portfolio.csv&#39;): print(line, end=&#39;&#39;) name,shares,price &quot;AA&quot;,100,32.20 &quot;IBM&quot;,50,91.10 &quot;CAT&quot;,150,83.44 &quot;MSFT&quot;,200,51.23 &quot;GE&quot;,95,40.37 &quot;MSFT&quot;,50,65.10 &quot;IBM&quot;,100,70.44 &gt;&gt;&gt; for line in filematch(&#39;Data/portfolio.csv&#39;, &#39;IBM&#39;): print(line, end=&#39;&#39;) &quot;IBM&quot;,50,91.10 &quot;IBM&quot;,100,70.44 &gt;&gt;&gt; . Generator example: monitoring a streaming data source . Suppose there is a running program that keeps writing to Data/stocklog.csv in realtime. We use the code below to monitor the stream. . # follow.py import os import time f = open(&#39;Data/stocklog.csv&#39;) f.seek(0, os.SEEK_END) # Move file pointer 0 bytes from end of file while True: line = f.readline() if line == &#39;&#39;: time.sleep(0.1) # Sleep briefly and retry continue fields = line.split(&#39;,&#39;) name = fields[0].strip(&#39;&quot;&#39;) price = float(fields[1]) change = float(fields[4]) if change &lt; 0: print(f&#39;{name:&gt;10s} {price:&gt;10.2f} {change:&gt;10.2f}&#39;) . This while True loop along with some if checks and small sleep time keeps checking the end of the file. readline() will either return new data or an empty string, so if we get empty string we continue to the next retry. . It is just like the Unix tail -f command that is used to watch a log file. . Producers, consumers and pipelines . Generators are a useful tool for setting various kinds of producer/consumer problems and dataflow pipelines. . Producer-Consumer Problems . # Producer def follow(f): ... while True: ... yield line # Produces value in `line` below ... # Consumer for line in follow(f): # Consumes vale from `yield` above ... . yield produces values that for consumes. . Generator Pipelines . You can use this aspect of generators to set up processing pipelines (like Unix pipes). . producer → processing → processing → consumer . Processing pipes have an initial data producer, some set of intermediate processing stages and a final consumer. . The producer is typically a generator. Although it could also be a list of some other sequence. yield feeds data into the pipeline. | Intermediate processing stages simultaneously consume and produce items. They might modify the data stream. They can also filter (discarding items). | Consumer is a for-loop. It gets items and does something with them. | . &quot;&quot;&quot; producer → processing → processing → consumer &quot;&quot;&quot; def producer(): ... yield item # yields the item that is received by the `processing` ... def processing(s): for item in s: # Comes from the `producer` ... yield newitem # yields a new item ... def consumer(s): for item in s: # Comes from the `processing` ... &quot;&quot;&quot; To actually use it and setup the pipeline &quot;&quot;&quot; a = producer() b = processing(a) c = consumer(b) . You can create various generator functions and chain them together to perform processing involving data-flow pipelines. In addition, you can create functions that package a series of pipeline stages into a single function call . Generator expressions . Generator expressions are like list comprehension, except that generator expressions use () instead of []. . &gt;&gt;&gt; b = (2*x for x in a) &gt;&gt;&gt; b &lt;generator object at 0x58760&gt; &gt;&gt;&gt; for i in b: ... print(i, end=&#39; &#39;) ... 2 4 6 8 . Differences with List Comprehensions. . Does not construct a list. | Only useful purpose is iteration. | Once consumed, can’t be reused. | . General syntax: (&lt;expression&gt; for i in s if &lt;conditional&gt;). . It can also serve as a function argument: sum(x*x for x in a). . It can be applied to any iterable. . &gt;&gt;&gt; a = [1,2,3,4] &gt;&gt;&gt; b = (x*x for x in a) &gt;&gt;&gt; c = (-x for x in b) &gt;&gt;&gt; for i in c: ... print(i, end=&#39; &#39;) ... -1 -4 -9 -16 . The main use of generator expressions is in code that performs some calculation on a sequence, but only uses the result once. For example, strip all comments from a file. . f = open(&#39;somefile.txt&#39;) lines = (line for line in f if not line.startswith(&#39;#&#39;)) for line in lines: ... f.close() . With generators, the code runs faster and uses little memory. It’s like a filter applied to a stream. . Why Generators . Many problems are much more clearly expressed in terms of iteration. Looping over a collection of items and performing some kind of operation (searching, replacing, modifying, etc.). | Processing pipelines can be applied to a wide range of data processing problems. | . | Better memory efficiency. Only produce values when needed. | Contrast to constructing giant lists. | Can operate on streaming data | . | Generators encourage code reuse Separates the iteration from code that uses the iteration | You can build a toolbox of interesting iteration functions and mix-n-match. | . | . itertools module . The itertools is a library module with various functions designed to help with iterators/generators. . itertools.chain(s1,s2) itertools.count(n) itertools.cycle(s) itertools.dropwhile(predicate, s) itertools.groupby(s) itertools.ifilter(predicate, s) itertools.imap(function, s1, ... sN) itertools.repeat(s, n) itertools.tee(s, ncopies) itertools.izip(s1, ... , sN) . All functions process data iteratively. They implement various kinds of iteration patterns. . Advanced topics . These are some useful advanced topics that you will use day-to-day. . Variable arguments . A function that accepts any number of arguments is said to use variable arguments. For example, *args is a tuple that contains any number of positional arguments: . def f(x, *args): ... f(1,2,3,4,5) def f(x, *args): # x -&gt; 1 # args -&gt; (2,3,4,5), a tuple . A function can also accept any number of keyword arguments. For example: . def f(x, y, **kwargs): ... f(2, 3, flag=True, mode=&#39;fast&#39;, header=&#39;debug&#39;) def f(x, y, **kwargs): # x -&gt; 2 # y -&gt; 3 # kwargs -&gt; { &#39;flag&#39;: True, &#39;mode&#39;: &#39;fast&#39;, &#39;header&#39;: &#39;debug&#39; }, a dict . Combining both we have: . def f(*args, **kwargs): ... f(2, 3, flag=True, mode=&#39;fast&#39;, header=&#39;debug&#39;) def f(*args, **kwargs): # args = (2, 3) # kwargs -&gt; { &#39;flag&#39;: True, &#39;mode&#39;: &#39;fast&#39;, &#39;header&#39;: &#39;debug&#39; } ... . This function takes any combination of positional or keyword arguments. It is sometimes used when writing wrappers or when you want to pass arguments through to another function. . Passing tuples and dicts . We can also use * to expand tuple, ** to expand dict, and pass into a function. . numbers = (2,3,4) f(1, *numbers) # Same as f(1,2,3,4) options = { &#39;color&#39; : &#39;red&#39;, &#39;delimiter&#39; : &#39;,&#39;, &#39;width&#39; : 400 } f(data, **options) # Same as f(data, color=&#39;red&#39;, delimiter=&#39;,&#39;, width=400) . Callback function, and Lambda anonymous function . If we want to sort a dictionary in-place, we do: . def stock_name(s): return s[&#39;name&#39;] # stock_name is a callback portfolio.sort(key=stock_name) &quot;&quot;&quot; # Check how the dictionaries are sorted by the `name` key [ {&#39;name&#39;: &#39;AA&#39;, &#39;price&#39;: 32.2, &#39;shares&#39;: 100}, {&#39;name&#39;: &#39;CAT&#39;, &#39;price&#39;: 83.44, &#39;shares&#39;: 150}, {&#39;name&#39;: &#39;GE&#39;, &#39;price&#39;: 40.37, &#39;shares&#39;: 95}, {&#39;name&#39;: &#39;IBM&#39;, &#39;price&#39;: 91.1, &#39;shares&#39;: 50}, {&#39;name&#39;: &#39;IBM&#39;, &#39;price&#39;: 70.44, &#39;shares&#39;: 100}, {&#39;name&#39;: &#39;MSFT&#39;, &#39;price&#39;: 51.23, &#39;shares&#39;: 200}, {&#39;name&#39;: &#39;MSFT&#39;, &#39;price&#39;: 65.1, &#39;shares&#39;: 50} ] &quot;&quot;&quot; . The key function is an example of a callback function. . The sort() method “calls back” to a function you supply. . Callback functions are often short one-line functions that are only used for that one operation. Programmers often ask for a short-cut for specifying this extra processing. . Use a lambda instead of creating the function. In our previous sorting example. . portfolio.sort(key=lambda s: s[&#39;name&#39;]) . This creates an unnamed function that evaluates a single expression. . Using lambda . lambda is highly restricted. | Only a single expression is allowed. | No statements like if, while, etc. | Most common use is with functions like sort(). | . Returning functions . We can use functions to create other functions. . Consider this example: . def add(x, y): def do_add(): # `x` and `y` are defined outside `do_add()` print(&#39;Adding&#39;, x, y) return x + y return do_add . x and y are defined outside do_add(). . Further observe that those variables are somehow kept alive after add() has finished! . &gt;&gt;&gt; a = add(3,4) &gt;&gt;&gt; a &lt;function do_add at 0x6a670&gt; &gt;&gt;&gt; a() Adding 3 4 # Where are these values coming from? 7 . Closures . When an inner function is returned as a result, that inner function is known as a closure. . def add(x, y): # `do_add` is a closure def do_add(): print(&#39;Adding&#39;, x, y) return x + y return do_add . Essential feature: A closure retains the values of all variables needed for the function to run properly later on. . Think of a closure as a function plus an extra environment that holds the values of variables that it depends on. . Use Closure in callback functions . Closure are an essential feature of Python. However, their use is often subtle. Common applications: . Use in callback functions | Delayed evaluation | Decorator functions | . Consider a function like this: . def after(seconds, func): time.sleep(seconds) func() . Usage example: . def greeting(): print(&#39;Hello Guido&#39;) after(30, greeting) . after executes the supplied function… later. . Closures carry extra information around. . def add(x, y): def do_add(): print(f&#39;Adding {x} + {y} -&gt; {x+y}&#39;) return do_add def after(seconds, func): time.sleep(seconds) func() after(30, add(2, 3)) # `do_add` has the references x -&gt; 2 and y -&gt; 3 . Use closure to avoid code repetition . Closures can also be used as technique for avoiding excessive code repetition. You can write functions that make code. . Consider this code: . class Stock: def __init__(self, name, shares, price): self.name = name self.shares = shares self.price = price ... @property def shares(self): return self._shares @shares.setter def shares(self, value): if not isinstance(value, int): raise TypeError(&#39;Expected int&#39;) self._shares = value ... . You want the type check to apply not just on shares, but on all other things, and you want to avoid typing this code again and again, what do you do? . # typedproperty.py def typedproperty(name, expected_type): private_name = &#39;_&#39; + name @property def prop(self): return getattr(self, private_name) @prop.setter def prop(self, value): if not isinstance(value, expected_type): raise TypeError(f&#39;Expected {expected_type}&#39;) setattr(self, private_name, value) return prop # stock.py from typedproperty import typedproperty class Stock: name = typedproperty(&#39;name&#39;, str) shares = typedproperty(&#39;shares&#39;, int) price = typedproperty(&#39;price&#39;, float) def __init__(self, name, shares, price): self.name = name self.shares = shares self.price = price &gt;&gt;&gt; s = Stock(&#39;IBM&#39;, 50, 91.1) &gt;&gt;&gt; s.name &#39;IBM&#39; &gt;&gt;&gt; s.shares = &#39;100&#39; ... should get a TypeError ... &gt;&gt;&gt; . Decorators . A decorator function is a function that wraps the decorated function with some additional stuff. . Say you want to do logging for add and sub, . def add(x, y): print(&#39;Calling add&#39;) return x + y def sub(x, y): print(&#39;Calling sub&#39;) return x - y . This is repetitive. I could have: . def logged(func): def wrapper(*args, **kwargs): print(&#39;Calling&#39;, func.__name__) return func(*args, **kwargs) return wrapper def add(x, y): return x + y logged_add = logged(add) logged_add(3, 4) # You see the logging message appear . This example illustrates the process of creating a so-called wrapper function. . A wrapper is a function that wraps around another function with some extra bits of processing, but otherwise works in the exact same way as the original function. . The logged() function creates the wrapper and returns it as a result. . Putting wrappers around functions is extremely common in Python. So common, there is a special syntax for it – the decorator. . def add(x, y): return x + y add = logged(add) # Special syntax @logged def add(x, y): return x + y . A decorator is just syntactic sugar. It’s exactly the same as the first approach. . There are many more subtle details to decorators than what has been presented here. For example, using them in classes. Or using multiple decorators with a function. However, the previous example is a good illustration of how their use tends to arise. Usually, it’s in response to repetitive code appearing across a wide range of function definitions. A decorator can move that code to a central definition. . For more information on decorators, check out this post. . Static and class methods . There are a few built-in decorators that are used in combination with method definitions. . class Foo: def bar(self,a): ... @staticmethod def spam(a): ... @classmethod def grok(cls,a): ... @property def name(self): ... . Static methods: for generic functionality or design patterns . @staticmethod is used to define a so-called static class methods (from C++/Java). . A static method is a function that is part of the class, but which does not operate on instances. . class Foo(object): @staticmethod def bar(x): print(&#39;x =&#39;, x) &gt;&gt;&gt; Foo.bar(2) x = 2 . Static methods are sometimes used to implement internal supporting code for a class. For example, code to help manage created instances (memory management, system resources, persistence, locking, etc). They’re also used by certain design patterns (not discussed here). . Class Methods: for alternative constructors . @classmethod is used to define class methods. . A class method is a method that receives the class object as the first parameter instead of the instance. . class Foo: def bar(self): print(self) @classmethod def spam(cls): print(cls) &gt;&gt;&gt; f = Foo() &gt;&gt;&gt; f.bar() &lt;__main__.Foo object at 0x971690&gt; # The instance `f` &gt;&gt;&gt; Foo.spam() &lt;class &#39;__main__.Foo&#39;&gt; # The class `Foo` . Class methods are most often used as a tool for defining alternate constructors. . class Date: def __init__(self,year,month,day): self.year = year self.month = month self.day = day @classmethod def today(cls): # Notice how the class is passed as an argument tm = time.localtime() # And used to create a new instance return cls(tm.tm_year, tm.tm_mon, tm.tm_mday) d = Date.today() . Class methods solve some tricky problems with features like inheritance. . class Date: ... @classmethod def today(cls): # Gets the correct class (e.g. `NewDate`) tm = time.localtime() return cls(tm.tm_year, tm.tm_mon, tm.tm_mday) class NewDate(Date): ... d = NewDate.today() . Reference . https://dabeaz-course.github.io/practical-python/Notes .",
            "url": "http://blog.logancyang.com/note/python/2020/07/02/python-basics-iii.html",
            "relUrl": "/note/python/2020/07/02/python-basics-iii.html",
            "date": " • Jul 2, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "[From NAND to TETRIS] Computer Architecture 101 Part II: Machine Language",
            "content": "Continuing the discussion through abstraction level 1-3 in the last post, we look at level 4: machine language in detail in this post. . Level 4: Machine language . Before we build the computer, let’s understand what we want the computer to be able to do from a user point of view. . The reason that computers can do a lot of different tasks is because it is modeled based on Universal Turing Machine in theory, and Von Neumann Architecture in practice. . . . The machine is designed to read binary instructions and perform different tasks. The binary instructions are called machine language. It is one of the most important elements of computer science because it is how software controls hardware. . Machine language consists of three conceptual parts: . Operations. It has a list of operations encoded in binary to go through. Each operation instructs it to do a different thing. | Program counter. Let the computer know which operation to execute next. | Address. Let the computer know what to operate on, where it can get it and where to put the output. | . We humans don’t program in machine language. We write high level language and let the compiler to translate it to machine language. But since now we are learning how to build a computer, we need to understand machine language. . Machine language is just a sequence of bits. For example, the ADD operation is defined as 0100010, say a variable R3 is 0011, R2 is0010, then 010001000110010 means ADD R3 R2. . We allow humans to write machine language via Assemly language. In Level 6, we will build “Assembler” to convert it into bits. . Machine language is usually in close correspondence to actual hardware architecture. If we want more powerful machine language operations, we need more advanced hardware. . Common machine operations: . Arithmetic | Logical operations | Flow control: goto instruction Y, if A then goto instruction C | . Differences between machine languages: . Richness of operations (division? bulk copy?) | Data types (width, floating point…) | . For example, some machines can only handle 8-bit arithmetic. To do a 64-bit addition, it can still do it by iterating through its 8 bits and relying on algorithm. But a 64-bit machine can do it much faster and more easily than the 8-bit machine. . Memory hierarchy . Accessing a memory location is expensive. . large memory has long addresses | getting the memory content into CPU takes time (slow compared to CPU carrying out the operation) | . The solution was introduced by Von Neumann when he built the first computer: memory hierarchy. . . We have CPU registers (smallest and fastest), then cache, main memory, and disk (biggest and slowest). Smaller memory is faster because there are only a few registers, the addresses are short, and they sit closer to CPU. . CPU registers . CPU registers are built with the fastest technology. And since they sit inside the CPU, there’s almost no delay. . There are 2 types of registers in the CPU. . Data registers. We can put numbers in them directly. | Pointer to main memory, e.g. put variable X into @A where A is an address in the main memory. | . . Input/Output . CPU needs some kind of protocol known as drivers to talk to input/output devices, such as mouse, keyboard, camera, sensors, screen, printer, sound, etc. . One general method of interaction uses “memory mapping”, e.g. . Memory location 12345 holds the direction of the last mouse movement. | Memory location 45678 is not a real memory location but a way to tell the printer which paper to use. | . Flow control . Flow control is the way we tell the hardware which instruction to execute next. . There is unconditional jump so we can loop: . . There is conditional jump if a condition is met: . . The Hack computer and machine language . The Hack computer we are building is going to a 16-bit computer with an architecture shown below: . . It has . a data memory (RAM) | an instruction memory (ROM) | a CPU consists of 16-bit ALUs that performs 16-bit instructions | some buses to move data between them. Think of them as highways of 16 lanes moving 16-bit data around. | . Hack software . We design the Hack machine language to have 2 types of instructions: . 16-bit A-instructions | 16-bit C-instructions | . Hack program = a sequence of instructions written in the Hack machine language. . The Hack program is loaded into the instruction memory (ROM), then the reset button is pushed to start the program. The reset button is pushed once for one program. . . The Hack machine language recognizes 3 registers: . D register: holds a 16-bit value | A register: holds a 16-bit value or an address | M register: called the selected memory register, a 16-bit RAM register addressed by A. | . . Notice that ROM (which stores the instructions) isn’t included in the 3 registers! . A-instruction . Example: @100 . When this A-instruction is executed, the A register holds 100, and RAM[100] is selected in the M register (RAM). . // Set RAM[100] to -1 @100 // A=100, select RAM[100] M = -1 // RAM[100]=-1 . The above Hack machine language means we select RAM[100] by setting A register to 100. M denotes the memory content of RAM[100], we assign -1 to it. . C-instruction . C-instruction is the work horse of the language. It has 3 fields: dest = comp ; jump . Computation. Consists of logical operations on A, D and M. | Destination (optional) | A jump directive (optional) | . Refer to the slide above for possible comp, dest, jump values. . Example: set RAM[300] to D-1 . @300 M = D-1 // D-1 is in the predefined comp as shown above . Example: if D-1 == 0, jump to execute the instruction stored in ROM[56] . @56 // A=56 D-1; JEQ // if D-1 == 0, goto 56 . JEQ checks if comp equals 0, if yes then jump to address A (keep in mind that jump aims at ROM addresses, not RAM). In this case, comp is D-1, so JEQ checks if D-1 equals 0. . Example code: . @1 M=A-1; JEQ . What does this do? . @1 sets A register to 1. | Compute comp which is A-1: A-1 equals 0. | Then it stores the result 0 into the M register, RAM[1], because A is 1. | JEQ checks if comp equals 0. Yes. | The next instruction is ROM[1] because A is 1. | We don’t need to remember all the possible values for the 3 registers. For details, check the website. . Hack language specification . We have two ways of specifying machine language code, one in symbols and one in binary numbers. An assembler can translate symbolic code into binary code. . A-instruction specification . . A-instruction has an “op code” (first bit in binary code) of 0 at the beginning of the binary code. . C-instruction specification . C-instruction has an “op code” of 1, followed by 2 bits we don’t use. By convention we set them to 1. . Here is a table where we can convert symbols to binary codes. . . For example, if we want to convert D+1, we find the symbol, it’s in the column a=0, and has a value of 011111. . So the comp field D+1 is 1110011111. . We also have a table for dest the destination field. . . Similarly, we have a table for jump values. Altogether we have everything in one slide here: . . Finally, here is an example of a small Hack program and its translation to binary. . . Working with registers and memory using machine language . Some examples . // D=10 @10 // There is no directive to directly set D=10. Set A=10 first D=A // and set D=A // D++ D=D+1 // This is easy // D = RAM[17] @17 D=M // RAM[17] = D @17 M=D // RAM[17] = 10 @10 D=A @17 M=D // RAM[5] = RAM[3] @3 D=M @5 M=D . Here’s another example: . . Note that the white spaces are ignored, and each line of code has a line number in the background automatically. . How to terminate the program properly . We haven’t talk about program termination. If we execute our code naively, a malicious hacker could add malicious code after our code to do bad things. It is called NOP slide, meaning they added null operations after the actual code and before the malicious code to hide the latter. . We use the fact that the computer never stands still and always executes something, we end the program with an infinite loop so it is under our control. This is the best practice for program termination. . ... @6 0; JMP . Built-in symbols . The Hack assembly language features built-in symbols which are virtual registers, not real registers. The symbols are R0, R1, R2, ... , R15 and they correspond to values 0, 1, 2, ..., 15. . Why do we need these virtual registers? . It is purely for style and readability. . For example, . // RAM[5]=15 @15 D=A @5 M=D . The two @x lines are intended to do completely different things. The first is to set A and then set D. The second is to get address 5 in RAM and store D there. . It means when we see @x we don’t know what it wants to do until we see the next line of code. So we introduce R5, . // RAM[5]=15 @15 D=A @R5 M=D . When we use @R5 it’s exactly the same as @5, but it means finding the address 5 so that it’s more readable for people. . Here are all the built-in symbols: . . Branching, variables, iteration using machine language . Branching . In any high level language there are many branching mechanisms such as if-else, while, switch, etc. In machine language, there is only one: goto using the jump directives. . Example: . // Program: signum.asm // Computes: // if R0 &gt; 0: // R1 = 1 // else: // R1 = 0 @R0 D=M // D = RAM[0] @8 D; JGT // if R0 &gt; 0, goto 8 @R1 M=0 // RAM[1] = 0 @10 0; JMP // end of program @R1 M=1 // R1=1 @10 0; JMP // end of program . You can see this code is quite unreadable if there’s no comment or documentation. One thing we introduce to make it more readable is labels. One example is shown below. (POSITIVE) is a label that points to its next line. @POSITIVE is using the label to goto that line. This way we have a much more readable branching mechanism. . . Variables . Say we want to exchange the values of R0 and R1: . // Program: flip.asm // Flips the value of RAM[0] and RAM[1] // temp = R1 // R1 = R0 // R0 = temp . The people who created the Assembler can define a contract: @somevar with no label (somevar) means it’s a variable, and it will use RAM[program_base_address+16]. Any new variable will increment 16, i.e. use +17, +18, etc. . . Iterations . Suppose we want to compute 1+2+...+n, we need an accumulator variable and an iteration in a high level language. The machine code is shown below: . . Pointers . From the machine’s perspective, an array is just a block of memory that starts at a certain base memory location with a certain length. The following example is to set -1 into the array. . . First we set 2 variables, the base memory location arr and length n. Then we set the variable i. To achieve RAM[arr+i] = -1, we set register A to be D+M. This is the first time we set A using an arithmetic operation. . . Variables that store memory addresses like arr and i are called pointers. When we need to access memory using a pointer, we need an instruction like A=M. . Typical pointer semantics: “set the address register to the content of some memory register”. . Input/Output using machine language . The computer gets data from humans via input devices like the keyboard, and outputs to output devices like the display. . In high level languages such as Java and Python, we write code using input devices and the output devices give us the high level results such as text, graphics, audio and video. This is in the realm of software hierarchy and isn’t the focus now. Now we focus on the hardware hierarchy. . All the high level functionalities depend on the low level operations on bits. . . Screen output . There is a part in RAM that’s called the Screen Memory Map which refreshes many times per second. It directly controls the display on the screen. When we need to display something, we manipulate this part of the memory. . . The Hack computer assumes a physical screen of 256x512 pixels black and white. In the memory, we use a chunk of 16-bit registers to represent this pixel matrix. . Since one word is 16-bit, and one row in the screen is 512 pixels, we need 512/16=32 words to represent a row. Doing some math we can calculate the memory address of each pixel. . Note that the whole screen memory chunk resides inside the big RAM and it has a starting address. We use a chip Screen[] to add the starting base address to the pixel coordinates to get the actual memory address. . . You might be shocked how much work we need to do just to manipulate one pixel. That’s the reality at low level! . Keyboard input . A physical keyboard is connected to the Keyboard Memory Map in RAM. The good thing is that we only need one 16-bit register to represent the keyboard. . . Each key has a binary scan code that goes into the keyboard memory map. If the keyboard is not pressed, the scan code is 0. . To see what key is pressed, just probe the keyboard chip. In the Hack computer, the keyboard memory map is at RAM[24576]. . Here is a complete scan code mapping for the Hack computer keyboard: . . Notice that the keyboard memory map has 16 bits, it can represent $2^{16}=65536$ different keys which is more than enough even for unicode characters. . Draw a rectangle with Hack programming . . Let’s consider the “hello world” program of computer graphics: drawing a rectangle. . . Let’s focus on the pseudocode. The goal is to manipulate the Screen Memory Map to show the rectangle. . . The following is the real Hack code . . Compiler: translates high level language to machine code . This is From NAND to TETRIS part I and we don’t concern ourselves with the compiler which translates a high level language to machine code. In part II, there’s material showing how to write a compiler and an operating system. . . Comments . Hack is a simplified version of the machine language. The machine language that controls our day-to-day personal computers is more complex and has more features such as floating point arithmetic. However, we can always use software to expand the capabilities of the machine language, and that is in the part II of From NAND to TETRIS. . Go to Part III . Reference . Coursera From NAND To TETRIS .",
            "url": "http://blog.logancyang.com/note/compsci/2020/07/02/computer-architecture-101-part-ii.html",
            "relUrl": "/note/compsci/2020/07/02/computer-architecture-101-part-ii.html",
            "date": " • Jul 2, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "[Python Review] Part II: Classes and Objects",
            "content": "Class and object basics . Overriding . Sometimes a class extends an existing method, but it wants to use the original implementation inside the redefinition. For this, use super() to call the old one: . class Stock: ... def cost(self): return self.shares * self.price ... class MyStock(Stock): def cost(self): # Check the call to `super` actual_cost = super().cost() return 1.25 * actual_cost . If __init__ is redefined, it is essential to initialize the parent. . class Stock: def __init__(self, name, shares, price): self.name = name self.shares = shares self.price = price class MyStock(Stock): def __init__(self, name, shares, price, factor): # Check the call to `super` and `__init__` super().__init__(name, shares, price) self.factor = factor def cost(self): return self.factor * super().cost() . Organizing inheritance . Inheritance is sometimes used to organize related objects. Think of a logical hierarchy or taxonomy. However, a more common (and practical) usage is related to making reusable or extensible code. For example, a framework might define a base class and instruct you to customize it. . class CustomHandler(TCPHandler): def handle_request(self): ... # Custom processing . The base class contains some general purpose code. Your class inherits and customized specific parts. . “is a” relationship . Inheritance establishes a type relationship. . class Shape: ... class Circle(Shape): ... &gt;&gt;&gt; c = Circle(4.0) &gt;&gt;&gt; isinstance(c, Shape) True . Important: Ideally, any code that worked with instances of the parent class will also work with instances of the child class. . object base class . If a class has no parent, you sometimes see object used as the base. object is the parent of all objects in Python. . Multiple Inheritance . You can inherit from multiple classes by specifying them in the definition of the class. . class Mother: ... class Father: ... class Child(Mother, Father): ... . The class Child inherits features from both parents. There are some rather tricky details. Don’t do it unless you know what you are doing. . Special methods . There are dozens of __xxx__ methods in Python. . __str__() is used to create a nice printable output. . __repr__() is used to create a more detailed representation for programmers. . Note: The convention for repr() is to return a string that, when fed to eval(), will recreate the underlying object. If this is not possible, some kind of easily readable representation is used instead. . Special dunder methods for math . a + b a.__add__(b) a - b a.__sub__(b) a * b a.__mul__(b) a / b a.__truediv__(b) a // b a.__floordiv__(b) a % b a.__mod__(b) a &lt;&lt; b a.__lshift__(b) a &gt;&gt; b a.__rshift__(b) a &amp; b a.__and__(b) a | b a.__or__(b) a ^ b a.__xor__(b) a ** b a.__pow__(b) -a a.__neg__() ~a a.__invert__() abs(a) a.__abs__() . Special dunder methods for item access . len(x) x.__len__() x[a] x.__getitem__(a) x[a] = v x.__setitem__(a,v) del x[a] x.__delitem__(a) # Implement a sequence class Sequence: def __len__(self): ... def __getitem__(self,a): ... def __setitem__(self,a,v): ... def __delitem__(self,a): ... . Bound method . A method that has not yet been invoked by the function call operator () is known as a bound method. It operates on the instance where it originated. . &gt;&gt;&gt; s = Stock(&#39;GOOG&#39;, 100, 490.10) &gt;&gt;&gt; s &lt;Stock object at 0x590d0&gt; &gt;&gt;&gt; c = s.cost &gt;&gt;&gt; c &lt;bound method Stock.cost of &lt;Stock object at 0x590d0&gt;&gt; &gt;&gt;&gt; c() 49010.0 . Bound methods are often a source of careless non-obvious errors: you simply forgot to add (). . Attribute access . There is an alternative way to access, manipulate and manage attributes. . getattr(obj, &#39;name&#39;) # Same as obj.name setattr(obj, &#39;name&#39;, value) # Same as obj.name = value delattr(obj, &#39;name&#39;) # Same as del obj.name hasattr(obj, &#39;name&#39;) # Tests if attribute exists . Example: . if hasattr(obj, &#39;x&#39;): x = getattr(obj, &#39;x&#39;): else: x = None . Note: getattr() also has a useful default value *arg. . x = getattr(obj, &#39;x&#39;, None) . Defining new exceptions . User defined exceptions are defined by classes. Exceptions always inherit from Exception. Usually they are empty classes. Use pass for the body. . class NetworkError(Exception): pass . You can make your own hierarchy of exceptions: . class AuthenticationError(NetworkError): pass class ProtocolError(NetworkError): pass . Inner workings of Python objects . Programmers coming from other programming languages often find Python’s notion of classes lacking in features. For example, there is no notion of access-control (e.g., private, protected), the whole self argument feels weird, and frankly, working with objects sometimes feel like a “free for all”. Maybe that’s true, but we’ll find out how it all works as well as some common programming idioms to better encapsulate the internals of objects. . It’s not necessary to worry about the inner details to be productive. However, most Python coders have a basic awareness of how classes work. . Dictionary revisited . The Python object system is largely based on an implementation involving dictionaries. They are used for critical parts of the interpreter and may be the most important type of data in Python. . For example, a module has .__dict__ or globals() . # foo.py x = 42 def bar(): ... def spam(): ... &gt;&gt;&gt; foo.__dict__ { &#39;x&#39; : 42, &#39;bar&#39; : &lt;function bar&gt;, &#39;spam&#39; : &lt;function spam&gt; } . An object has .__dict__ as well. In fact, the entire object system is mostly an extra layer that’s put on top of dictionaries. . &gt;&gt;&gt; s = Stock(&#39;GOOG&#39;, 100, 490.1) &gt;&gt;&gt; s.__dict__ {&#39;name&#39; : &#39;GOOG&#39;, &#39;shares&#39; : 100, &#39;price&#39;: 490.1 } . You populate this dict (and instance) when assigning to self. . class Stock: def __init__(self, name, shares, price): self.name = name self.shares = shares self.price = price . Each instance gets its own private dictionary. . s = Stock(&#39;GOOG&#39;, 100, 490.1) # s.__dict__: {&#39;name&#39; : &#39;GOOG&#39;,&#39;shares&#39; : 100, &#39;price&#39;: 490.1 } t = Stock(&#39;AAPL&#39;, 50, 123.45) # t.__dict__: {&#39;name&#39; : &#39;AAPL&#39;,&#39;shares&#39; : 50, &#39;price&#39;: 123.45 } . If you created 100 instances of some class, there are 100 dictionaries sitting around holding data. . A separate dictionary for class members, Stock.__dict__ also holds the methods: . class Stock: def __init__(self, name, shares, price): self.name = name self.shares = shares self.price = price def cost(self): return self.shares * self.price def sell(self, nshares): self.shares -= nshares &gt;&gt;&gt; Stock.__dict__ { &#39;cost&#39;: &lt;function&gt;, &#39;sell&#39;: &lt;function&gt;, &#39;__init__&#39;: &lt;function&gt; } . Instances and classes are linked together. The __class__ attribute refers back to the class. . The instance dictionary holds data unique to each instance, whereas the class dictionary holds data collectively shared by all instances. . When you work with an object, you access data and methods using the . operator. . x = obj.name # Getting obj.name = value # Setting del obj.name # Deleting . These operations are directly tied to the dictionaries sitting underneath the covers. Operations that modify an object update the underlying dictionary. . Reading attribute . Suppose you read an attribute on an instance. . x = obj.name . The attribute may exist in two places: . Local instance dictionary. | Class dictionary. | . Both dictionaries must be checked. First, check in local __dict__. If not found, look in __dict__ of class through __class__. This lookup scheme is how the members of a class get shared by all instances. . How inheritance works . Classes may inherit from other classes. The base classes are stored in a tuple in each class. This provides a link to parent classes. . class A(B, C): ... &gt;&gt;&gt; A.__bases__ (&lt;class &#39;__main__.B&#39;&gt;, &lt;class &#39;__main__.C&#39;&gt;) . Reading Attributes with Inheritance . Logically, the process of finding an attribute is as follows. First, check in local __dict__. If not found, look in __dict__ of the class. If not found in class, look in the base classes through __bases__. However, there are some subtle aspects of this discussed next. . Reading Attributes with Single Inheritance . In inheritance hierarchies, attributes are found by walking up the inheritance tree in order. . class A: pass class B(A): pass class C(A): pass class D(B): pass class E(D): pass . With single inheritance, there is single path to the top. You stop with the first match. . Method Resolution Order (MRO) . Python precomputes an inheritance chain and stores it in the MRO attribute on the class. You can view it. . &gt;&gt;&gt; E.__mro__ (&lt;class &#39;__main__.E&#39;&gt;, &lt;class &#39;__main__.D&#39;&gt;, &lt;class &#39;__main__.B&#39;&gt;, &lt;class &#39;__main__.A&#39;&gt;, &lt;type &#39;object&#39;&gt;) . This chain is called the Method Resolution Order. To find an attribute, Python walks the MRO in order. The first match wins. . MRO in Multiple Inheritance . With multiple inheritance, there is no single path to the top. Let’s take a look at an example. . class A: pass class B: pass class C(A, B): pass class D(B): pass class E(C, D): pass . What happens when you access an attribute? An attribute search process is carried out, but what is the order? That’s a problem. . Python uses cooperative multiple inheritance which obeys some rules about class ordering. . Children are always checked before parents | Parents (if multiple) are always checked in the order listed. | . The MRO is computed by sorting all of the classes in a hierarchy according to those rules. . &gt;&gt;&gt; E.__mro__ ( &lt;class &#39;E&#39;&gt;, &lt;class &#39;C&#39;&gt;, &lt;class &#39;A&#39;&gt;, &lt;class &#39;D&#39;&gt;, &lt;class &#39;B&#39;&gt;, &lt;class &#39;object&#39;&gt;) . The underlying algorithm is called the C3 Linearization Algorithm. The gist of the order is: children first, followed by parents. . Why use multiple inheritance: the “Mixin” pattern . Consider two completely unrelated objects: . class Dog: def noise(self): return &#39;Bark&#39; def chase(self): return &#39;Chasing!&#39; class LoudDog(Dog): def noise(self): # Code commonality with LoudBike (below) return super().noise().upper() # And class Bike: def noise(self): return &#39;On Your Left&#39; def pedal(self): return &#39;Pedaling!&#39; class LoudBike(Bike): def noise(self): # Code commonality with LoudDog (above) return super().noise().upper() . There is a code commonality in the implementation of LoudDog.noise() and LoudBike.noise(). In fact, the code is exactly the same. Naturally, code like that is bound to attract software engineers. . The Mixin pattern is a class with a fragment of code. . class Loud: def noise(self): return super().noise().upper() . This class is not usable in isolation. It mixes with other classes via inheritance. . class LoudDog(Loud, Dog): pass class LoudBike(Loud, Bike): pass . Miraculously, loudness was now implemented just once and reused in two completely unrelated classes. This sort of trick is one of the primary uses of multiple inheritance in Python. . Why super() . Always use super() when overriding methods. super() delegates to the next class on the MRO. . The tricky bit is that you don’t know what it is. You especially don’t know what it is if multiple inheritance is being used. . Caution: Multiple inheritance is a powerful tool. Remember that with power comes responsibility. Frameworks / libraries sometimes use it for advanced features involving composition of components. . Encapsulation techniques . When writing classes, it is common to try and encapsulate internal details. This section introduces a few Python programming idioms for this including private variables and properties. . Public vs. private . One of the primary roles of a class is to encapsulate data and internal implementation details of an object. However, a class also defines a public interface that the outside world is supposed to use to manipulate the object. This distinction between implementation details and the public interface is important. . However, in Python, almost everything about classes and objects is open. . You can easily inspect object internals. | You can change things at will. | There is no strong notion of access-control (i.e., private class members) | . That is an issue when you are trying to isolate details of the internal implementation. . Python relies on programming conventions to indicate the intended use of something. These conventions are based on naming. There is a general attitude that it is up to the programmer to observe the rules as opposed to having the language enforce them. . Any attribute name with leading _ is considered to be private. . class Person(object): def __init__(self, name): self._name = 0 . You can still modify it, it’s just a naming style. If you find yourself using such names directly, you’re probably doing something wrong. Look for higher level functionality. . @property: applying property checks . If you want to enforce some checks on the properties: . s.shares = &#39;50&#39; # Raise a TypeError, this is a string . Use the @property decorator. . class Stock: def __init__(self, name, shares, price): self.name = name self.shares = shares self.price = price @property def shares(self): return self._shares @shares.setter def shares(self, value): if not isinstance(value, int): raise TypeError(&#39;Expected int&#39;) self._shares = value &quot;&quot;&quot; Normal attribute access now triggers the getter and setter methods under @property and @shares.setter. &quot;&quot;&quot; &gt;&gt;&gt; s = Stock(&#39;IBM&#39;, 50, 91.1) &gt;&gt;&gt; s.shares # Triggers @property 50 &gt;&gt;&gt; s.shares = 75 # Triggers @shares.setter . With this pattern, there are no changes needed to the source code. The new setter is also called when there is an assignment within the class, including inside the __init__() method. . class Stock: def __init__(self, name, shares, price): ... # This assignment calls the setter below self.shares = shares ... ... @shares.setter def shares(self, value): if not isinstance(value, int): raise TypeError(&#39;Expected int&#39;) self._shares = value . There is often a confusion between a property and the use of private names. Although a property internally uses a private name like _shares, the rest of the class (not the property) can continue to use a name like shares. . @property: uniform access . Properties are also useful for computed data attributes. . class Stock: def __init__(self, name, shares, price): self.name = name self.shares = shares self.price = price @property def cost(self): return self.shares * self.price ... &gt;&gt;&gt; s = Stock(&#39;GOOG&#39;, 100, 490.1) &gt;&gt;&gt; s.shares # Instance variable 100 &quot;&quot;&quot; Notice, s.cost does not need () &quot;&quot;&quot; &gt;&gt;&gt; s.cost # Computed Value 49010.0 . @property on a method allows you to drop the parentheses, hiding the fact that it’s actually a method! . It makes things look more uniform for methods that look like data attributes. It is called “uniform access”. . &gt;&gt;&gt; s = Stock(&#39;GOOG&#39;, 100, 490.1) &gt;&gt;&gt; a = s.cost() # Method 49010.0 &gt;&gt;&gt; b = s.shares # Data attribute 100 . __slot__ attribute . You can restrict the set of attributes names. It will raise an error for other attributes. . class Stock: __slots__ = (&#39;name&#39;,&#39;_shares&#39;,&#39;price&#39;) def __init__(self, name, shares, price): self.name = name ... &gt;&gt;&gt; s.price = 385.15 &gt;&gt;&gt; s.prices = 410.2 Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in ? AttributeError: &#39;Stock&#39; object has no attribute &#39;prices&#39; . Although this prevents errors and restricts usage of objects, it’s actually used for performance and makes Python use memory more efficiently. . It should be noted that slots is most commonly used as an optimization on classes that serve as data structures. Using slots will make such programs use far-less memory and run a bit faster. You should probably avoid slots on most other classes however. . A comment . Don’t go overboard with private attributes, properties, slots, etc. They serve a specific purpose and you may see them when reading other Python code. However, they are not necessary for most day-to-day coding. . __init__ vs __call__ . __init__ vs __call__ for a class: https://stackoverflow.com/questions/9663562/what-is-the-difference-between-init-and-call . __init__ uses the class name and creates an instance. . __call__ is for an instance to be called as a function. . inst = MyClass() # MyClass.__init__ s = inst() # MyClass.__call__ . Reference . https://dabeaz-course.github.io/practical-python/Notes .",
            "url": "http://blog.logancyang.com/note/python/2020/06/30/python-basics-ii.html",
            "relUrl": "/note/python/2020/06/30/python-basics-ii.html",
            "date": " • Jun 30, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "[Python Review] Part I: The Basics",
            "content": "String methods . s.endswith(suffix) # Check if string ends with suffix s.find(t) # First occurrence of t in s s.index(t) # First occurrence of t in s s.isalpha() # Check if characters are alphabetic s.isdigit() # Check if characters are numeric s.islower() # Check if characters are lower-case s.isupper() # Check if characters are upper-case s.join(slist) # Join a list of strings using s as delimiter s.lower() # Convert to lower case s.replace(old,new) # Replace text s.rfind(t) # Search for t from end of string s.rindex(t) # Search for t from end of string s.split([delim]) # Split string into list of substrings s.startswith(prefix) # Check if string starts with prefix s.strip() # Strip leading/trailing space s.upper() # Convert to upper case . Strings are immutable. All operations and methods that manipulate string data, always create new strings. . Byte strings . A string of 8-bit bytes, commonly encountered with low-level I/O: . data = b&#39;Hello World r n&#39; len(data) # 13 data[0:5] # b&#39;Hello&#39; data.replace(b&#39;Hello&#39;, b&#39;Cruel&#39;) # b&#39;Cruel World r n&#39; data[0] # 72 (ASCII code for &#39;H&#39;) text = data.decode(&#39;utf-8&#39;) # bytes -&gt; text data = text.encode(&#39;utf-8&#39;) # text -&gt; bytes . The ‘utf-8’ argument specifies a character encoding. Other common values include ‘ascii’ and ‘latin1’. . Another example for handling Chinese unicode characters: . s = u&#39;好&#39; print(s) # 好 b = s.encode(&quot;utf-8&quot;) # unicode str to byte str. &quot;utf-8&quot; is default print(b) # b&#39; xe5 xa5 xbd&#39; ch = b.decode() # byte str to unicode str Chinese print(ch) # 好 . For full details. check out this article. . Raw strings . Raw strings are string literals with an uninterpreted backslash. . &gt;&gt;&gt; rs = r&#39;c: newdata test&#39; # Raw (uninterpreted backslash) &gt;&gt;&gt; rs &#39;c: newdata test&#39; . The string is the literal text enclosed inside, exactly as typed. This is useful in situations where the backslash has special significance. Example: filename, regular expressions, etc. . f-Strings (python3.6 or newer) . A string with formatted expression substitution. . &gt;&gt;&gt; name = &#39;IBM&#39; &gt;&gt;&gt; shares = 100 &gt;&gt;&gt; price = 91.1 &gt;&gt;&gt; a = f&#39;{name:&gt;10s} {shares:10d} {price:10.2f}&#39; &gt;&gt;&gt; a &#39; IBM 100 91.10&#39; &gt;&gt;&gt; b = f&#39;Cost = ${shares*price:0.2f}&#39; &gt;&gt;&gt; b &#39;Cost = $9110.00&#39; &gt;&gt;&gt; . Formatting . d Decimal integer b Binary integer x Hexadecimal integer f Float as [-]m.dddddd e Float as [-]m.dddddde+-xx g Float, but selective use of E notation s String c Character (from integer) . Common modifiers . :&gt;10d Integer right aligned in 10-character field :&lt;10d Integer left aligned in 10-character field :^10d Integer centered in 10-character field :0.2f Float with 2 digit precision . Some tips on lists . l.sort() sorts l in-place and doesn’t return anything. sorted(l) keeps l unchanged and returns a new list. . Sort by descending order: l.sort(reverse=True). . Lists have builtin method l.count(). . Caution: Be cautious whenever doing something like l = [obj] * 5. This means this list has 5 of the same object. Once obj is updated, all the elements in the list are updated. . enumerate: use enumerate more in loops, it gets the index and the item. . names = [&#39;Elwood&#39;, &#39;Jake&#39;, &#39;Curtis&#39;] for i, name in enumerate(names): # Loops with i = 0, name = &#39;Elwood&#39; # i = 1, name = &#39;Jake&#39; # i = 2, name = &#39;Curtis&#39; . enumerate(sequence [, start = 0]), start is optional. Example: for lineno, line in enumerate(f, start=1):.... . Files: common idioms . Read an entire file all at once as a string. . with open(&#39;foo.txt&#39;, &#39;rt&#39;) as file: data = file.read() # `data` is a string with all the text in `foo.txt` . Read a file line-by-line by iterating. . with open(filename, &#39;rt&#39;) as file: for line in file: # Process the line . Write a file, . with open(&#39;outfile&#39;, &#39;wt&#39;) as out: out.write(&#39;Hello World n&#39;) ... . Redirect the print function, . with open(&#39;outfile&#39;, &#39;wt&#39;) as out: print(&#39;Hello World&#39;, file=out) ... . Catching and handling exceptions . Exceptions can be caught and handled. . To catch, use the try - except statement. . for line in f: fields = line.split() try: shares = int(fields[1]) except ValueError: print(&quot;Couldn&#39;t parse&quot;, line) ... . It is often difficult to know exactly what kinds of errors might occur in advance depending on the operation being performed. For better or for worse, exception handling often gets added after a program has unexpectedly crashed (i.e., “oh, we forgot to catch that error. We should handle that!”). . To raise an exception, use the raise statement. . raise RuntimeError(&#39;What a kerfuffle&#39;) . This will cause the program to abort with an exception traceback. Unless caught by a try-except block. . Data structures . Tuples look like read-only lists. However, tuples are most often used for a single item consisting of multiple parts. Lists are usually a collection of distinct items, usually all of the same type. . A dictionary is mapping of keys to values. It’s also sometimes called a hash table or associative array. . To delete a value in a dictionary, use the del statement. del s[&#39;date&#39;]. . Set default value if key doesn’t exist. . name = d.get(key, default) . Dict and set are both unordered. . collections . There’s collections which has . ChainMap, namedtuple, deque, Counter, OrderedDict, defaultdict, UserDict, UserList, UserString . Counter: Counting into buckets . portfolio = [ (&#39;GOOG&#39;, 100, 490.1), (&#39;IBM&#39;, 50, 91.1), (&#39;CAT&#39;, 150, 83.44), (&#39;IBM&#39;, 100, 45.23), (&#39;GOOG&#39;, 75, 572.45), (&#39;AA&#39;, 50, 23.15) ] from collections import Counter total_shares = Counter() for name, shares, price in portfolio: total_shares[name] += shares total_shares[&#39;IBM&#39;] # 150 . One-many mapping: defaultdict . defaultdict ensures that every time you access a key you get a default value. . If we want to group the following list of tuples into key value pairs where key is name and value is a list of (share, price) tuples and default to an empty list, we do the following. . portfolio = [ (&#39;GOOG&#39;, 100, 490.1), (&#39;IBM&#39;, 50, 91.1), (&#39;CAT&#39;, 150, 83.44), (&#39;IBM&#39;, 100, 45.23), (&#39;GOOG&#39;, 75, 572.45), (&#39;AA&#39;, 50, 23.15) ] from collections import defaultdict holdings = defaultdict(list) for name, shares, price in portfolio: holdings[name].append((shares, price)) holdings[&#39;IBM&#39;] # [ (50, 91.1), (100, 45.23) ] . deque: queue and stack . Problem: keep the last N elements. . from collections import deque history = deque(maxlen=N) with open(filename) as f: for line in f: history.append(line) ... . zip function . The zip function takes multiple sequences and makes an iterator that combines them. . columns = [&#39;name&#39;, &#39;shares&#39;, &#39;price&#39;] values = [&#39;GOOG&#39;, 100, 490.1 ] pairs = zip(columns, values) # (&#39;name&#39;,&#39;GOOG&#39;), (&#39;shares&#39;,100), (&#39;price&#39;,490.1) . To get the result you must iterate. You can use multiple variables to unpack the tuples as shown earlier. . for column, value in pairs: ... . A common use of zip is to create key/value pairs for constructing dictionaries. . d = dict(zip(columns, values)) . Object model . In Python, everything is an object. There’s danger dealing with mutable objects. . Assignment operations never make a copy of the value being assigned. All assignments are merely reference copies (or pointer copies if you prefer). . Remember: Variables are names, not memory locations. . Identity and References . Use the is operator to check if two values are exactly the same object. . &gt;&gt;&gt; a = [1,2,3] &gt;&gt;&gt; b = a &gt;&gt;&gt; a is b True . is compares the object identity (an integer). The identity can be obtained using id(). . &gt;&gt;&gt; id(a) 3588944 &gt;&gt;&gt; id(b) 3588944 . Shallow copies: beware! . Lists and dicts have methods for copying. . &gt;&gt;&gt; a = [2,3,[100,101],4] &gt;&gt;&gt; b = list(a) # Make a copy &gt;&gt;&gt; a is b False . It’s a new list, but the list items are shared!! . &gt;&gt;&gt; a[2].append(102) &gt;&gt;&gt; b[2] [100,101,102] &gt;&gt;&gt; &gt;&gt;&gt; a[2] is b[2] True . . Deep copies . Sometimes you need to make a copy of an object and all the objects contained within it. . You can use the copy module for this: copy.deepcopy() . &gt;&gt;&gt; a = [2,3,[100,101],4] &gt;&gt;&gt; import copy &gt;&gt;&gt; b = copy.deepcopy(a) &gt;&gt;&gt; a[2].append(102) &gt;&gt;&gt; b[2] [100,101] &gt;&gt;&gt; a[2] is b[2] False . Type checking . How to tell if an object is a specific type. . if isinstance(a, list): print(&#39;a is a list&#39;) . Checking for one of many possible types. . if isinstance(a, (list,tuple)): print(&#39;a is a list or tuple&#39;) . Caution: Don’t go overboard with type checking. It can lead to excessive code complexity. Usually you’d only do it if doing so would prevent common mistakes made by others using your code. . Everything is an object . Numbers, strings, lists, functions, exceptions, classes, instances, etc. are all objects. . It means that all objects that can be named can be passed around as data, placed in containers, etc., without any restrictions. . There are no special kinds of objects. Sometimes it is said that all objects are “first-class”. . &gt;&gt;&gt; import math &gt;&gt;&gt; items = [abs, math, ValueError ] &gt;&gt;&gt; items [&lt;built-in function abs&gt;, &lt;module &#39;math&#39; (builtin)&gt;, &lt;type &#39;exceptions.ValueError&#39;&gt;] &gt;&gt;&gt; items[0](-45) 45 &gt;&gt;&gt; items[1].sqrt(2) 1.4142135623730951 &gt;&gt;&gt; try: x = int(&#39;not a number&#39;) except items[2]: print(&#39;Failed!&#39;) Failed! . Program organization . Function best practices . Define functions at the top of a script, put all of the code related to a single task all in one function. . Functions need to be modular and predictable. . Write docstring, describe what the function does in one sentence, and add information per argument. . Add optional type hints to function definitions. . def read_prices(filename: str) -&gt; dict: &#39;&#39;&#39; Read prices from a CSV file of name,price data &#39;&#39;&#39; prices = {} with open(filename) as f: f_csv = csv.reader(f) for row in f_csv: prices[row[0]] = float(row[1]) return prices . These type hints do nothing operationally. Yet IDEs can use them to give hints. . When calling a function with optional arguments, use keywords instead of just True or False. . parse_data(data, False, True) # ????? NO parse_data(data, ignore_errors=True) parse_data(data, debug=True) parse_data(data, debug=True, ignore_errors=True) . Always give short but meaningful names. . Regarding variable scope, remember: All assignments in functions are local. . If you must update a global variable, use the global keyword. . name = &#39;Dave&#39; def spam(): global name name = &#39;Guido&#39; # Changes the global name above . Actually, avoid global variable entirely if you can. If you want to modify a state outside of a function, use a class instead. . Arguments passed into functions are references. If a mutable object get passed in and you use its method to modify it, it will be modified in-place. . But keep in mind, reassignment to an old variable name inside a function only modifies the name in the local scope, it doesn’t change the original object: . def foo(items): items.append(42) # Modifies the input object in-place because .append() a = [1, 2, 3] foo(a) print(a) # [1, 2, 3, 42] # VS def bar(items): items = [4,5,6] # Changes local `items` variable to point to a different object b = [1, 2, 3] bar(b) print(b) # [1, 2, 3] . Error handling . Python doesn’t check data type or values, if there’s any error, it will appear at run time as an exception. . # raise def authenticate(name): if name not in authorized: raise RuntimeError(f&#39;{name} not authorized&#39;) # try - except try: authenticate(username) except RuntimeError as e: # error msg f&#39;{name} not authorized&#39; is in `e` print(e) # HANDLE THE EXCEPTION HERE!!! ... statements # Resumes execution here after handling exception statements # And continues here ... . Some Python builtin exceptions for indicating what is wrong: . ArithmeticError AssertionError EnvironmentError EOFError ImportError IndexError KeyboardInterrupt KeyError MemoryError NameError ReferenceError RuntimeError SyntaxError SystemError TypeError ValueError ... . Handling multiple errors in different ways: . try: ... except LookupError as e: ... except RuntimeError as e: ... except IOError as e: ... except KeyboardInterrupt as e: ... . Grouping them and handle the same way: . try: ... except (IOError,LookupError,RuntimeError) as e: ... . To catch all errors: . try: ... except Exception: # DANGER. See below print(&#39;An error occurred&#39;) . This is a bad idea because you’ll have no idea why it failed. . Recommended approach: as e . try: go_do_something() except SomeSpecificException as e: print(&#39;Computer says no. Reason :&#39;, e) raise . Using raise allows you to take action (e.g. logging) and pass the exception to the caller. . The finally clause: . lock = Lock() ... lock.acquire() try: ... finally: lock.release() # this will ALWAYS be executed. With and w/o exception. . Finally is commonly used to safely manage resources (especially locks, files, etc.). . However, the best practice is to use with and avoid this approach. . lock = Lock() with lock: # lock acquired ... # lock released . Modules . Any .py file is a module. The import statement loads and executes a module. . A module is a collection of named values and is sometimes said to be a namespace. The names are all of the global variables and functions defined in the source file. After importing, the module name is used as a prefix. Hence the namespace. . import foo a = foo.grok(2) b = foo.spam(&#39;Hello&#39;) ... . The module name is directly tied to the file name (foo -&gt; foo.py). . Modules are isolated. foo.x and bar.x are different: . # foo.py x = 42 # bar.py x = 37 . Global variables are always bound to the enclosing module (same file). Each source file is its own little universe. . Module execution . When a module is imported, all of the statements in the module execute one after another until the end of the file is reached. The contents of the module namespace are all of the global names that are still defined at the end of the execution process. If there are scripting statements that carry out tasks in the global scope (printing, creating files, etc.) you will see them run on import. . Module Loading . Each module loads and executes only once. Note: Repeated imports just return a reference to the previously loaded module. . sys.modules is a dict of all loaded modules. . &gt;&gt;&gt; import sys &gt;&gt;&gt; sys.modules.keys() [&#39;copy_reg&#39;, &#39;__main__&#39;, &#39;site&#39;, &#39;__builtin__&#39;, &#39;encodings&#39;, &#39;encodings.encodings&#39;, &#39;posixpath&#39;, ...] . Caution: A common confusion arises if you repeat an import statement after changing the source code for a module. Because of the module cache sys.modules, repeated imports always return the previously loaded module – even if a change was made! The safest way to load modified code into Python is to quit and restart the interpreter/kernel!! . Locating modules . Python consults a path list (sys.path) when looking for modules. The current working directory is usually first. . &gt;&gt;&gt; import sys &gt;&gt;&gt; sys.path [ &#39;&#39;, &#39;/usr/local/lib/python36/python36.zip&#39;, &#39;/usr/local/lib/python36&#39;, ... ] . sys.path contains the search paths. You can manually adjust if you need to: . import sys sys.path.append(&#39;/project/foo/pyfiles&#39;) . Paths can also be added via environment variables. . % env PYTHONPATH=/project/foo/pyfiles python3 Python 3.6.0 (default, Feb 3 2017, 05:53:21) [GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.38)] &gt;&gt;&gt; import sys &gt;&gt;&gt; sys.path [&#39;&#39;,&#39;/project/foo/pyfiles&#39;, ...] . As a general rule, it should not be necessary to manually adjust the module search path. However, it sometimes arises if you’re trying to import Python code that’s in an unusual location or not readily accessible from the current working directory. . Main module . In many programming languages, there is a concept of a main function or method. This is the first function that executes when an application is launched. . Python has no main function or method. Instead, there is a main module. The main module is the source file that runs first. Whatever file you give to the interpreter at startup becomes main. It doesn’t matter the name. . Any Python file can either run as main or as a library import: . bash % python3 prog.py # Running as main import prog # Running as library import . In both cases, __name__ is the name of the module. However, it will only be set to __main__ if running as main. . Usually, you don’t want statements that are part of the main program to execute on a library import. So, it’s common to have an if-check in code that might be used either way. . # prog.py ... if __name__ == &#39;__main__&#39;: # Running as the main program ... statements ... . Here is common Python program template: . # prog.py # Import statements (libraries) import modules # Functions def spam(): ... def blah(): ... # Main function def main(): ... if __name__ == &#39;__main__&#39;: main() . When used as a CLI tool, like bash % python report.py portfolio.csv prices.csv, the list of arguments is in sys.argv: . # In the previous bash command sys.argv # [&#39;report.py, &#39;portfolio.csv&#39;, &#39;prices.csv&#39;] . sys.stdout, sys.stderr and sys.stdin are files that work the same way as normal files. By default, print is directed to sys.stdout. Input is read from sys.stdin. Tracebacks and errors are directed to sys.stderr. . stdio could be connected to terminals, files, pipes, etc. . bash % python3 prog.py &gt; results.txt # or bash % cmd1 | python3 prog.py | cmd2 . Environment variables . Environment variables are set in the shell. . bash % setenv NAME dave bash % setenv RSH ssh bash % python prog.py . os.environ is a dictionary that contains these values. . import os name = os.environ[&#39;NAME&#39;] # &#39;dave&#39; . Program Exit . Program exit is handled through exceptions. A non-zero exit code indicates an error. . raise SystemExit raise SystemExit(exitcode) raise SystemExit(&#39;Informative message&#39;) # Or import sys sys.exit(exitcode) . The !# line . On Unix, the #! shebang line can launch a script as Python. Add the following to the first line of your script file. . #!/usr/bin/env python3 . It requires the executable permission. . bash % chmod +x prog.py # Then you can execute bash % prog.py ... output ... . Note: The Python Launcher on Windows also looks for the #! line to indicate language version. . Design generic and flexible functions: Duck Typing . Compare the following two versions of a function . # VERSION 1 # Provide a filename def read_data(filename): records = [] with open(filename) as f: for line in f: ... records.append(r) return records d = read_data(&#39;file.csv&#39;) # VERSION 2 # Provide lines def read_data(lines): records = [] for line in lines: ... records.append(r) return records with open(&#39;file.csv&#39;) as f: d = read_data(f) . Version 2 is better because it’s more generic, it takes in any iterable. . Duck Typing is a computer programming concept to determine whether an object can be used for a particular purpose. It is an application of the duck test. . If it looks like a duck, swims like a duck, and quacks like a duck, then it probably is a duck. . Code libraries are often better served by embracing flexibility. Don’t restrict your options. With great flexibility comes great power. . Reference . https://dabeaz-course.github.io/practical-python/Notes .",
            "url": "http://blog.logancyang.com/note/python/2020/06/29/python-basics.html",
            "relUrl": "/note/python/2020/06/29/python-basics.html",
            "date": " • Jun 29, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "[From NAND to TETRIS] Computer Architecture 101 Part I: Logic Gates, ALU and Memory",
            "content": "Ever wondered how a computer executes your code with those tiny silicon thingys? If it sounds like a mystery to you, you need to read this article. . A computer system is a multiple-layer abstraction. You worry about what you want to do using interfaces that are abstracted away from you. Getting to know what the abstractions are and having an rough idea of how they work under the hood is an important and often overlooked prerequisite for a good coder. If you don’t have this knowledge as a data scientist or software developer, you will feel you work on top of a floating and unreliable foundation. . In this article, I will introduce all the concepts in an easily consumable manner by walking through the process of building a general purpose computer called Hack from the ground up using first principles. . . After this exercise, you will walk away more confident and eliminate the insecurities of not knowing the foundational knowledge of computers. . Ready? Let’s get started and go through the levels of abstractions in the computer system. . . Level 1. Boolean operations and logic gates . . Booleans functions -&gt; truth table. . . Boolean algebra laws . . How to construct a function from basic operations? (how to construct hardware) . Find all the 1s in truth table, write an expression for each, and OR them together! We can further simplify the expression. However, finding the shortest expression is NP hard. . Any boolean function can be constructed with AND and NOT operations. . OR can be constructed with AND and NOT. . . The amazing NAND operation . Any boolean function can be constructed with NAND operations. . Because AND and NOT can be constructed with only the NAND gate. . . # Def: NAND x NAND y = NOT(x AND y) # NAND =&gt; NOT NOT(x) = x NAND x # NAND =&gt; AND x AND y = NOT(x NAND y) . Logic gates are physical chips consisting of transistors that actually implement these boolean operations. We can use elementary logic gates such as AND, OR, NAND to construct composite gates. . Electrical engineers are responsible for constructing these gates with circuits and transistors. These chips have very clear specifications. . (wikipedia and gate) . We are not going to dive into the design of the circuit. We take the specifications of inputs and outputs and use them to build more complex logic. There is a type of language called Hardware Description Language (HDL) that can be used in this process. The two most popular HDLs are VHDL and Verilog, and there are others as well. . Designing composite gates from elementary ones takes experience. Here is an example of XOR built with AND, OR and NOT. . . We can write HDL code and load it into a hardware simulator, run test scripts to make sure it works. . . A hardware construction project usually involves a system architect and some developers. The system architect breaks down the task into an overall chip API and smaller individual chips, and the developers build them using HDL. . Multi-bit buses: an array of bits as one entity . When we manipulate many bits together, like adding two 16-bit integers, we can think of the bits as groups. Each group or array of bits is called “bus”. HDL provides ways to manipulate buses as arrays. . . These buses are indexed from right to left. Right means the least significant bit. . Programmable elementary gates . The mutiplexor gate mux is an example of programmable gates. You give it a “selected bit” sel and it selects one of the inputs to be the output. This is a foundamental operations that exists in all kinds of programs. . . . The demultiplexor gate dmux is the inverse of mux. . . A concrete example of an application using mux and dmux is a single communication line that interleaves two (or more) messages together! . . Level 2. Boolean arithmetic and Arithmetic Logic Unit (ALU) . Doing binary addition is similar to decimal addition except that we carry over 1 to the next digit one position reaches 2 and not 10. . For an 8-bit binary number, we call “8” the word size. Anything outside the range of 8-bit representation is going to cause “overflow” . What the computer does when faced with overflow is just to ignore the overflow and truncate the result back to the word size. . . Building an adder . First we look at a “half adder”. It takes in 2 bits and outputs a sum bit and a carry bit. The carry bit is 1 when the sum reaches 2. . . Then we can build a “full adder” that takes in 2 bits and a carry bit from a previous addition, and outputs a sum bit and a carry bit. . . . With a half adder for the least significant bit, and several full adders, we can build a multi-bit adder, say, for 16-bit binary numbers. . . . Negative numbers . We could use the first bit as the sign and the remaining bits for the actual number. But this approach has problems such as -0 and we need extra logic to handle subtraction. . The approach we actually use is called the 2’s complement. . . Negative addition . Negative addition comes for free because we throw away the overflowing bit, i.e. do a modulo $2^n$. . . Negative subtraction: solve negation first . We use a math trick, . 2n−x=1+(2n−1)−x2^n - x = 1 + (2^n - 1) - x2n−x=1+(2n−1)−x . We use it for these reasons: . Since $2^n - x = -x$ in our 2’s complement representation, the left hand side is -x. | $(2^n - 1)$ in binary is 11111...1, just n 1’s, subtracting a binary number x from it is easy - just flip all its bits! | Now we have $(2^n - 1) - x$ in binary, adding 1 is trivial. | . Arithmetic Logic Unit (ALU) . The famous Von Neumann architecture . . ALU performs a function on input bits . . For the Hack computer we are building, we define the Hack ALU as follows, . . The pins at the top are the control bits. . The output of the ALU is determined completely by the truth table below, . . There are many more functions we can compute, but we choose these ones for Hack. . The control bits are directives that ALU examines and executes from left to right sequentially. . if zx then x = 0, zero all bits of x. | if nx then x != x, flip all bits of x. | if zy then y = 0, zero all bits of y. | if ny then y != y, flip all bits of y. | if f then out = x + y, else out = x &amp; y | if no then out != out, flip all bits of out. | . You can check one operation, i.e. a row in the following truth table and verify that these sequential control directives output the right output for the desired operation on the inputs. . . We also have output control bits zr and ng defined as follows, . . Why do we need these output control bits? The reason will become clear when we have the big picture of computer architecture. . The Hack ALU is simple, elegant and easy to implement. . Leonardo da Vinci: Simplicity is the ultimate sophistication. . However, keep in mind that this is an extremely simplified version of the ALU. The real ALU in computers are much more complicated. . Comments . Hardware / software tradeoff . There is a tradeoff between the hardware and software. For example, we can let the hardware do multiplication and division instead using software. A complex hardware is faster for operations but more costly to design and manufacture. It’s the freedom of the hardware designer to decide whether to move some operations to software. . Unit testing . You might know that in software development we have unit tests. Actually, the idea comes from hardware development since we can isolate one logic unit and test whether it works correctly with its defined interfaces. . Unit tests conceptually break a program into discrete pieces and test each piece in isolation, while integration tests make sure that the pieces are working properly together as a whole. Unit tests are generally small and don’t take much time to write because they only test a small section of code. . Level 3. Memory . Time . Now we will learn how the computer executes one thing after another, i.e. how it perceives time. . The way we let a computer perceive time is to use some kind of digital oscillator as a discrete time clock. This way we convert physical time into discrete time. . Since the hardware uses voltage to represent bit state 0 and 1, and it takes time for the hardware to stablize after a state transition, we design the unit time step in the discrete clock to be slightly bigger than the time it takes to stablize. We then sample the stable state to be the state of that time unit. . A time unit is at the atomic level, the computer only knows one state within one time unit. . . Combinatorial vs. sequential logic . Combinatorial logic just means that the output for a time unit only depends on the input in that time unit. . out[t] = function(in[t]) . Sequential logic means we use the same wire to store the bit, and current step’s output depends on the last time step. . state[t] = function(state[t-1]) . This is the prototype of the iterator. With the sequential logic we can operate with time as input to our function. . . Flip Flops: Hardware that implements the sequential logic . Previously we had logic gates and ALUs that can compute a variety of operations, but we are missing one key piece to enable sequential logic: the hardware to remember the state from time t-1 to time t. How to do that? . We need something called the Clocked Data Flip Flop. Its output is input shifted one step forward. . . The little triangle in the diagram means it has a time-dependent chip. The combinatorial chips get the outputs instantaneously, while the sequential logic chips are time-dependent. . Conceptually, with the NAND gate and the Data Flip Flop as a foundation, we can build everything needed in a computer! . We are not going to describe how the flip flops are built physically here. The digital circuit is quite clever and elegant though. If you are interested, read about it here. . 1-bit register . We can build a chip by using the D Flip Flop that remembers the last input state when the load state is 1. . . The logic is: . if load(t-1): out[t] = in[t-1] else: out[t] = out[t-1] . A load of 1 at t-1 means we need to remember the input at t-1 from now on until a next load of 1 regardless of how input changes during this period. . How to implement a 1-bit register using the D Flip Flop? Recall the mutiplexor (select one of two inputs based on a “select” bit). . . With the 1-bit register, we can start to build the memory unit! . The Memory Unit . There are different types of memory. The most important type is the RAM. It stores both data and instructions. . It is a volatile device meaning it depends on an external power supply to store information. Physically it is not like the disk or ROM (read-only memory), it clears everything whenever the computer is disconnected from power while in the disk or ROM, info is persisted. . . We can build an N-bit register by putting N 1-bit registers side by side. Here we talk about 16-bit registers without loss of generality. The register’s state is the value stored in the register. To lookup the state, we just need to probe its output pins. . To store a new input state v, we set load to 1 at that time step. . The RAM abstraction: A sequence of n addressable registers with addresses 0 to n-1. . Important: At any given time, ONLY ONE REGISTER in the RAM is selected! . Since we have n addressable registers, we need a binary number to represent the address. Turning n to a binary number, we say it has k bits. So k = log2(n). . . RAM is a sequential chip with a clocked behavior. . Why is it called Random Access Memory? . A: Because regardless of how many registers the memory unit has, be it 1 million or 1 billion, it takes exactly the same access time to access any one of the registers with a given address! . Some different RAM chips: . . The Counter . A Program Counter (PC) is a chip (hardware device) that realizes the following 3 abstractions: . Reset: fetch the first instruction, PC = 0 | Next: fetch the next instruction, PC++ | Goto: fetch instruction n, PC = n | Here is the logic the Counter does: . if reset[t] == 1: out[t+1] = 0 # reset counter to 0 elif load[t] == 1: out[t+1] = in[t] # set counter to input value at t elif inc[t] == 1: out[t+1] = out[t] + 1 # increment counter by 1 else: out[t+1] = out[t] # maintain the last value . . ROM vs. RAM vs. Flash vs. Cache memory . The ROM (read-only memory) is an involatile device that keeps information persisted even without external power supply. For example, it is used in the computer booting process for this reason. . Another technology is flash memory. It combines the good things of both ROM and RAM, it doesn’t need external power supply to persist data, and is writable. . Cache memory is a small and expensive memory that is close to the processor. It has very high performance. There is a hierarchy of cache memories, the closer to the processor, the faster, smaller and more expensive it gets. . Go to Part II . Reference . Coursera From NAND To TETRIS .",
            "url": "http://blog.logancyang.com/note/compsci/2020/06/29/computer-architecture-101.html",
            "relUrl": "/note/compsci/2020/06/29/computer-architecture-101.html",
            "date": " • Jun 29, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "[Python Essentials] Digging into Package and Environment Management to Find the Best Approach",
            "content": "This post is for clarifying several points about Python package and environment management. . pip vs. conda? | virtualenv vs. conda env? | How to choose the package and environment tools for Python development? | . pip vs. conda . Difference 1 . pip is a Python package manager. . conda is a language-agnostic cross-platform environment manager. . Package management and environment management are two different things. So pip and conda are different things and do not compete with each other. . Pip installs Python software packaged as wheels or source distributions. Source distributions may require that the system have compatible compilers, and possibly libraries, installed before invoking pip to succeed. . Conda packages are binaries. There is never a need to have compilers available to install them. Additionally conda packages are not limited to Python software. They may also contain C or C++ libraries, R packages or any other software. . Before using pip, a Python interpreter must be installed via a system package manager or by downloading and running an installer. Conda on the other hand can install Python packages as well as the Python interpreter directly. . Difference 2 . conda has the ability to create isolated environments, pip needs virtualenv or venv to create a virtual environment. . (poetry is a tool that wraps pip and virtualenv together and is recommended for application development. Will discuss in later sections.) . Difference 3 . When installing packages, pip installs dependencies in a recursive, serial loop. No effort is made to ensure that the dependencies of all packages are fulfilled simultaneously. This can lead to environments that are broken in subtle ways, if packages installed earlier in the order have incompatible dependency versions relative to packages installed later in the order. In contrast, conda uses a satisfiability (SAT) solver to verify that all requirements of all packages installed in an environment are met. This check can take extra time but helps prevent the creation of broken environments. As long as package metadata about dependencies is correct, conda will predictably produce working environments. . . Now we know the difference between pip and conda, but choosing which to use also depends on the way they work with virtual environments. I will discuss that in later sections. Now let’s understand some basics how Python installs pacakges, and check the resulting package locations after pip install and conda install. . Where does Python install packages (mac system Python, DON’T DO THIS) . By default, without a virtual environment, all python and pip commands will use the default executables, usually your system Python install. . It is strongly recommended to keep your system Python clean of unnecessary site packages by using virtual environments. . Otherwise, over time, you will add lots of things to your system packages and things might conflict and cause problems. Using an isolated environment for each project ensures easy reproducability and reduced conflict. . DO NOT install into your global Python interpreter! ALWAYS use an environment when developing locally! . To see what directories are being used to search for packages, invoke the site module directly by running python -m site outside virtual environments: . $ python -m site sys.path = [ &#39;&lt;current_dir&gt;&#39;, &#39;/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python27.zip&#39;, &#39;/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7&#39;, &#39;/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plat-darwin&#39;, &#39;/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plat-mac&#39;, &#39;/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plat-mac/lib-scriptpackages&#39;, &#39;/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-tk&#39;, &#39;/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-old&#39;, &#39;/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload&#39;, &#39;/Library/Python/2.7/site-packages&#39;, &#39;/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python&#39;, &#39;/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/PyObjC&#39;, ] USER_BASE: &#39;/Users/&lt;username&gt;/Library/Python/2.7&#39; (doesn&#39;t exist) USER_SITE: &#39;/Users/&lt;username&gt;/Library/Python/2.7/lib/python/site-packages&#39; (doesn&#39;t exist) ENABLE_USER_SITE: True . The Python at /System/Library/Frameworks/... is MacOS’s pre-installed Python. . Run pip show &lt;package_name&gt; to see where it’s installed. For system Python, it’s something like: . $ pip show setuptools Name: setuptools Version: 41.0.1 Summary: Easily download, build, install, upgrade, and uninstall Python packages Home-page: https://github.com/pypa/setuptools Author: Python Packaging Authority Author-email: distutils-sig@python.org License: UNKNOWN Location: /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python Requires: Required-by: . You can also run pip list -v to check install locations. . pip install vs. conda install: where do they install the packages . If you run pip install in a virtual environment, by default it will install into . &lt;project_root&gt;/&lt;virtualenv_name&gt;/lib/&lt;python_ver&gt;/site-packages/ . For conda, it always installs packages into a conda environment. Without an activated conda environment, conda install will install into the base conda environment. With a conda environment, e.g. testenv activated, by default it installs to . /home/&lt;user&gt;/anaconda3/envs/testenv/ . (conda envs are located in a centralized path by default. Later I will mention the pros and cons of this approach.) . We can use conda list to check the path of the current environment and the packages installed in it. . Later I will compare the two and choose my preferred way to manage packages and environments. . virtualenv vs conda env . With pip vs. conda out of the way, a more valid comparison is virtualenv vs. conda environments. When to use which? To answer that, we need to really understand what virtual environment is and how they achieve isolation. . Always use a virtual environment! . A virtual environment is a directory that contains its own installation of Python and its own set of libraries (site packages). It solves problems such as conflicting requirements for two applications by having an isolated environment for each application. . How does virtualenv achieve isolation . Notice the difference between the first path in $PATH before and after the activation: . $ echo $PATH /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin: $ source env/bin/activate (env) $ echo $PATH /Users/&lt;username&gt;/&lt;project&gt;/venv/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin: . This raises the following questions: . What’s the difference between these two executables anyway? | How is the virtual environment’s Python executable able to use something other than the system’s site-packages? | . This can be explained by how Python starts up and where it is located on the system. There actually isn’t any difference between these two Python executables. It’s their directory locations that matter. . When Python is starting up, it looks at the path of its binary. In a virtual environment, it is actually just a copy of, or symlink to, your system’s Python binary. It then sets the location of sys.prefix and sys.exec_prefix based on this location, omitting the bin portion of the path. . The path located in sys.prefix is then used for locating the site-packages directory by searching the relative path lib/pythonX.X/site-packages/, where X.X is the version of Python you’re using. . In our example, the binary is located at . /Users/&lt;username&gt;/&lt;project&gt;/venv/bin . which means sys.prefix would be . /Users/&lt;username&gt;/&lt;project&gt;/venv/ . and therefore the site-packages directory used would be . /Users/&lt;username&gt;/&lt;project&gt;/venv/lib/pythonX.X/site-packages . Finally, this path is stored in the sys.path array, which contains all of the locations where a package can reside. . Considerations for picking virtualenv or conda env . conda env can replace virtualenv, but there are some differences. There is a great answer from stackoverflow, I quote and arrange into the points below. . Where do the envs live: central vs. per-project . By default conda prefers to manage a list of environments for you in a central location, i.e. . /home/&lt;user&gt;/anaconda3/envs/ . whereas virtualenv makes a folder in the current directory. . conda (centralized) makes sense if you are e.g. doing machine learning and just have a couple of broad environments that you use across many projects and want to jump into them from anywhere. | virtualenv (per project folder) makes sense if you are doing projects that have completely different sets of lib requirements that really belong more to the project itself. | . Environment size (favors virtualenv) . The empty environment that Conda creates is about 122MB whereas the virtualenv’s is about 12MB, so that’s another reason you may prefer not to scatter Conda environments around everywhere. . Prefix for shell display (favors virtualenv) . Finally, another superficial indication that conda prefers its centralized envs is that (again, by default) if you do create a conda env in your own project folder and activate it the name prefix that appears in your shell is the (way too long) absolute path to the folder. You can fix that by giving it a name, but virtualenv does the right thing by default. . Weird question of mine: what if virtualenv and conda env are both activated? . I did this experiment by activating one before the other and check which python, and guess what, the result is that the active environment is whichever environment activated last! . Some tips . Avoid using pip by itself. Using python -m pip will always guarantee you are using the pip associated with that specific python being called, instead of potentially calling a pip associated with a different python. | Sometimes there is a pip3 to go with python3 to differentiate python 2 executables python/pip. | Sometimes Linux distributions require you to install virtualenv as a separate package. For example sudo apt install python3-virtualenv. | You should never copy or move around virtual environments. Always create new ones. Ignore the virtual environment directories from repositories. For example, .gitignore them. | . pyenv . pyenv is a mature tool for installing and managing multiple Python versions on macOS. You can install it with brew. If you’re using Windows, you can use pyenv-win. After you’ve got pyenv installed, you can install multiple versions of Python into your Python environment with a few short commands: . $ pyenv versions * system $ python --version Python 2.7.10 $ pyenv install 3.7.3 # This may take some time $ pyenv versions * system 3.7.3 . You can manage which Python you’d like to use in your current session, globally, or on a per-project basis as well. pyenv will make the python command point to whichever Python you specify. Note that none of these overrides the default system Python for other applications, so you’re safe to use them however they work best for you within your Python environment: . $ pyenv global 3.7.3 $ pyenv versions system * 3.7.3 (set by /Users/&lt;username&gt;/.pyenv/version) $ pyenv local 3.7.3 $ pyenv versions system * 3.7.3 (set by /Users/&lt;username&gt;/myproj/.python-version) $ pyenv shell 3.7.3 $ pyenv versions system * 3.7.3 (set by PYENV_VERSION environment variable) $ python --version Python 3.7.3 . There is something called pyenv virtualenv. Install it with brew install pyenv-virtualenv and use it with pyenv virtualenv &lt;python version&gt; &lt;project name&gt;. . // Create virtual environment $ pyenv virtualenv 3.7.3 my-env // Activate virtual environment $ pyenv activate my-env // Exit virtual environment (my-env)$ pyenv deactivate $ pyenv virtualenv 3.7.3 proj1 $ pyenv virtualenv 3.7.3 proj2 $ cd /Users/&lt;username&gt;/proj1 $ pyenv local proj1 (proj1)$ cd ../proj2 $ pyenv local proj2 (proj2)$ pyenv versions system 3.7.3 3.7.3/envs/proj1 3.7.3/envs/proj2 proj1 * proj2 (set by /Users/&lt;username&gt;/proj2/.python-version) . It can set the default environment for a directory! This way you don’t have to remember which environment to use. But the caveat is that it has conda-like centralized envs. You can find them by the command pyenv virtualenvs. . For more details, check out https://realpython.com/intro-to-pyenv/ and https://github.com/pyenv/pyenv-virtualenv. . My setup 2020 . Tools: . pyenv, virtualenv for library development. | poetry for application development. poetry wraps pip and virtualenv to provide a unified method for working with these environments. | . I use virtualenv instead of pyenv-virtualenv to explicitly have a folder for the virtual environment in the project directory because it is the clearest way for me to manage virtual environments for different projects. . conda can be used for one-off experiments with broad machine learning environments. . For a comparison of library vs. application development and more Python packaging considerations, check out this great article: The Packaging Gradient. . Application packaging must not be confused with library packaging. Python is for both, but pip is for libraries. . Quick intro to poetry . Use poetry instead of pipenv for Python application development. pipenv is not actively issuing updates as of 2020. . poetry addresses additional facets of package management, including creating and publishing your own packages. After installing poetry, you can use it to create a new project: . $ poetry new myproj Created package myproj in myproj $ ls myproj/ README.rst myproj pyproject.toml tests . Similarly to how pipenv creates the Pipfile, poetry creates a pyproject.toml file. This recent standard contains metadata about the project as well as dependency versions: . # Config file [tool.poetry] name = &quot;myproj&quot; version = &quot;0.1.0&quot; description = &quot;&quot; authors = [&quot;Logan Yang &lt;logancyang@gmail.com&gt;&quot;] [tool.poetry.dependencies] python = &quot;^3.7&quot; [tool.poetry.dev-dependencies] pytest = &quot;^3.0&quot; [build-system] requires = [&quot;poetry&gt;=0.12&quot;] build-backend = &quot;poetry.masonry.api&quot; . You can install packages with poetry add (or as development dependencies with poetry add –dev): . $ poetry add requests Using version ^2.22 for requests Updating dependencies Resolving dependencies... (0.2s) Writing lock file Package operations: 5 installs, 0 updates, 0 removals - Installing certifi (2019.6.16) - Installing chardet (3.0.4) - Installing idna (2.8) - Installing urllib3 (1.25.3) - Installing requests (2.22.0) . poetry also maintains a lock file, and it has a benefit over pipenv because it keeps track of which packages are subdependencies. As a result, you can uninstall requests and its dependencies with poetry remove requests. . References . https://www.anaconda.com/blog/understanding-conda-and-pip | https://jakevdp.github.io/blog/2016/08/25/conda-myths-and-misconceptions/ | https://stackoverflow.com/questions/54834579/specific-reasons-to-favor-pip-vs-conda-when-installing-python-packages | https://stackoverflow.com/a/59755292/2280673 | https://realpython.com/effective-python-environment/ | https://realpython.com/python-virtual-environments-a-primer/ | https://packaging.python.org/tutorials/managing-dependencies/ | https://sedimental.org/the_packaging_gradient.html | https://youtu.be/3J02sec99RM | https://youtu.be/o1Vue9CWRxU | .",
            "url": "http://blog.logancyang.com/note/python/2020/06/17/pip-vs-conda.html",
            "relUrl": "/note/python/2020/06/17/pip-vs-conda.html",
            "date": " • Jun 17, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "[Python Essentials] Mental Models of Expert Level Features",
            "content": "Metaclass: problem . Suppose there is an infra team that writes library code for other developers to use, and there is user code for business logics. . Here we have library.py from the infra team, user.py from the user facing team: . # library.py class Base: def foo(self): return &#39;foo&#39; # user.py from library import Base class Derived(Base): def bar(self): return self.foo . Suppose we are on the user facing team and we can’t change the library code. What if the foo method gets removed? We don’t want our bar method to break at runtime, we want to catch this before runtime production environment. How to do this easily? . We could add a line above the Derived class, . assert hasattr(Base, &quot;foo&quot;), &quot;You broke it!&quot; . This enforces a constraint on the Base class. . Now let’s consider the scenario below where we are in the infra team and are responsible for the library code. We need to make sure the business logic team implement the bar method. . # library.py class Base: def foo(self): return self.bar() # user.py from library import Base class Derived(Base): def bar(self): return &#39;bar&#39; . We can’t do the same thing as before assert hasattr(Derived, &quot;bar&quot;), &quot;...&quot;. Try-except also doesn’t work because it only catches the error at runtime, not before. . Python is a “protocol-oriented” language. In C++ and Java, a class definition is not executable code. But in Python it is. . Python has this __build_class__() method that lets you check things at the time when the class is being built. . old_bc = __build_class__ def my_bc(func, name, base=None, **kw): if base is Base: print(&quot;Check if bar method defined&quot;) if base is not None: return old_bc(func, name, base, **kw) return old_bc(func, name, **kw) import builtins builtins.__build_class__ = my_bc &quot;&quot;&quot; Running the code above: Check if bar method defined &quot;&quot;&quot; . This isn’t typically what we do for the purpose of checking whether the library code is going to break when used in user code. This is just to show that in Python we can hook into the processes of building classes, defining functions, importing modules, and do what we want. . This pattern exists but we don’t use it for this purpose, people use 2 fundamental features of Python to solve this problem of enforcing constraints. . The first one is Metaclass. . Metaclass: solution . A metaclass is just a class derived from type class that allows you to intercept derived types. . class BaseMeta(type): def __new__(cls, name, bases, body): print(&#39;BaseMeta.__new__&#39;, cls, name, bases, body) # To prevent user class without `bar` method if not &#39;bar&#39; in body: raise TypeError(&quot;Bad user class&quot;) return super().__new__(cls, name, bases, body) class Base(metaclass=BaseMeta): def foo(self): return self.bar() . Metaclass is way to enforce constraints on the derived classes, e.g. user code, in the base classes, i.e. library code. . Checkout collections.abc’s (doc) abc metaclass which has decorators such as @abstractmethod so we don’t have to write metaclass ourselves! . Decorators . Again, Python is a “live” language as in that function definitions, class definitions are executable code that gets executed line by line at runtime. . The function decorator is a very important pattern in Python to simplify user code. It makes quick function wrappers. Say we want to time a function, we can do: . from time import time from functools import wraps def timeit(func): @wraps(func) def wrapper_func(*args, **kwargs): before = time() rv = func(*args, **kwargs) after = time() print(&quot;elapsed time:&quot;, after - before) return rv return wrapper_func @timeit def add(x, y): return x + y &quot;&quot;&quot; Decorator is merely a syntax, it is equivalent to &quot;&quot;&quot; add = timeit(add) . functools.wraps . functools.wraps is simply a way to preserve the name and docstring of the the original function that is being wrapped. Without it, the wrapped function would lose its original metadata. . from functools import wraps def logged(func): @wraps(func) def with_logging(*args, **kwargs): print(func.__name__ + &quot; was called&quot;) return func(*args, **kwargs) return with_logging @logged def f(x): &quot;&quot;&quot;does some math&quot;&quot;&quot; return x + x * x print(f.__name__) # prints &#39;f&#39; print(f.__doc__) # prints &#39;does some math&#39; . Note that not all decorators wrap functions, some of them just “register” the name of the input function, for example. In that case we don’t need to use functools.wraps. But usually we should use it for decorators. . Decorator with parameters: the partial trick . One way of thinking about decorators with parameters is . @decorator def foo(*args, **kwargs): pass . translates to . foo = decorator(foo) . So if the decorator had parameters, . @decorator_with_params(param) def foo(*args, **kwargs): pass . translates to . foo = decorator_with_params(param)(foo) . To make this work, we need to write the decorator function as follows, . from functools import partial, wraps def _outer(func, params): # magic sauce to lift the name and doc of the function @wraps(func) def inner(*args, **kwargs): #do stuff here, for eg. print (&quot;decorator params: %s&quot; % str(params)) return func(*args, **kwargs) return inner # THIS IS THE KEY TRICK TO TAKE PARAMS INTO DECORATORS real_decorator = partial(_outer, params=param) @real_decorator def bar(*args, **kwargs): pass . The line real_decorator = partial(_outer, params=param) is the key trick to take parameters into a decorator function! . Generators: two mental models . There are two important mental models for generators. . Laziness . A generator avoids eager execution and gives you the item you want one by one. This lets you avoid waiting for the entire loop to finish and storing all the items in it. . for i in range(10): sleep(0.5) yield i . Sequencing . A generator can enforce sequences in execution. It enables interleaving of coroutines: some library code runs, then some user code, then some library code… . Say we have an API that needs run some methods in order. . &quot;&quot;&quot; In this scenario, there is no enforcement of order &quot;&quot;&quot; class Api: def run_this_first(self): first() def run_this_second(self): second() def run_this_last(self): last() &quot;&quot;&quot; In this scenario, last() will never be run before second(), by using the generator. &quot;&quot;&quot; def api(): first() yield second() yield last() . Context managers . Simply put, a Context Manager makes sure a pair of commands: an initial action and a final action are always executed. . Here’s what a context manager is under the hood. . with ctx() as x: pass # &lt;=&gt; x = ctx().__enter__() try: pass finally: x.__exit__() . What do we need this? For example, when we do something in a database and would like to drop table when we are done: . from sqlite3 import connect with connect(&#39;test.db&#39;) as conn: cur = conn.cursor() cur.execute(&#39;create table points(x int, y int)&#39;) cur.execute(&#39;insert into points (x, y) values(1, 1)&#39;) cur.execute(&#39;insert into points (x, y) values(1, 2)&#39;) cur.execute(&#39;insert into points (x, y) values(2, 1)&#39;) for row in cur.execute(&#39;select x, y from points&#39;): print(row) cur.execute(&#39;drop table points&#39;) . We can write our own context manager to achieve this by: . class temptable: def __init__(self, cur): self.cur = cur def __enter__(self): print(&#39;__enter__&#39;) self.cur.execute(&#39;create table points(x int, y int)&#39;) def __exit__(self): print(&#39;__exit__&#39;) self.cur.execute(&#39;drop table points&#39;) with connect(&#39;test.db&#39;) as conn: cur = conn.cursor() with temptable(cur): cur.execute(&#39;insert into points (x, y) values(1, 1)&#39;) cur.execute(&#39;insert into points (x, y) values(1, 2)&#39;) cur.execute(&#39;insert into points (x, y) values(2, 1)&#39;) for row in cur.execute(&#39;select x, y from points&#39;): print(row) . This is a bit better, we can enforce running __enter__ before __exit__ by using context manager, i.e. in this case we enforce create table before drop table. . Now we notice we have sequencing in execution because one must run before another. This reminds us about generators. . We can refactor, . # THIS IS A GENERATOR WITH INPUT `cur` def temptable(cur): cur.execute(&#39;create table points(x int, y int)&#39;) yield cur.execute(&#39;drop table points&#39;) # THIS IS A CALLABLE CLASS WITH A GENERATOR INPUT class Contextmanager: def __init__(self, gen): self.gen = gen def __call__(self, *args, **kwargs): self.args, self.kwargs = args, kwargs def __enter__(self): print(&#39;__enter__&#39;) self.gen_instance = self.gen(*self.args, **self.kwargs) next(self.gen_instance) def __exit__(self, *args): print(&#39;__exit__&#39;) next(self.gen_instance, None) temptable = Contextmanager(temptable) with connect(&#39;test.db&#39;) as conn: cur = conn.cursor() with temptable(cur): cur.execute(&#39;insert into points (x, y) values(1, 1)&#39;) cur.execute(&#39;insert into points (x, y) values(1, 2)&#39;) cur.execute(&#39;insert into points (x, y) values(2, 1)&#39;) for row in cur.execute(&#39;select x, y from points&#39;): print(row) . The line temptable = Contextmanager(temptable) reminds us of decorators! . Fortunately, we don’t need to write Contextmanager class ourselves, there is this library called contextlib which already provides it. . from sqlite3 import connect from contextlib import contextmanager # THIS IS A GENERATOR WITH INPUT `cur` # @contextmanager TURNS A GENERATOR INTO A CONTEXT MANAGER! @contextmanager def temptable(cur): cur.execute(&#39;create table points(x int, y int)&#39;) try: yield finally: cur.execute(&#39;drop table points&#39;) with connect(&#39;test.db&#39;) as conn: cur = conn.cursor() with temptable(cur): cur.execute(&#39;insert into points (x, y) values(1, 1)&#39;) cur.execute(&#39;insert into points (x, y) values(1, 2)&#39;) cur.execute(&#39;insert into points (x, y) values(2, 1)&#39;) for row in cur.execute(&#39;select x, y from points&#39;): print(row) . @contextmanager turns a generator into a context manager! This is a really useful pattern for creating a context manager for a pair of commands. . This last example combined the 3 core features of Python together: . decorator: syntactic sugar for function wrapping | generator: avoid eagerness, save resources, force sequencing | context manager: force paired commands | . Summary . Python is a language oriented around protocols. There are ways to implement these protocols on any objects using “dunder” methods. If you forget how to use these methods, just google “Python data model”. . Reference . James Powell: So you want to be a Python expert? PyData Seattle 2017 (~2hr) | functools.wraps: https://stackoverflow.com/questions/308999/what-does-functools-wraps-do | The partial trick for decorators with parameters: https://stackoverflow.com/a/25827070/2280673 | .",
            "url": "http://blog.logancyang.com/note/python/2020/06/15/python-metaclasses.html",
            "relUrl": "/note/python/2020/06/15/python-metaclasses.html",
            "date": " • Jun 15, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "FastAI Lesson 12: Advanced training techniques; ULMFiT from scratch",
            "content": "We implement some really important training techniques today, all using callbacks: . MixUp, a data augmentation technique that dramatically improves results, particularly when you have less data, or can train for a longer time | Label smoothing, which works particularly well with MixUp, and significantly improves results when you have noisy labels | Mixed precision training, which trains models around 3x faster in many situations. | . We also implement xresnet, which is a tweaked version of the classic resnet architecture that provides substantial improvements. And, even more important, the development of it provides great insights into what makes an architecture work well. . Finally, we show how to implement ULMFiT from scratch, including building an LSTM RNN, and looking at the various steps necessary to process natural language data to allow it to be passed to a neural network. . Jeremy’s starting comments . We haven’t done any NLP yet, but NLP and CV share the same code for basic building blocks which we have written. . Comment: for code formatting, Jeremy doesn’t like using rules and formatters because he can do his own custom formatting for better readability like this: . def one_batch(self, i, xb, yb): try: self.iter = i self.xb,self.yb = xb,yb; self(&#39;begin_batch&#39;) self.pred = self.model(self.xb); self(&#39;after_pred&#39;) self.loss = self.loss_func(self.pred, self.yb); self(&#39;after_loss&#39;) if not self.in_train: return self.loss.backward(); self(&#39;after_backward&#39;) self.opt.step(); self(&#39;after_step&#39;) self.opt.zero_grad() except CancelBatchException: self(&#39;after_cancel_batch&#39;) finally: self(&#39;after_batch&#39;) . The goal of formatting is to make the code more readable, and debugging ML code is very difficult. This kind of formatting can help. . Jeremy’s answer to some in-class questions . Jeremy on cross validation . Cross validation is a good idea when the data is very small. Once the data has over 1k rows, it is not needed. In general, if the valid accuracy varies too much from run to run, consider cross validation. Otherwise, a good validation set is enough. . Best tips for debugging deep learning . Don’t make mistakes in the first place: make the code as simple as possible. | A horror story from Jeremy: he forgot to write .opt somewhere and it took countless hours and $5k of AWS credit to find that bug. | Testing for DL is different from standard software engineering, it needs to work for randomness. Working for one random seed doesn’t mean it works for another. You need non-reproducible tests, you need to be warned if something looks off statistically. | Once you realize there’s a specific bug, you write a test that fails on it everytime. | DL debugging is really hard, again, need to make sure you don’t make a mistake in the first place. | . Scientific journal: all scientists should have one . A note that has all the scientific experiment settings and their results. It should be easy to search through if we need some old records. . For example, noble gases and penicillin are discovered by good practice of scientific journaling. . Jeremy tried changing batch norm in different ways in Keras. His journal helped him keep track of all the things that didn’t work and what worked. . Git commit ID or dataset versions can be recorded in the journal if needed. Or just keep the dates and make sure you push every day. . Comments on stopword removal, stemming, lemmatization . Jeremy says it’s a terrible idea. The rule of thumb is to leave the raw text alone and do not do the stopwords removal, stemming, etc. These are for traditional NLP before deep learning. We don’t want to lose information. . (Me: some of these processing may still be useful for certain tasks. Jeremy’s answer here works for text classification in general.) . MixUp and label smoothing . In the last lesson we can run data augmentation on GPUs. . Now, we can use something called MixUp that makes other data augmentation irrelevant. It’s in the paper: . mixup: Beyond Empirical Risk Minimization . Highly recommended read. . Tip: Using Greek letters in Python code makes it easy to check the code against the paper. Python supports unicode. . class MixUp(Callback): _order = 90 #Runs after normalization and cuda def __init__(self, α:float=0.4): self.distrib = Beta(tensor([α]), tensor([α])) def begin_fit(self): self.old_loss_func,self.run.loss_func = self.run.loss_func,self.loss_func def begin_batch(self): if not self.in_train: return #Only mixup things during training λ = self.distrib.sample((self.yb.size(0),)).squeeze().to(self.xb.device) λ = torch.stack([λ, 1-λ], 1) self.λ = unsqueeze(λ.max(1)[0], (1,2,3)) shuffle = torch.randperm(self.yb.size(0)).to(self.xb.device) xb1,self.yb1 = self.xb[shuffle],self.yb[shuffle] # Doing a linear combination on the images self.run.xb = lin_comb(self.xb, xb1, self.λ) def after_fit(self): self.run.loss_func = self.old_loss_func def loss_func(self, pred, yb): if not self.in_train: return self.old_loss_func(pred, yb) with NoneReduce(self.old_loss_func) as loss_func: loss1 = loss_func(pred, yb) loss2 = loss_func(pred, self.yb1) # Doing a linear combination on the losses loss = lin_comb(loss1, loss2, self.λ) return reduce_loss(loss, getattr(self.old_loss_func, &#39;reduction&#39;, &#39;mean&#39;)) . MixUp applies linear combination in the form v1 * α + v2 * (1 - α) (previous we used it for Exponentially Weighted Moving Average) on images and losses. . We can use MixUp not just to the input layer. We can use it in the 1st layer, for example. It’s not well researched yet. . Label smoothing: a cure for noisy labels . Softmax wants to produce one number that is very close to 1. With MixUp and label noise in the dataset, we don’t want 100% certainty in the output. Use label smoothing, a regularization technique. . Tip: Don’t wait until there is a perfectly labeled dataset to start modelling. Use label smoothing. It works well even with noisy labels! . Label smoothing is designed to make the model a little bit less certain of it’s decision by changing a little bit its target: instead of wanting to predict 1 for the correct class and 0 for all the others, we ask it to predict 1-ε for the correct class and ε for all the others, with ε a (small) positive number and N the number of classes. This can be written as: . loss=(1−ε)ce(i)+ε∑ce(j)/Nloss = (1-ε) ce(i) + ε sum ce(j) / Nloss=(1−ε)ce(i)+ε∑ce(j)/N . where ce(x) is cross-entropy of x (i.e. $- log(p_{x})$), and i is the correct class. This can be coded in a loss function: . class LabelSmoothingCrossEntropy(nn.Module): def __init__(self, ε:float=0.1, reduction=&#39;mean&#39;): super().__init__() self.ε,self.reduction = ε,reduction def forward(self, output, target): c = output.size()[-1] log_preds = F.log_softmax(output, dim=-1) loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction) nll = F.nll_loss(log_preds, target, reduction=self.reduction) return lin_comb(loss/c, nll, self.ε) . And we use it by . learn = get_learner(nfs, data, 0.4, conv_layer, cb_funcs=cbfs, loss_func=LabelSmoothingCrossEntropy()) learn.fit(1) . Train in half precision floating point . Notebook: 10c_fp16 . We use NVidia’s apex library to use half precision floating point so we can make training much faster. To avoid inaccurate computation, we only make the forward and backward passes use half precision, and use full precision for everywhere else. . Refer to the notebook and the lecture for more details. . xresnet: train Imagenette . Notebook: 11_train_imagenette . This is like ResNet but there are a few tweaks. . The 1st tweak is called ResNet-C. The idea is that instead of using 7x7 kernel we use 3 times 3x3 kernel. | 2nd tweak is that we initialize the batch norm to sometimes have weights of 0 and sometimes weights of 1. The idea behind this is that then sometimes ResBlock can be ignored. This way we can train very deep models with high learning rates because if the model doesn’t need the layer it can skip it by keeping the batch norm weights to zero. | 3rd tweak is to move stride two one convolution up. | . The paper Jeremy is talking about: bag of tricks . Big companies try to brag with how big batches they can train once. For us, normal people, increasing the learning rate is something we want. That way we can speed training and generalize better. . Jeremy showed how using these techniques he made 3rd best ImageNet model. The two models above this are much bigger and require a lot of computation power. . Comment: for both research and production, code refactoring is very important! . def cnn_learner(arch, data, loss_func, opt_func, c_in=None, c_out=None, lr=1e-2, cuda=True, norm=None, progress=True, mixup=0, xtra_cb=None, **kwargs): cbfs = [partial(AvgStatsCallback,accuracy)]+listify(xtra_cb) if progress: cbfs.append(ProgressCallback) if cuda: cbfs.append(CudaCallback) if norm: cbfs.append(partial(BatchTransformXCallback, norm)) if mixup: cbfs.append(partial(MixUp, mixup)) arch_args = {} if not c_in : c_in = data.c_in if not c_out: c_out = data.c_out if c_in: arch_args[&#39;c_in&#39; ]=c_in if c_out: arch_args[&#39;c_out&#39;]=c_out return Learner(arch(**arch_args), data, loss_func, opt_func=opt_func, lr=lr, cb_funcs=cbfs, **kwargs) lr = 1e-2 pct_start = 0.5 def create_phases(phases): phases = listify(phases) return phases + [1-sum(phases)] phases = create_phases(pct_start) sched_lr = combine_scheds(phases, cos_1cycle_anneal(lr/10., lr, lr/1e5)) sched_mom = combine_scheds(phases, cos_1cycle_anneal(0.95, 0.85, 0.95)) cbsched = [ ParamScheduler(&#39;lr&#39;, sched_lr), ParamScheduler(&#39;mom&#39;, sched_mom)] learn = cnn_learner(xresnet34, data, loss_func, opt_func, norm=norm_imagenette) learn.fit(5, cbsched) . Transfer learning from scratch . Now we illustrate how to do transfer learning from scratch using an ImageWoof model on the Pets task. The ImageWoof model is small and only has 10 classes, and the Pets data has 37 classes and it has cats. So it is an unusual case for transfer learning, but a very interesting experiment to see whether it works. . Train a xresnet model for 40 epochs from scratch on ImageWoof, save the model. (valid accuracy ~80%) | Inspect how well xresnet model does on another task, Pets, by training for 1 epoch from scratch. (valid accuracy ~30%) | Custom the head Load the saved ImageWoof model for Pets, set the learner to have c_out=10 because ImageWoof has 10 classes; the learner now has ImageWoof model pointing to the Pets databunch. | Remove the output linear layer which has 10 activations, and replace it with a linear layer with 37 activations according to the Pets data. This is tricky because we don’t know the input dimensions of this new linear layer. We need to find out by looking at the previous layer’s output shape. Specifically, we pass a batch of data through the cut down model (without the head), and look the output shape. | | The transferred model: nn.Sequential(m_cut, AdaptiveConcatPool2d(), Flatten(),nn.Linear(ni*2, data.c_out)). The AdaptiveConcatPool2d() is something fastai has been using for a long time, somebody recently wrote a paper about it. It gives a nice boost. With this, the output linear layer needs 2x number of inputs because it has 2 kinds of pooling. | | DON’T DO THIS. Naive finetuning: train without freezing the body. | DO THIS. Correct finetuning: train the model with new head with body frozen. | Unfreeze the body and train some more. Something weird happens. The frozen body of the ImageWoof model has frozen batch norm layers where they have means and stds that are not compatible to the Pets data. | Solution: freeze non-batchnorm layers only! | | | Saving the model . After we train a model on the initial ImageWoof dataset, we need to save it. To save the model means we need to save the weights, and the weights are in model.state_dict() which is an OrderedDict that has keys as &lt;layer_name&gt;.weight and &lt;layer_name&gt;.bias. . Use torch.save(&lt;state_dict&gt;, &lt;path&gt;) to save the model. We can also use Pickle. torch.save also uses Pickle behind the scenes, it just adds some metadata about the model version and type info. . st = st = learn.model.state_dict() mdl_path = path/&#39;models&#39; mdl_path.mkdir(exist_ok=True) torch.save(st, mdl_path/&#39;iw5&#39;) . Tip: If you have trouble loading something, just try torch.load(...) into a dict and take a look at the keys and values and check what’s wrong. . # This is a Pets learner, with `data` as pets data # Note that c_out is changed to 10 to be able to load ImageWoof model learn = cnn_learner( xresnet18, data, loss_func, opt_func, c_out=10, norm=norm_imagenette) st = torch.load(mdl_path/&#39;iw5&#39;) m = learn.model m.load_state_dict(st) . Tip: In Jupyter Notebook, select multiple cells, c to copy, v to paste, shift+m to merge into one cell, add a function header and we have a function from previous cells! . Custom the head . Terminology: Body means the part that is transferred, in other words, the initial frozen part of the model. Head means the part that we train, the output layer(s). . The code below takes the ImageWoof model and adapts it for the task: Pets. . def adapt_model(learn, data): # Find everything before the AdaptiveAvgPool2d layer cut = next(i for i,o in enumerate(learn.model.children()) if isinstance(o,nn.AdaptiveAvgPool2d)) # Get the num of channels `ni` before average pooling m_cut = learn.model[:cut] xb,yb = get_batch(data.valid_dl, learn) pred = m_cut(xb) ni = pred.shape[1] # New model. We added `AdaptiveConcatPool2d(), Flatten()` m_new = nn.Sequential( m_cut, AdaptiveConcatPool2d(), Flatten(), nn.Linear(ni*2, data.c_out)) learn.model = m_new . To use these pretrained weights we need to remove the last linear layer and replace it another layer that have the right number of outputs. . &quot;&quot;&quot; Since learn.model is nn.Sequential(), [0] is the body. This line means we freeze the parameters in the body. &quot;&quot;&quot; for p in learn.model[0].parameters(): p.requires_grad_(False) . Don’t freeze batch norm layers! . The important thing to notice when fine-tuning is that batch norm will mess the accuracy because it is frozen with the statistics for a different task. The solution to this is to only freeze the layers that don’t contain batch norm. . def apply_mod(m, f): &quot;&quot;&quot; Nice little function that recursively applies f to all children of m &quot;&quot;&quot; f(m) for l in m.children(): apply_mod(l, f) def set_grad(m, b): if isinstance(m, (nn.Linear,nn.BatchNorm2d)): return if hasattr(m, &#39;weight&#39;): for p in m.parameters(): p.requires_grad_(b) . Trick: pytorch has the same recursive functionality of apply_mod(model, func), it is model.apply(func). . . Notice: require_grad=False means freezing, not updating the tensor. Pay attention to it in the micrograd framework for better understanding of autograd and its recursion. — . . Discriminative learning rate and parameter groups . Discriminative learning rate is an approach to freeze some layers without setting require_grad=False, but set the learning rate to 0 for these layers. . Look at the code below. It groups parameters into 2 groups g1 and g2, g2 for batch norm layers, and anything else with weights to g1. And it does it recursively. . def bn_splitter(m): def _bn_splitter(l, g1, g2): if isinstance(l, nn.BatchNorm2d): g2 += l.parameters() elif hasattr(l, &#39;weight&#39;): g1 += l.parameters() for ll in l.children(): _bn_splitter(ll, g1, g2) g1,g2 = [],[] _bn_splitter(m[0], g1, g2) g2 += m[1:].parameters() return g1,g2 . This is one of those things that if you got wrong, the model won’t train correctly but you won’t get an error. That’s very hard to debug. So we need a debug callback to look into the model. . Assume the code you write is wrong, have a good test strategy! . Jeremy introduced a DebugCallback that overwrites the __call__() method, and it works for any callbacks with cb_name passed in. . class DebugCallback(Callback): _order = 999 def __init__(self, cb_name, f=None): self.cb_name,self.f = cb_name,f def __call__(self, cb_name): if cb_name==self.cb_name: if self.f: self.f(self.run) else: set_trace() # Usage def _print_det(o): print (len(o.opt.param_groups), o.opt.hypers) raise CancelTrainException() learn.fit(1, disc_lr_sched + [DebugCallback(cb_types.after_batch, _print_det)]) . Refer to the notebook and the lecture at around 1:07:00 for more details. . ULMFiT is transfer learning applied to AWD-LSTM . Jeremy: LSTMs and RNNs are not inferior to transformer models such as GPT-2 and BERT, and they should not be a thing in the past. . Transformers and CNNs for texts don’t have state. For example, for speech recognition, you need to do analyses for all the samples around one sample again and again, so they are super wasteful. | They are fiddly, so they are not extensively used in industry-grade NLP yet. At the moment, Jeremy’s go-to choice for real world NLP tasks is still ULMFiT and RNNs. | . A language modeling is generic, it could be predicting the next sample of a piece of music or speech, or next genome in a sequence, etc. . Text preprocessing . Notebook: 12_text . 1. Make TextList . Adapt ItemList to TextList, . #export def read_file(fn): with open(fn, &#39;r&#39;, encoding = &#39;utf8&#39;) as f: return f.read() class TextList(ItemList): @classmethod def from_files(cls, path, extensions=&#39;.txt&#39;, recurse=True, include=None, **kwargs): return cls(get_files(path, extensions, recurse=recurse, include=include), path, **kwargs) def get(self, i): if isinstance(i, Path): return read_file(i) return i . For any other custom task and files, just implement the read_file function, and override get in ItemList. . 2. Tokenization . Next, we use spacy for tokenization. . Before tokenization, we can write a list of preprocessing functions such as . replace &lt;br /&gt; with n | remove excessive spaces | add space around # and / | . or any custom behavior you want. . Then we define some symbols/tokens: . UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ = &quot;xxunk xxpad xxbos xxeos xxrep xxwrep xxup xxmaj&quot;.split() . Do custom things for them in text, and add the functions to a list and call it the pre_rules . default_pre_rules = [ fixup_text, replace_rep, replace_wrep, spec_add_spaces, rm_useless_spaces, sub_br ] default_spec_tok = [UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ] . Jeremy finds spacy’s tokenizer complex but essential to produce good models. But it’s slow. Use Python’s ProcessPoolExecutor to speed it up. . from spacy.symbols import ORTH from concurrent.futures import ProcessPoolExecutor def parallel(func, arr, max_workers=4): &quot;&quot;&quot;Wrap ProcessPoolExecutor&quot;&quot;&quot; if max_workers&lt;2: results = list(progress_bar(map(func, enumerate(arr)), total=len(arr))) else: with ProcessPoolExecutor(max_workers=max_workers) as ex: return list(progress_bar(ex.map(func, enumerate(arr)), total=len(arr))) if any([o is not None for o in results]): return results class TokenizeProcessor(Processor): def __init__(self, lang=&quot;en&quot;, chunksize=2000, pre_rules=None, post_rules=None, max_workers=4): self.chunksize,self.max_workers = chunksize,max_workers self.tokenizer = spacy.blank(lang).tokenizer for w in default_spec_tok: self.tokenizer.add_special_case(w, [{ORTH: w}]) self.pre_rules = default_pre_rules if pre_rules is None else pre_rules self.post_rules = default_post_rules if post_rules is None else post_rules def proc_chunk(self, args): i,chunk = args # Notice the use of `compose` to apply the pre_rules functions # in a functional programming way chunk = [compose(t, self.pre_rules) for t in chunk] docs = [[d.text for d in doc] for doc in self.tokenizer.pipe(chunk)] docs = [compose(t, self.post_rules) for t in docs] return docs def __call__(self, items): toks = [] if isinstance(items[0], Path): items = [read_file(i) for i in items] chunks = [items[i: i+self.chunksize] for i in (range(0, len(items), self.chunksize))] # Use `parallel` to speed up the tokenizer toks = parallel(self.proc_chunk, chunks, max_workers=self.max_workers) return sum(toks, []) def proc1(self, item): return self.proc_chunk([item])[0] def deprocess(self, toks): return [self.deproc1(tok) for tok in toks] def deproc1(self, tok): return &quot; &quot;.join(tok) . 3. Numericalize tokens to vocab . Once we have tokenized our texts, we replace each token by an individual number, this is called numericalizing. Again, we do this with a processor (not so different from the CategoryProcessor). . import collections class NumericalizeProcessor(Processor): def __init__(self, vocab=None, max_vocab=60000, min_freq=2): self.vocab,self.max_vocab,self.min_freq = vocab,max_vocab,min_freq def __call__(self, items): #The vocab is defined on the first use. if self.vocab is None: freq = Counter(p for o in items for p in o) self.vocab = [o for o,c in freq.most_common(self.max_vocab) if c &gt;= self.min_freq] for o in reversed(default_spec_tok): if o in self.vocab: self.vocab.remove(o) self.vocab.insert(0, o) if getattr(self, &#39;otoi&#39;, None) is None: self.otoi = collections.defaultdict(int,{v:k for k,v in enumerate(self.vocab)}) return [self.proc1(o) for o in items] def proc1(self, item): return [self.otoi[o] for o in item] def deprocess(self, idxs): assert self.vocab is not None return [self.deproc1(idx) for idx in idxs] def deproc1(self, idx): return [self.vocab[i] for i in idx] . 4. Batching . The way to do batch is to let each batch have different sequences of the same documents, because the RNN will have a state for each batch. . Each batch has size (bs, bptt). This is really important. . Question: how to set bs, bptt combination? Jeremy says he doesn’t know the answer, it’s a good thing to experiment with. . To get rectangular tensor for texts of varying lengths, use padding tokens. This also works for rectangular images. Refer to the pad_collate() function. . Build the RNN: AWD-LSTM . Notebook: 12a_awd_lstm . Recall from part I, an RNN is just a regular neural network with as many layers as the number of tokens in the sequence to learn. For a long sequence we need so many layers, we use a for loop. Note that we use the same weight matrix for these layers (yellow in the diagram below). . . . With say 2000 layers, we have problems like vanishing gradients and exploding gradients. To make things worse, we can have stacked RNNs with more thousands of layers. . To make the RNN easier to train, we use something called an LSTM cell. . . Conceptually this is a lot. The code is actually not much: . class LSTMCell(nn.Module): def __init__(self, ni, nh): super().__init__() self.ih = nn.Linear(ni,4*nh) self.hh = nn.Linear(nh,4*nh) def forward(self, input, state): h,c = state #One big multiplication for all the gates is better than 4 smaller ones gates = (self.ih(input) + self.hh(h)).chunk(4, 1) ingate,forgetgate,outgate = map(torch.sigmoid, gates[:3]) cellgate = gates[3].tanh() c = (forgetgate*c) + (ingate*cellgate) h = outgate * c.tanh() return h, (h,c) . There is another type of cell called GRU. They are both ways to forget things. . The fast option is to use pytorch cuda. There is also something called jit and it translates Python into C++ in the background. However it doesn’t always work. . Dropout . Do dropout for an entire sequence at a time. Keywords: RNN dropout, weight dropout, and embedding dropout. . Gradient clipping . AWD-LSTM also uses gradient clipping to avoid exproding gradients. It allows us to use bigger learning rate. . Train the language model . Notebook: 12b_lm_pretrain, 12c_ulmfit . Just use the code implemented in the previous two notebooks. It trains for ~5hrs to get the language model. . If a word is in the task dataset (e.g. IMDB) and isn’t in the LM dataset (Wikitext103), we just use the mean weight and mean bias. If the word exists in both dataset, we directly use the embedding from the LM dataset. . Next, we create layer groups just as before, and train the model some more (~1hr), then we get a finetuned language model. . Tip: concat pooling is helpful for text as well as images. . Swift for TensorFlow . Swift and/or Julia is the future for deep learning. The days for Python is numbered. . Learning Swift is going to make me a better developer. . Swift resources: the official documentation book, Swift playground. . Chris Lattner . Papers . mixup: Beyond Empirical Risk Minimization | Rethinking the Inception Architecture for Computer Vision (label smoothing is in part 7) | Bag of Tricks for Image Classification with Convolutional Neural Networks | .",
            "url": "http://blog.logancyang.com/note/fastai/2020/06/04/fastai-lesson12.html",
            "relUrl": "/note/fastai/2020/06/04/fastai-lesson12.html",
            "date": " • Jun 4, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "FastAI Lesson 11: Data Block API, and generic optimizer",
            "content": "We start lesson 11 with a brief look at a smart and simple initialization technique called Layer-wise Sequential Unit Variance (LSUV). We implement it from scratch, and then use the methods introduced in the previous lesson to investigate the impact of this technique on our model training. It looks pretty good! . Then we look at one of the jewels of fastai: the Data Block API. We already saw how to use this API in part 1 of the course; but now we learn how to create it from scratch, and in the process we also will learn a lot about how to better use it and customize it. We’ll look closely at each step: . Get files: we’ll learn how os.scandir provides a highly optimized way to access the filesystem, and os.walk provides a powerful recursive tree walking abstraction on top of that | Transformations: we create a simple but powerful list and function composition to transform data on-the-fly | Split and label: we create flexible functions for each | DataBunch: we’ll see that DataBunch is a very simple container for our DataLoaders | . Next up, we build a new StatefulOptimizer class, and show that nearly all optimizers used in modern deep learning training are just special cases of this one class. We use it to add weight decay, momentum, Adam, and LAMB optimizers, and take a look a detailed look at how momentum changes training. . Finally, we look at data augmentation, and benchmark various data augmentation techniques. We develop a new GPU-based data augmentation approach which we find speeds things up quite dramatically, and allows us to then add more sophisticated warp-based transformations. . Layer-wise Sequential Unit Variance: a smart and simple init . Paper: All You Need is a Good Init . Notebook: 07a_lsuv . Trick: in deep learning, Modules are like a tree, so recursion for finding modules is needed. To concatenate the list of modules in the recursion, we can use sum(list, []), beginning with an empty list. . def find_modules(m, cond): if cond(m): return [m] return sum([find_modules(o,cond) for o in m.children()], []) mods = find_modules(learn.model, lambda o: isinstance(o,ConvLayer)) . The code for LSUV . def append_stat(hook, mod, inp, outp): d = outp.data hook.mean,hook.std = d.mean().item(),d.std().item() mdl = learn.model.cuda() with Hooks(mods, append_stat) as hooks: mdl(xb) for hook in hooks: print(hook.mean,hook.std) def lsuv_module(m, xb): h = Hook(m, append_stat) while mdl(xb) is not None and abs(h.mean) &gt; 1e-3: m.bias -= h.mean while mdl(xb) is not None and abs(h.std-1) &gt; 1e-3: m.weight.data /= h.std h.remove() return h.mean,h.std for m in mods: print(lsuv_module(m, xb)) %time run.fit(2, learn) . While the mean is not near zero: keep subtracting the bias with it. . While the std is not 1, keep dividing the weight by it. . This is the fastai way of initialization, just a loop, no math! . Note: LSUV is run once at the beginning before training. If you have a small batch size, run it for 5 batches and take the mean. . Data Block API . MNIST is a toy problem and it is too easy for serious research. CIFAR-10’s problem is that it’s 32x32, and it has very different characteristics from large images. Once the images are smaller than 96x96, things are quite different. Yet ImageNet is too large to experiment on. . So Jeremy created new datasets: Imagenette and Imagewoof. Imagenette (with French pronunciation) is designed to be easier. It has 10 very different classes. Imagewoof has only dog breeds. . Note: a big part of making deep learning useful in any domain is to make small workable datasets. . The Data Block API enables us to load huge datasets bit by bit because we can’t fit the whole dataset in our RAM. . Trick: add a custom function to any standard library. Just take the class and define a new method (this is the advantage of using a dynamic language) . import PIL,os,mimetypes Path.ls = lambda x: list(x.iterdir()) path.ls() . Joke: If a person says they are a deep learning practioner they must know tenches. Tench is the 1st class in ImageNet. . When we load an image it’s size is (160, 239, 3) (h, w, ch). The pixels are uint8. . Notice it’s not square, and the images in this dataset have different sizes. We need to resize them in order to put them in the same batch. . Tip: Python’s os.scandir(path) is a super fast way to check the directory for content, it’s written in C. os.walk(path) is similar but it’s able to recursively walk the directory. It is faster than glob and is lower-level. glob has more functionality and should be using scandir under the hood. . For 13394 files, it took ~70ms, extremely fast. The original ImageNet is 100x bigger, it will take just a few seconds. . Note: Jeremy spent a lot of time of these notebooks in part II, they are his research journal. Much of the code is already in fastai v1. . Prepare for modeling . What we need to do: . Get files | Split validation set random%, folder name, csv, … | . | Label: folder name, file name/re, csv, … | . | Transform per image (optional) | Transform to tensor | DataLoader | Transform per batch (optional) | DataBunch | Add test set (optional) | . def compose(x, funcs, *args, order_key=&#39;_order&#39;, **kwargs): key = lambda o: getattr(o, order_key, 0) for f in sorted(listify(funcs), key=key): x = f(x, **kwargs) return x class ItemList(ListContainer): def __init__(self, items, path=&#39;.&#39;, tfms=None): super().__init__(items) self.path,self.tfms = Path(path),tfms def __repr__(self): return f&#39;{super().__repr__()} nPath: {self.path}&#39; def new(self, items, cls=None): if cls is None: cls=self.__class__ &quot;&quot;&quot;cls , or self.__class__, becomes the constructor&quot;&quot;&quot; return cls(items, self.path, tfms=self.tfms) def get(self, i): return i def _get(self, i): return compose(self.get(i), self.tfms) def __getitem__(self, idx): res = super().__getitem__(idx) if isinstance(res,list): return [self._get(o) for o in res] return self._get(res) class ImageList(ItemList): @classmethod def from_files(cls, path, extensions=None, recurse=True, include=None, **kwargs): if extensions is None: extensions = image_extensions return cls(get_files(path, extensions, recurse=recurse, include=include), path, **kwargs) def get(self, fn): return PIL.Image.open(fn) . Tip: compose is a very useful concept in functional programming, it has a list of functions, calls the current function, get the result, and plug the result into the next function. . Labeling . We need some kind of Processor to do things on the training set and be able to do the same to the validation/test sets later, things such as . processing texts to tokenize and numericalize them | filling in missing values with median computed from training set for tabular data, storing the statistics in the Processor | converting label strings to numbers in a consistent and reproducible way | . In the image classification case, we create a list of possible labels in the training set, and then convert our labels to numbers based on this vocab. . Note: when a trained model is no better than random, the most common issue is that the validation set and the training set have different processing/mapping. Validation set should use the same processing with the same vocab and statistics as the training set! . Tip: whatever framework you use, have something like the fastai Labeler and Processor classes to help you remember the right things to do. . . Question: How to make the model handle unseen categories at inference time? . Answer: Great question especially for tasks with unlimited classes. In that case, find some rare classes in your data and label them as “other”, train with some examples in the “other” category. That way, the model should be able to hanlde unseen categories better. . . Note: learn Python @classmethod, what they are, when and why to use them. Refer to the post here. . Then we resize and turn the images into tensors. . tfms = [make_rgb, ResizeFixed(128), to_byte_tensor, to_float_tensor] il = ImageList.from_files(path, tfms=tfms) sd = SplitData.split_by_func(il, splitter) ll = label_by_func(sd, parent_labeler, proc_y=CategoryProcessor()) . Modeling . Data Bunch . class DataBunch(): def __init__(self, train_dl, valid_dl, c_in=None, c_out=None): self.train_dl,self.valid_dl,self.c_in,self.c_out = train_dl,valid_dl,c_in,c_out @property def train_ds(self): return self.train_dl.dataset @property def valid_ds(self): return self.valid_dl.dataset def databunchify(sd, bs, c_in=None, c_out=None, **kwargs): dls = get_dls(sd.train, sd.valid, bs, **kwargs) return DataBunch(*dls, c_in=c_in, c_out=c_out) SplitData.to_databunch = databunchify &quot;&quot;&quot; Summarize all the steps from the start &quot;&quot;&quot; path = datasets.untar_data(datasets.URLs.IMAGENETTE_160) tfms = [ make_rgb, ResizeFixed(128), to_byte_tensor, to_float_tensor ] il = ImageList.from_files(path, tfms=tfms) sd = SplitData.split_by_func( il, partial(grandparent_splitter, valid_name=&#39;val&#39;)) ll = label_by_func(sd, parent_labeler, proc_y=CategoryProcessor()) data = ll.to_databunch(bs, c_in=3, c_out=10, num_workers=4) . Model . In CNN, 3x3 kernels are the best bang for your buck. Papers: . Visualizing and understanding convolution networks | Bag of tricks for image classification with convolutional neural networks | . cbfs = [partial(AvgStatsCallback,accuracy), CudaCallback] def normalize_chan(x, mean, std): return (x-mean[...,None,None]) / std[...,None,None] _m = tensor([0.47, 0.48, 0.45]) _s = tensor([0.29, 0.28, 0.30]) norm_imagenette = partial(normalize_chan, mean=_m.cuda(), std=_s.cuda()) cbfs.append(partial(BatchTransformXCallback, norm_imagenette)) nfs = [64,64,128,256] def prev_pow_2(x): return 2**math.floor(math.log2(x)) def get_cnn_layers(data, nfs, layer, **kwargs): def f(ni, nf, stride=2): return layer(ni, nf, 3, stride=stride, **kwargs) l1 = data.c_in l2 = prev_pow_2(l1*3*3) layers = [f(l1 , l2 , stride=1), f(l2 , l2*2, stride=2), f(l2*2, l2*4, stride=2)] nfs = [l2*4] + nfs layers += [f(nfs[i], nfs[i+1]) for i in range(len(nfs)-1)] layers += [nn.AdaptiveAvgPool2d(1), Lambda(flatten), nn.Linear(nfs[-1], data.c_out)] return layers def get_cnn_model(data, nfs, layer, **kwargs): return nn.Sequential(*get_cnn_layers(data, nfs, layer, **kwargs)) def get_learn_run(nfs, data, lr, layer, cbs=None, opt_func=None, **kwargs): model = get_cnn_model(data, nfs, layer, **kwargs) init_cnn(model) return get_runner(model, data, lr=lr, cbs=cbs, opt_func=opt_func) sched = combine_scheds([0.3,0.7], cos_1cycle_anneal(0.1,0.3,0.05)) learn,run = get_learn_run(nfs, data, 0.2, conv_layer, cbs=cbfs+[ partial(ParamScheduler, &#39;lr&#39;, sched) ]) . Create model_summary() . def model_summary(run, learn, data, find_all=False): xb,yb = get_batch(data.valid_dl, run) device = next(learn.model.parameters()).device#Model may not be on the GPU yet xb,yb = xb.to(device),yb.to(device) mods = find_modules(learn.model, is_lin_layer) if find_all else learn.model.children() f = lambda hook,mod,inp,out: print(f&quot;{mod} n{out.shape} n&quot;) with Hooks(mods, f) as hooks: learn.model(xb) . Training the model, . %time run.fit(5, learn) &quot;&quot;&quot; train: [1.7975745138242594, tensor(0.3771, device=&#39;cuda:0&#39;)] valid: [1.950084228515625, tensor(0.3640, device=&#39;cuda:0&#39;)] train: [1.331341733558244, tensor(0.5549, device=&#39;cuda:0&#39;)] valid: [1.182614013671875, tensor(0.6160, device=&#39;cuda:0&#39;)] train: [1.0004353405653792, tensor(0.6729, device=&#39;cuda:0&#39;)] valid: [0.9452028198242187, tensor(0.6740, device=&#39;cuda:0&#39;)] train: [0.744675257750698, tensor(0.7583, device=&#39;cuda:0&#39;)] valid: [0.8292762451171874, tensor(0.7360, device=&#39;cuda:0&#39;)] train: [0.5341721137253761, tensor(0.8359, device=&#39;cuda:0&#39;)] valid: [0.798895751953125, tensor(0.7360, device=&#39;cuda:0&#39;)] CPU times: user 25.6 s, sys: 10.7 s, total: 36.4 s Wall time: 1min 7s &quot;&quot;&quot; . This is the most basic CNN, and its performance is not bad! . Optimizers . Notebook: 09_optimizers . Jeremy: we don’t need to re-implement the optimizer every time a new one comes out. There is only ONE generic optimizer, and we can change it to get every optimizer. . In pytorch, the optimizer is just a dictionary. . class Optimizer(): def __init__(self, params, steppers, **defaults): &quot;&quot;&quot;params is a list of lists of parameter tensors&quot;&quot;&quot; # might be a generator self.param_groups = list(params) # ensure params is a list of lists if not isinstance(self.param_groups[0], list): self.param_groups = [self.param_groups] &quot;&quot;&quot; One dict of hyperparameters for each parameter group. This line below copies the defaults dict rather than pointing to the same reference. Hyperparameters include lr, eps, etc. &quot;&quot;&quot; self.hypers = [{**defaults} for p in self.param_groups] self.steppers = listify(steppers) def grad_params(self): return [ (p,hyper) for pg,hyper in zip( self.param_groups,self.hypers) for p in pg if p.grad is not None ] def zero_grad(self): for p,hyper in self.grad_params(): p.grad.detach_() p.grad.zero_() def step(self): for p,hyper in self.grad_params(): compose(p, self.steppers, **hyper) &quot;&quot;&quot;Stepper&quot;&quot;&quot; def sgd_step(p, lr, **kwargs): p.data.add_(-lr, p.grad.data) return p opt_func = partial(Optimizer, steppers=[sgd_step]) . We can also add weight decay. We can do it in one of two ways: . Add L2 regularization to the loss, or | Add weight decay to the gradient weight.grad += wd * weight. For a vanilla SGD the update is | . weight = weight - lr*(weight.grad + wd*weight) . These two ways are only equivalent for vanilla SGD. For RMSprop and Adam, the second way is better. It is mentioned in the paper DECOUPLED WEIGHT DECAY REGULARIZATION. fastai made the second way the default. . We can implement stepper for weight decay. . def weight_decay(p, lr, wd, **kwargs): p.data.mul_(1 - lr*wd) return p weight_decay._defaults = dict(wd=0.) # Or def l2_reg(p, lr, wd, **kwargs): # Tip: Pytorch add_ can take two parameters a, b, # it does a mult of a and b first and then add p.grad.data.add_(wd, p.data) return p l2_reg._defaults = dict(wd=0.) . Tip: Pytorch add_ can take two parameters a, b, it does a mult of a and b first and then add the result to the tensor. . Add momentum . Momentum needs the previous state of all parameters to work. We store it in a dict state. . class StatefulOptimizer(Optimizer): def __init__(self, params, steppers, stats=None, **defaults): self.stats = listify(stats) maybe_update(self.stats, defaults, get_defaults) super().__init__(params, steppers, **defaults) self.state = {} def step(self): for p,hyper in self.grad_params(): if p not in self.state: # Create a state for p and call all the # statistics to initialize it. self.state[p] = {} maybe_update( self.stats, self.state[p], lambda o: o.init_state(p) ) state = self.state[p] for stat in self.stats: state = stat.update(p, state, **hyper) compose(p, self.steppers, **state, **hyper) self.state[p] = state class AverageGrad(Stat): _defaults = dict(mom=0.9) def init_state(self, p): return {&#39;grad_avg&#39;: torch.zeros_like(p.grad.data)} def update(self, p, state, mom, **kwargs): state[&#39;grad_avg&#39;].mul_(mom).add_(p.grad.data) return state def momentum_step(p, lr, grad_avg, **kwargs): p.data.add_(-lr, grad_avg) return p sgd_mom_opt = partial( StatefulOptimizer, steppers=[momentum_step,weight_decay], stats=AverageGrad(), wd=0.01 ) learn,run = get_learn_run( nfs, data, 0.3, conv_layer, cbs=cbfs, opt_func=sgd_mom_opt ) . !!NOTE!! Batch norm mults makes L2 regularization not work the way we expected! . Everybody has been doing it wrong! This paper L2 regularization vs batch and weight normalization pointed it out. But this paper also isn’t completely correct. L2 regularization DOES DO SOMETHING. There are some more recent papers that tried to explain what L2 regularization does along with batch norm, but the current state is that no one understands it completely. . Jeremy: The theory people who did this kind of research don’t know how to train models. The practioners forget about theories. If you can combine the two, you can find interesting results! . Jane street blog post | Paper: Three Mechanisms of Weight Decay Regularization | . Momentum is also interesting. We use Exponentially Weighted Moving Average (ewma) or lerp in pytorch. . Next, the notebook describes the implementation of Adam and LAMB (paper) with the generic optimizer structure. Refer to the notebook for more details. . Jeremy implemented the LAMB approach a week before the paper came out. The paper is highly recommended. . Refactor: remove Runner, just have Learner . Notebook: 09b_learner . https://github.com/fastai/course-v3/blob/master/nbs/dl2/09b_learner.ipynb . Jeremy realized the Runner class just stores 3 things and it’s much better to just put it in Learner. This makes the code much easier to use. . Add a progress bar . Notebook: 09c_add_progress_bar . https://github.com/fastai/course-v3/blob/master/nbs/dl2/09c_add_progress_bar.ipynb . The components used are . from fastprogress import master_bar, progress_bar . And we create the callback . class AvgStatsCallback(Callback): def __init__(self, metrics): self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False) def begin_fit(self): met_names = [&#39;loss&#39;] + [ m.__name__ for m in self.train_stats.metrics] names = [&#39;epoch&#39;] + [f&#39;train_{n}&#39; for n in met_names] + [f&#39;valid_{n}&#39; for n in met_names] + [&#39;time&#39;] self.logger(names) def begin_epoch(self): self.train_stats.reset() self.valid_stats.reset() self.start_time = time.time() def after_loss(self): stats = self.train_stats if self.in_train else self.valid_stats with torch.no_grad(): stats.accumulate(self.run) def after_epoch(self): stats = [str(self.epoch)] for o in [self.train_stats, self.valid_stats]: stats += [f&#39;{v:.6f}&#39; for v in o.avg_stats] stats += [format_time(time.time() - self.start_time)] self.logger(stats) . Then we add the progress bars… with a Callback of course! master_bar handles the count over the epochs while its child progress_bar is looping over all the batches. We just create one at the beginning or each epoch/validation phase, and update it at the end of each batch. By changing the logger of the Learner to the write function of the master bar, everything is automatically written there. . Note: this requires fastprogress v0.1.21 or later. . # export class ProgressCallback(Callback): _order=-1 def begin_fit(self): self.mbar = master_bar(range(self.epochs)) self.mbar.on_iter_begin() self.run.logger = partial(self.mbar.write, table=True) def after_fit(self): self.mbar.on_iter_end() def after_batch(self): self.pb.update(self.iter) def begin_epoch (self): self.set_pb() def begin_validate(self): self.set_pb() def set_pb(self): self.pb = progress_bar(self.dl, parent=self.mbar) self.mbar.update(self.epoch) . Then use it like this . cbfs = [partial(AvgStatsCallback,accuracy), CudaCallback, ProgressCallback, partial(BatchTransformXCallback, norm_imagenette)] learn = get_learner(nfs, data, 0.4, conv_layer, cb_funcs=cbfs) learn.fit(2) . Data augmentation . Notebook: 10_augmentation . To further improve our Imagenette model, we need data augmentation. . The key takeaway is that there is no “best transform” for data augmentation. Try things out and take a close look at the results. . Some transform examples: . img.resize((128,128), resample=PIL.Image.ANTIALIAS) img.resize((128,128), resample=PIL.Image.BILINEAR) img.resize((128,128), resample=PIL.Image.NEAREST) img.resize((256,256), resample=PIL.Image.BICUBIC) .resize((128,128), resample=PIL.Image.NEAREST) . Tip: doing transforms on bytes (i.e. uint8) is much faster than on floats. Converting bytes to floats is much slower than something as complex as a warp! . Some useful transforms that are particularly useful: . zooming in. It works great for image, for text and audio. | perspective warping for image. It needs to solve a system of linear equations. Pytorch has this solver! | . One comment from Jeremy is that for zooming in, if the object of interest - the tench - is cropped out by the zoom-in effect, it’s OK! It’s still the ImageNet winning strategy. Ultimately it creates noisy labels where some labels are just wrong. All the research showed that noisy labels are okay - because the model learns to link other things in that image with the label. . For music data augmentation, for example, you can do pitch shifting, volume changes, cutting, etc. Ultimately it depends on the domain and what you need. . In the next lesson, we will introduce MixUp, a data augmentation technique that dramatically improves results no matter the domain, and can be run on GPU. It will make some of the techniques here irrelevant. . Papers to read . Visualizing and understanding convolution networks | Bag of tricks for image classification with convolutional neural networks | L2 Regularization versus Batch and Weight Normalization | Norm matters: efficient and accurate normalization schemes in deep networks | Three Mechanisms of Weight Decay Regularization | Nesterov’s Accelerated Gradient and Momentum as approximations to Regularised Update Descent | Adam: A Method for Stochastic Optimization | Reducing BERT Pre-Training Time from 3 Days to 76 Minutes | Blog post on the interaction between L2 Regularization and Batchnorm, including experiments | .",
            "url": "http://blog.logancyang.com/note/fastai/2020/06/02/fastai-lesson11.html",
            "relUrl": "/note/fastai/2020/06/02/fastai-lesson11.html",
            "date": " • Jun 2, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "FastAI Lesson 10: Looking Inside the Model",
            "content": "In lesson 10 we start with a deeper dive into the underlying idea of callbacks and event handlers. We look at many different ways to implement callbacks in Python, and discuss their pros and cons. Then we do a quick review of some other important foundations: . __dunder__ special symbols in Python | How to navigate source code using your editor | Variance, standard deviation, covariance, and correlation | Softmax | Exceptions as control flow | . Next up, we use the callback system we’ve created to set up CNN training on the GPU. . Then we move on to the main topic of this lesson: looking inside the model to see how it behaves during training. To do so, we first need to learn about hooks in PyTorch, which allow us to add callbacks to the forward and backward passes. We will use hooks to track the changing distribution of our activations in each layer during training. By plotting this distributions, we can try to identify problems with our training. . In order to fix the problems we see, we try changing our activation function, and introducing batchnorm. We study the pros and cons of batchnorm, and note some areas where it performs poorly. Finally, we develop a new kind of normalization layer to overcome these problems, and compare it to previously published approaches, and see some very encouraging results. . Jeremy’s starting comments: take it easy and make progress slowly . The amount of material in part II is meant to keep the student busy until the next version of the course. Don’t expect to understand every thing in one go, digest them bit by bit. . It also covers the software engineering side. Jeremy’s opinion is that data scientists need to be good software engineers as well. . . We will stick to using nothing but the foundation tools in the picture above to recreate the fastai library. . Next week we will develop a new library called fastai.audio! (Exactly what I interested in: DL in audio and library development!) . . Then we will get into seq2seq models, transformer, and more advanced vision models that requires setting up a DL box and doing experiments. fastec2 library is useful for running experiments in AWS. . . . At last we will dive into Swift for DL. . Revisiting Callbacks . Notebook: 05a_foundations . What is a callback . Callbacks are functions that get triggered at certain events. We pass the callback function object itself to a method. . How to create a callback . def slow_calculation(cb=None): res = 0 for i in range(5): res += i*i sleep(1) if cb: cb(i) return res def show_progress(epoch): print(f&quot;Awesome! We&#39;ve finished epoch {epoch}!&quot;) slow_calculation(show_progress) &quot;&quot;&quot; Awesome! We&#39;ve finished epoch 0! Awesome! We&#39;ve finished epoch 1! Awesome! We&#39;ve finished epoch 2! Awesome! We&#39;ve finished epoch 3! Awesome! We&#39;ve finished epoch 4! &quot;&quot;&quot; . This callback show_progress(epoch) is just a function that’s passed into the target function as an object. The target function has an expectation how to call it, i.e. passing in the # epoch in this case. . Since we are using it once, a better way to do this is to use the lambda function (similar to arrow functions in JavaScript ES6). . slow_calculation(lambda o: print(f&quot;Awesome! We&#39;ve finished epoch {o}!&quot;)) . Callback with more than one argument . def make_show_progress(exclamation): # Leading &quot;_&quot; is generally understood to be &quot;private&quot; # `exclamation` is a context variable for _inner(epoch) # this is called closure def _inner(epoch): print(f&quot;{exclamation}! We&#39;ve finished epoch {epoch}!&quot;) return _inner slow_calculation(make_show_progress(&quot;Nice!&quot;)) &quot;&quot;&quot; Nice!! We&#39;ve finished epoch 0! Nice!! We&#39;ve finished epoch 1! Nice!! We&#39;ve finished epoch 2! Nice!! We&#39;ve finished epoch 3! Nice!! We&#39;ve finished epoch 4! &quot;&quot;&quot; . exclamation is a context variable outside _inner(epoch). This is called closure. This concept is prevalent in JS. . f2 = make_show_progress(&quot;Terrific&quot;) slow_calculation(f2) &quot;&quot;&quot; Terrific! We&#39;ve finished epoch 0! Terrific! We&#39;ve finished epoch 1! Terrific! We&#39;ve finished epoch 2! Terrific! We&#39;ve finished epoch 3! Terrific! We&#39;ve finished epoch 4! &quot;&quot;&quot; . partial function . In Python, with from functools import partial we can make a new function that is the old function with predefined argument(s). . from functools import partial slow_calculation(partial(show_progress, &quot;OK I guess&quot;)) &quot;&quot;&quot; OK I guess! We&#39;ve finished epoch 0! OK I guess! We&#39;ve finished epoch 1! OK I guess! We&#39;ve finished epoch 2! OK I guess! We&#39;ve finished epoch 3! OK I guess! We&#39;ve finished epoch 4! &quot;&quot;&quot; . partial(func, arg, arg, ...) takes positional arguments and knows how to set them in order. . Callbacks as callable classes . Wherever we can use a closure to store a context, we can also use a class. . class ProgressShowingCallback(): def __init__(self, exclamation=&quot;Awesome&quot;): self.exclamation = exclamation def __call__(self, epoch): &quot;&quot;&quot;This is the part that makes the class callable as a function!&quot;&quot;&quot; print(f&quot;{self.exclamation}! We&#39;ve finished epoch {epoch}!&quot;) cb = ProgressShowingCallback(&quot;Just super&quot;) slow_calculation(cb) &quot;&quot;&quot; Just super! We&#39;ve finished epoch 0! Just super! We&#39;ve finished epoch 1! Just super! We&#39;ve finished epoch 2! Just super! We&#39;ve finished epoch 3! Just super! We&#39;ve finished epoch 4! &quot;&quot;&quot; . In Python, obj.__call__() makes the obj callable as a function when used like obj()! . Python *args and **kwargs . A Python function puts the positional arguments into a tuple args, and the keyword arguments into a dictionary kwargs. . def f(*args, **kwargs): print(f&quot;args: {args}; kwargs: {kwargs}&quot;) f(3, &#39;a&#39;, thing1=&quot;hello&quot;) &quot;&quot;&quot; args: (3, &#39;a&#39;); kwargs: {&#39;thing1&#39;: &#39;hello&#39;} &quot;&quot;&quot; . There are some downsides to using args and kwargs, e.g. when you check the signature of a function and you only see this and don’t know what exactly is passed in. For example, if there’s a typo in a parameter name, it’s hard to track down. . Sometimes we do want to use them. For example, here the callback cb has two methods, one takes 1 argument and the other takes 2. . def slow_calculation(cb=None): res = 0 for i in range(5): if cb: cb.before_calc(i) res += i*i sleep(1) if cb: cb.after_calc(i, val=res) return res class PrintStepCallback(): def __init__(self): pass def before_calc(self, *args, **kwargs): &quot;&quot;&quot;In this case we don&#39;t care about what&#39;s passed in&quot;&quot;&quot; print(f&quot;About to start&quot;) def after_calc (self, *args, **kwargs): print(f&quot;Done step&quot;) . In this case we don’t care about what’s passed into the methods, args and kwargs are passed and not used. . If we remove them there will be an error when calling the methods with any arguments. With them, we can pass in whatever arguments at calling. . To make the methods do something with the input, . class PrintStatusCallback(): def __init__(self): pass def before_calc(self, epoch, **kwargs): print(f&quot;About to start: {epoch}&quot;) def after_calc (self, epoch, val, **kwargs): print(f&quot;After {epoch}: {val}&quot;) slow_calculation(PrintStatusCallback()) &quot;&quot;&quot; About to start: 0 After 0: 0 About to start: 1 After 1: 1 About to start: 2 After 2: 5 About to start: 3 After 3: 14 About to start: 4 After 4: 30 &quot;&quot;&quot; . Here we put **kwargs in case we want to add something in the future and make sure it doesn’t break. If we pass in any unexpected positional arguments it should break. . Callbacks: modifying behavior . Early stopping . We can modify the target function with the callback. Here’s an example of early stopping using a callback. . def slow_calculation(cb=None): res = 0 for i in range(5): # `hasattr` avoids breaking if cb doesn&#39;t have the method if cb and hasattr(cb,&#39;before_calc&#39;): cb.before_calc(i) res += i*i sleep(1) if cb and hasattr(cb,&#39;after_calc&#39;): if cb.after_calc(i, res): print(&quot;stopping early&quot;) break return res class PrintAfterCallback(): def after_calc(self, epoch, val): print(f&quot;After {epoch}: {val}&quot;) if val&gt;10: return True slow_calculation(PrintAfterCallback()) &quot;&quot;&quot; After 0: 0 After 1: 1 After 2: 5 After 3: 14 stopping early &quot;&quot;&quot; . Modifying the state . We can also directly modify the state of the object with the callback by passing the object into the callback. . class SlowCalculator(): def __init__(self, cb=None): self.cb, self.res = cb, 0 def callback(self, cb_name, *args): if not self.cb: return cb = getattr(self.cb,cb_name, None) if cb: return cb(self, *args) def calc(self): for i in range(5): # We can use `__call__()` instead of `callback()` above, # then here becomes `self(&#39;before_calc&#39;, i)` self.callback(&#39;before_calc&#39;, i) self.res += i*i sleep(1) if self.callback(&#39;after_calc&#39;, i): print(&quot;stopping early&quot;) break class ModifyingCallback(): def after_calc(self, calc, epoch): print(f&quot;After {epoch}: {calc.res}&quot;) if calc.res&gt;10: return True # HERE WE MODIFIES `calc` object that is passed in! if calc.res&lt;3: calc.res = calc.res*2 # Init the instance with the modifying callback calculator = SlowCalculator(ModifyingCallback()) calculator.calc() &quot;&quot;&quot; After 0: 0 After 1: 1 After 2: 6 After 3: 15 stopping early &quot;&quot;&quot; calculator.res &quot;&quot;&quot; 15 &quot;&quot;&quot; . Revisiting Python Dunder Methods . The Python doc for its data model has all the info about the special dunder methods __xxx__(). . A toy example, . class SloppyAdder(): def __init__(self,o): self.o=o def __add__(self,b): return SloppyAdder(self.o + b.o + 0.01) def __repr__(self): return str(self.o) a = SloppyAdder(1) b = SloppyAdder(2) # `+` is overridden by __add__ a+b &quot;&quot;&quot; 3.01 &quot;&quot;&quot; . Some examples: . __getitem__ | __getattr__ | __setattr__ | __del__ | __init__ | __new__ | __enter__ | __exit__ | __len__ | __repr__ | __str__ | . Fundamental ability of an engineer: browsing source code . Must know and practice how to do all these in vscode, . Jump to tag/symbol | Jump to current tag | Jump to library tags | Go back | Search | Outlining / folding | . Jeremy uses Vim because it’s good for developing on remote machines. Nowadays vscode can use the ssh extension. . Variance, covariance, and correlation . &quot;&quot;&quot; VARIANCE &quot;&quot;&quot; t = torch.tensor([1.,2.,4.,18]) m = t.mean() (t-m).pow(2).mean() &quot;&quot;&quot; STANDARD DEVIATION &quot;&quot;&quot; (t-m).pow(2).mean().sqrt() &quot;&quot;&quot; MEAN ABSOLUTE DEVIATION &quot;&quot;&quot; (t-m).abs().mean() . Note that Mean Absolute Deviation should be used more because it’s more robust than the standard deviation for outliers. . Notice that . (t-m).pow(2).mean() == (t*t).mean() - (m*m) . This is equivalent to, . var⁡[X]=E⁡[X2]−E⁡[X]2 operatorname{var}[X] = operatorname{E} left[X^2 right] - operatorname{E}[X]^2var[X]=E[X2]−E[X]2 . When we calculate the variance in code, we should use (t*t).mean() - (m*m) instead of the definition form because it’s more efficient (doesn’t require multiple passes). . Similarly, we can calculate the covariance of two variables t and v by . cov = (t*v).mean() - t.mean()*v.mean() . because, . cov⁡(X,Y)=E⁡[(X−E⁡[X])(Y−E⁡[Y])] operatorname{cov}(X,Y) = operatorname{E}{ big[(X - operatorname{E}[X])(Y - operatorname{E}[Y]) big]}cov(X,Y)=E[(X−E[X])(Y−E[Y])] =E⁡[XY]−E⁡[X]E⁡[Y]= operatorname{E} left[X Y right] - operatorname{E} left[X right] operatorname{E} left[Y right]=E[XY]−E[X]E[Y] . Variance and covariance are the same thing, because variance is just the covariance of X with itself. . Next we have correlation, or Pearson correlation coefficient, . ρX,Y=cov⁡(X,Y)σXσY rho_{X,Y}= frac{ operatorname{cov}(X,Y)}{ sigma_X sigma_Y}ρX,Y​=σX​σY​cov(X,Y)​ . In code, . corr = cov / (t.std() * v.std()) . The correlation is just a scaled version of the covariance. . Remember: from now on, always write code for a math equation, not (just) the LaTeX! . Softmax . A recap of the softmax function and the multiclass cross entropy loss. . In code, log softmax is . def log_softmax(x): return x - x.exp().sum(-1, keepdim=True).log() . Here x is the activation vector, log_softmax(x) is a vector with the same shape as x. . In equation it is (i for the ith element of one prediction vector) . y^i=softmax(x)i=exi/∑jexj hat{y}_i = text{softmax}( mathbf{x})_{i} = e^{x_{i}} / sum_{j} e^{x_{j}}y^​i​=softmax(x)i​=exi​/j∑​exj​ . logsoftmax(x)i=xi−log⁡∑jexj text{logsoftmax}( mathbf{x})_{i} = x_{i} - log sum_{j} e^{x_{j}}logsoftmax(x)i​=xi​−logj∑​exj​ . And cross entropy loss (NLL) for $ mathbf{x}$, i.e. the activation vector of one prediction is: . −log⁡(y^i)- log( hat{y}_i)−log(y^​i​) . This is because the ground truth y is one-hot encoded. Refer to lesson 9’s note to recall the selection trick. . For multiple predictions, recall that the cross entropy loss or NLL is . def nll(softmax_preds, targets): &quot;&quot;&quot; Use array indexing to select the corresponding values for cross entropy loss. &quot;&quot;&quot; log_sm = softmax_preds.log() return -log_sm[range(targets.shape[0]), targets].mean() . The mean() is for averaging over multiple rows of log softmax predictions to get an overall batch prediction loss. . When to use softmax and when not to . Softmax likes to pick one thing and make it big, because it’s exponential. . . In the above Excel example, the activations for these categories in image 1 are larger than in image 2, which means image 1 is more likely to have these objects in it (Me: this teaches us that the activations before the softmax express confidence of having those things in the image). . But, the softmax outputs are the same because after the exp() and the normalization, each component captures the same percentage. . Yet they are different. . Very important remarks by Jeremy . . Be careful when softmax is a BAD IDEA: . To use softmax, make sure that the entries in your dataset all have one or more objects of interests, PREFERRABLY ONE OF EACH TYPE. If none of the images have the objects of interest in them, softmax will still give a high probability of seeing them! If a category has more than one object in an image, softmax finds the most likely ONE. This also applies to audio or tabular data. . For yes or no (whether there is an object of type A or B or C in the image) kind of tasks, we should use sigmoid instead of softmax, as shown at the far right in the above Excel example (note that they don’t sum to one anymore). . Why did we always use softmax in object recognition tasks? Because of ImageNet! The data entries in ImageNet always have ONE of some object of interest in them!! . A lot of well-regarded academic papers or applications use Nothing as a category alongside others like Cat, Fish, etc. But Jeremy says it’s terrible idea! Because there is no feature like “furriness” or “smoothness” or “shininess” that describes “No-Cat”, “No-Fish”, etc. Of course we can hack it by somehow producing another model that captures the “none-cat-ness” features but that is too hard and unnecessary. Just use a binary model for predicting whether there’s an object in the scene! . Me: Again, this lesson teaches us that the activations before the last classification outout layer is a monotonic function that indicates the confidence of predicting that category. . When you see a paper that uses softmax for classifying exist/non-exist tasks, try to use a sigmoid, you may get better result! . When is softmax a good idea? Language modeling! Predicting the next word is the perfect case for using softmax because it’s always one word and no more or less than one word. . . Build a Learning Rate Finder . Notebook: 05b_early_stopping . Using Exceptions as control flow! . It is not easy to use callbacks and a boolean stop value to do early stopping because we need to check many places. Using Exception is a neat trick. . An exception in Python is just a class that inherits from Exception. Most of the time you don’t need to give it any behavior, just pass, like this, . class CancelTrainException(Exception): pass . We have the Runner class and the Callback class. . class Callback(): _order=0 def set_runner(self, run): self.run=run def __getattr__(self, k): return getattr(self.run, k) @property def name(self): name = re.sub(r&#39;Callback$&#39;, &#39;&#39;, self.__class__.__name__) return camel2snake(name or &#39;callback&#39;) def __call__(self, cb_name): f = getattr(self, cb_name, None) if f and f(): return True return False class TrainEvalCallback(Callback): def begin_fit(self): self.run.n_epochs=0. self.run.n_iter=0 def after_batch(self): if not self.in_train: return self.run.n_epochs += 1./self.iters self.run.n_iter += 1 def begin_epoch(self): self.run.n_epochs=self.epoch self.model.train() self.run.in_train=True def begin_validate(self): self.model.eval() self.run.in_train=False class CancelTrainException(Exception): pass class CancelEpochException(Exception): pass class CancelBatchException(Exception): pass ######################### class Runner(): def __init__(self, cbs=None, cb_funcs=None): cbs = listify(cbs) for cbf in listify(cb_funcs): cb = cbf() setattr(self, cb.name, cb) cbs.append(cb) self.stop,self.cbs = False,[TrainEvalCallback()]+cbs @property def opt(self): return self.learn.opt @property def model(self): return self.learn.model @property def loss_func(self): return self.learn.loss_func @property def data(self): return self.learn.data def one_batch(self, xb, yb): try: self.xb,self.yb = xb,yb self(&#39;begin_batch&#39;) self.pred = self.model(self.xb) self(&#39;after_pred&#39;) self.loss = self.loss_func(self.pred, self.yb) self(&#39;after_loss&#39;) if not self.in_train: return self.loss.backward() self(&#39;after_backward&#39;) self.opt.step() self(&#39;after_step&#39;) self.opt.zero_grad() except CancelBatchException: self(&#39;after_cancel_batch&#39;) finally: self(&#39;after_batch&#39;) def all_batches(self, dl): self.iters = len(dl) try: for xb,yb in dl: self.one_batch(xb, yb) except CancelEpochException: self(&#39;after_cancel_epoch&#39;) def fit(self, epochs, learn): self.epochs,self.learn,self.loss = epochs,learn,tensor(0.) try: for cb in self.cbs: cb.set_runner(self) self(&#39;begin_fit&#39;) for epoch in range(epochs): self.epoch = epoch if not self(&#39;begin_epoch&#39;): self.all_batches(self.data.train_dl) with torch.no_grad(): if not self(&#39;begin_validate&#39;): self.all_batches(self.data.valid_dl) self(&#39;after_epoch&#39;) except CancelTrainException: self(&#39;after_cancel_train&#39;) finally: self(&#39;after_fit&#39;) self.learn = None def __call__(self, cb_name): res = False for cb in sorted(self.cbs, key=lambda x: x._order): res = cb(cb_name) or res return res . We see that CancelBatchException, CancelEpochException and CancelTrainException are used as control flow to enable graceful skip or stopping, by placing it with except between try and finally blocks. . We can use CancelTrainException to make a learning rate finder, . class LR_Find(Callback): _order=1 def __init__(self, max_iter=100, min_lr=1e-6, max_lr=10): self.max_iter,self.min_lr,self.max_lr = max_iter,min_lr,max_lr self.best_loss = 1e9 def begin_batch(self): if not self.in_train: return pos = self.n_iter/self.max_iter lr = self.min_lr * (self.max_lr/self.min_lr) ** pos for pg in self.opt.param_groups: pg[&#39;lr&#39;] = lr def after_step(self): if self.n_iter&gt;=self.max_iter or self.loss&gt;self.best_loss*10: raise CancelTrainException() if self.loss &lt; self.best_loss: self.best_loss = self.loss . In after_step() we check if the loss gets much worse, if yes we stop training. . Recreate CNN (CPU and GPU) . Notebook: 06_cuda_cnn_hooks_init . # MNIST x_train,y_train,x_valid,y_valid = get_data() # Normalize based on training data def normalize_to(train, valid): m,s = train.mean(),train.std() return normalize(train, m, s), normalize(valid, m, s) x_train,x_valid = normalize_to(x_train,x_valid) train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid) nh,bs = 50,512 c = y_train.max().item()+1 loss_func = F.cross_entropy data = DataBunch(*get_dls(train_ds, valid_ds, bs), c) class Lambda(nn.Module): &quot;&quot;&quot;This is for putting into pytorch nn.Sequential()&quot;&quot;&quot; def __init__(self, func): super().__init__() self.func = func def forward(self, x): return self.func(x) . To refactor layers, it’s useful to have a Lambda layer that can take a basic function and convert it to a layer you can put in nn.Sequential. . Note: if you use a Lambda layer with a lambda function, your model won’t pickle so you won’t be able to save it with PyTorch. So it’s best to give a name to the function you’re using inside your Lambda (like flatten here). . def flatten(x): &quot;&quot;&quot;Flatten after nn.AdaptiveAvgPool2d and before the final nn.Linear&quot;&quot;&quot; return x.view(x.shape[0], -1) def mnist_resize(x): &quot;&quot;&quot; Resize bs x 784 to batches of 28x28 images. -1 means the batch size remains whatever it is before &quot;&quot;&quot; return x.view(-1, 1, 28, 28) . Create the CNN model, . def get_cnn_model(data): return nn.Sequential( # This lambda layer is preprocessing original bs x 784 to # bs x 1 x 28 x 28 Lambda(mnist_resize), nn.Conv2d( 1, 8, 5, padding=2,stride=2), nn.ReLU(), #14 nn.Conv2d( 8,16, 3, padding=1,stride=2), nn.ReLU(), # 7 nn.Conv2d(16,32, 3, padding=1,stride=2), nn.ReLU(), # 4 nn.Conv2d(32,32, 3, padding=1,stride=2), nn.ReLU(), # 2 nn.AdaptiveAvgPool2d(1), Lambda(flatten), nn.Linear(32,data.c) ) . Now run the model on CPU, . model = get_cnn_model(data) # Callbacks from previous notebook cbfs = [Recorder, partial(AvgStatsCallback,accuracy)] opt = optim.SGD(model.parameters(), lr=0.4) learn = Learner(model, opt, loss_func, data) run = Runner(cb_funcs=cbfs) %time run.fit(1, learn) &quot;&quot;&quot; train: [1.7832209375, tensor(0.3780)] valid: [0.68908681640625, tensor(0.7742)] CPU times: user 7.84 s, sys: 5.79 s, total: 13.6 s Wall time: 5.87 s &quot;&quot;&quot; . This is a bit slow, let’s run it on GPU! . Move to GPU: CUDA . A somewhat flexible way: . # 0 means you have 1 GPU device = torch.device(&#39;cuda&#39;, 0) class CudaCallback(Callback): &quot;&quot;&quot;pytorch has .to(device) for model and tensors&quot;&quot;&quot; def __init__(self,device): self.device=device def begin_fit(self): self.model.to(self.device) def begin_batch(self): self.run.xb,self.run.yb = self.xb.to(self.device),self.yb.to(self.device) . A less flexible but more convenient way if you only have 1 GPU: . # This only needs to be called once, and pytorch defaults to it torch.cuda.set_device(device) class CudaCallback(Callback): &quot;&quot;&quot;Now instead of .to(device), just do .cuda()&quot;&quot;&quot; def begin_fit(self): self.model.cuda() def begin_batch(self): self.run.xb,self.run.yb = self.xb.cuda(),self.yb.cuda() cbfs.append(CudaCallback) model = get_cnn_model(data) opt = optim.SGD(model.parameters(), lr=0.4) learn = Learner(model, opt, loss_func, data) run = Runner(cb_funcs=cbfs) %time run.fit(3, learn) &quot;&quot;&quot; train: [1.8033628125, tensor(0.3678, device=&#39;cuda:0&#39;)] valid: [0.502658544921875, tensor(0.8599, device=&#39;cuda:0&#39;)] train: [0.3883639453125, tensor(0.8856, device=&#39;cuda:0&#39;)] valid: [0.205377734375, tensor(0.9413, device=&#39;cuda:0&#39;)] train: [0.17645265625, tensor(0.9477, device=&#39;cuda:0&#39;)] valid: [0.15847452392578126, tensor(0.9543, device=&#39;cuda:0&#39;)] CPU times: user 4.36 s, sys: 1.07 s, total: 5.43 s Wall time: 5.41 s &quot;&quot;&quot; . This is much faster than CPU! For a much deeper model, it will be even faster. . Refactoring the model . First we can regroup all the conv/relu in a single function: . def conv2d(ni, nf, ks=3, stride=2): return nn.Sequential( nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride), nn.ReLU()) . We can do the mnist resize in a batch transform, that we can do with a Callback. . class BatchTransformXCallback(Callback): _order=2 def __init__(self, tfm): self.tfm = tfm def begin_batch(self): self.run.xb = self.tfm(self.xb) def view_tfm(*size): &quot;&quot;&quot; Using closure to create a view or reshape to `size` with any batch size &quot;&quot;&quot; def _inner(x): return x.view(*((-1,)+size)) return _inner mnist_view = view_tfm(1,28,28) cbfs.append(partial(BatchTransformXCallback, mnist_view)) . Get familiar with closure and partial with the above code. . This model can now work on any size input, . nfs = [8,16,32,32] def get_cnn_layers(data, nfs): nfs = [1] + nfs return [ conv2d(nfs[i], nfs[i+1], 5 if i==0 else 3) for i in range(len(nfs)-1) ] + [nn.AdaptiveAvgPool2d(1), Lambda(flatten), nn.Linear(nfs[-1], data.c)] def get_cnn_model(data, nfs): return nn.Sequential(*get_cnn_layers(data, nfs)) #export def get_runner(model, data, lr=0.6, cbs=None, opt_func=None, loss_func = F.cross_entropy): if opt_func is None: opt_func = optim.SGD opt = opt_func(model.parameters(), lr=lr) learn = Learner(model, opt, loss_func, data) return learn, Runner(cb_funcs=listify(cbs)) model = get_cnn_model(data, nfs) learn,run = get_runner(model, data, lr=0.4, cbs=cbfs) model &quot;&quot;&quot; Sequential( (0): Sequential( (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2)) (1): ReLU() ) (1): Sequential( (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) (1): ReLU() ) (2): Sequential( (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) (1): ReLU() ) (3): Sequential( (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) (1): ReLU() ) (4): AdaptiveAvgPool2d(output_size=1) (5): Lambda() (6): Linear(in_features=32, out_features=10, bias=True) ) &quot;&quot;&quot; run.fit(3, learn) &quot;&quot;&quot; train: [1.90592640625, tensor(0.3403, device=&#39;cuda:0&#39;)] valid: [0.743217529296875, tensor(0.7483, device=&#39;cuda:0&#39;)] train: [0.4440590625, tensor(0.8594, device=&#39;cuda:0&#39;)] valid: [0.203494482421875, tensor(0.9409, device=&#39;cuda:0&#39;)] train: [0.1977476953125, tensor(0.9397, device=&#39;cuda:0&#39;)] valid: [0.13920831298828126, tensor(0.9606, device=&#39;cuda:0&#39;)] &quot;&quot;&quot; . Hooks . Manual insertion . Having our own Sequential, we can store each layer activations’ mean and standard deviation. . class SequentialModel(nn.Module): def __init__(self, *layers): super().__init__() self.layers = nn.ModuleList(layers) self.act_means = [[] for _ in layers] self.act_stds = [[] for _ in layers] def __call__(self, x): for i,l in enumerate(self.layers): x = l(x) self.act_means[i].append(x.data.mean()) self.act_stds [i].append(x.data.std ()) return x def __iter__(self): return iter(self.layers) model = SequentialModel(*get_cnn_layers(data, nfs)) learn,run = get_runner(model, data, lr=0.9, cbs=cbfs) run.fit(2, learn) . When we plot the means and stds for the layer activations over the training process, we see they explode and drop off a cliff several times. That is really concerning. We don’t know if the parameters are stuck in zero gradient places and never come back, and only a small number of them are training. . Pytorch hooks . Pytorch call them “hooks”, we have been calling them “callbacks”. . pytorch hooks == callbacks . A minimal example, . model = get_cnn_model(data, nfs) learn,run = get_runner(model, data, lr=0.5, cbs=cbfs) # Global vars. We can use a Hook class to avoid this. act_means = [[] for _ in model] act_stds = [[] for _ in model] def append_stats(i, mod, inp, outp): &quot;&quot;&quot; A hook is attached to a layer, and needs to have a function that takes three arguments: module, input, output. Here we store the mean and std of the output in the correct position of our list. &quot;&quot;&quot; act_means[i].append(outp.data.mean()) act_stds [i].append(outp.data.std()) for i,m in enumerate(model): # Check the pytorch doc for register_forward_hook() for more details m.register_forward_hook(partial(append_stats, i)) run.fit(1, learn) &quot;&quot;&quot; train: [2.2561553125, tensor(0.1835, device=&#39;cuda:0&#39;)] valid: [2.00057578125, tensor(0.3186, device=&#39;cuda:0&#39;)] (now act_means, act_stds are populated) &quot;&quot;&quot; . Check the notebook’s section for the Hook class and Hooks class for better implementation. . Tip: When registered hooks, don’t forget to remove them when not needed, or you will run out of memory. . Use the hook with the with block like this: . for l in model: if isinstance(l, nn.Sequential): init.kaiming_normal_(l[0].weight) l[0].bias.data.zero_() with Hooks(model, append_stats) as hooks: run.fit(2, learn) fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4)) for h in hooks: ms,ss = h.stats ax0.plot(ms[:10]) ax1.plot(ss[:10]) plt.legend(range(6)); fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4)) for h in hooks: ms,ss = h.stats ax0.plot(ms) ax1.plot(ss) plt.legend(range(6)); . Python tip: What the with block does is that, it calls the __exit__() method on the object, in this case hooks, after the block. . After using kaiming_normal_, we see that the rise and drop problem is fixed. But what we are really interested in is that, did many activations get super small? Were they nicely activated? . For that, we can add some more statistics into the hooks. . It turns out that after adding histograms and percentage of small activations, we see that over 90% of our activations are wasted (dead ReLU). This is really concerning. . Generalized ReLU . To avoid wasting most our activations, we can generalize the ReLU by . leaky ReLU | subtract by a number and move it into the negatives a bit | cap it with some max value | . Note: kaiming_normal_ and kaiming_uniform_ perform similarly for this model. Some people think uniform does better because it has less around 0, but not rigorously studied yet. . Batch Normalization . Notebook: 07_batchnorm . Here is the code for batch norm: . class BatchNorm(nn.Module): def __init__(self, nf, mom=0.1, eps=1e-5): super().__init__() # NB: pytorch bn mom is opposite of what you&#39;d expect self.mom,self.eps = mom,eps # mults and adds are like weights and biases, they are the # parameters of the model that we need to learn. # They are the beta and gamma in the batch norm paper self.mults = nn.Parameter(torch.ones (nf,1,1)) self.adds = nn.Parameter(torch.zeros(nf,1,1)) # nn.Module.register_buffer(var, tensor) is the same as # var = tensor, but it does more. It automatically moves # things to GPU, and it saves them in the model for future use self.register_buffer(&#39;vars&#39;, torch.ones(1,nf,1,1)) self.register_buffer(&#39;means&#39;, torch.zeros(1,nf,1,1)) def update_stats(self, x): # mean and var over dim 0, 2, 3, meaning over batch, width, # and height of the images. The result is that each channel/filter # has one number for the mean and for the variance m = x.mean((0,2,3), keepdim=True) v = x.var ((0,2,3), keepdim=True) # lerp means linear interpolation self.means.lerp_(m, self.mom) self.vars.lerp_(v, self.mom) return m,v def forward(self, x): if self.training: with torch.no_grad(): m,v = self.update_stats(x) else: m,v = self.means,self.vars x = (x-m) / (v+self.eps).sqrt() return x*self.mults + self.adds . The lerp part is the exponentially weighted moving average. We define a momentum mom = 0.9, say we have a sequence [3, 5, 4, ...], the moving average is . mu1 = 3 mu2 = 0.9 * mu1 + 5 * 0.1 mu3 = 0.9 * mu2 + 4 + 0.1 ... &quot;&quot;&quot; This is the way of calculating the moving average: mu_n = mom * mu_{n-1} + new_val * (1 - mom) This is a way of linear interpolation (lerp) a * beta + b * (1-beta) So the moving average is equivalent to m.lerp(new_val, mom) &quot;&quot;&quot; . Refer to the above for the definition of the moving average and lerp. . Note: pytorch’s lerp’s momentum is the exact opposite of the momentum we just defined, so it’s a momentum of 0.1 for the case above where we have 0.9. Hence, pytorch’s batchnorm has momentum opposite to the momentum normally defined in the optimizers. (Refer to the note here) . After applying batch norm, we have gotten rid of the rise and crash in the means and stds during training entirely! . . Batch norm deficiencies . Note: We cannot use batch norm for ONLINE LEARNING and SEGMENTATION because of small batch size, the variance is infinity or unstable, and we can’t use it for RNNs. . The layer norm paper proposed the solution to this. The entire paper is essentially this: . class LayerNorm(nn.Module): __constants__ = [&#39;eps&#39;] def __init__(self, eps=1e-5): super().__init__() self.eps = eps self.mult = nn.Parameter(tensor(1.)) self.add = nn.Parameter(tensor(0.)) def forward(self, x): # The only change compared to batchnorm is # instead of (0, 2, 3), we have mean and var over dim (1,2,3) # and we don&#39;t have moving averages. That&#39;s it! m = x.mean((1,2,3), keepdim=True) v = x.var ((1,2,3), keepdim=True) x = (x-m) / ((v+self.eps).sqrt()) return x*self.mult + self.add . Layer norm helps, but it’s not as useful as batch norm. But for RNNs, layer norm is the only thing to use. . There are other attempts to work around this, such as instance norm (for style transfer) and group norm. Check out the group norm paper for details. . . However, none of them are as good as batch norm. Jeremy says he doesn’t know how to fix it for RNNs, but for small batch size, he has some idea: use eps! . class BatchNorm(nn.Module): def __init__(self, nf, mom=0.1, eps=1e-5): ... . It prevents the numbers to blow up. . A better idea: new algorithm for running batch norm! Visit the notebook section 4 and watch the video for more details. The keyword is debiasing. . Ablation study in deep learning research . Jeremy mentioned ablation study briefly. It is good to know . https://stats.stackexchange.com/questions/380040/what-is-an-ablation-study-and-is-there-a-systematic-way-to-perform-it . Papers to read . Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift | Layer Normalization | Instance Normalization: The Missing Ingredient for Fast Stylization | Group Normalization | Revisiting Small Batch Training for Deep Neural Networks | . My Random Thoughts . It is getting really hardcore in part II lessons! The material has great quality and quatity, extremely rare to find even in top universities. Jeremy is really doing great work for DL learners around the world! . The lessons are great practical lessons to learn . Advanced Python | Pytorch fundamentals | Software engineering | Turning paper into code | Code-first research methodology | . My goal is to be able to use the fastai library effectively, and implement things in its style effectively. Then I can even become a fastai contributor. .",
            "url": "http://blog.logancyang.com/note/fastai/2020/05/28/fastai-lesson10.html",
            "relUrl": "/note/fastai/2020/05/28/fastai-lesson10.html",
            "date": " • May 28, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "FastAI Lesson 9: How to Train Your Model",
            "content": "Tip: always include this following code as the 1st cell of any notebook to avoid restarting kernel for imported module changes. . %load_ext autoreload %autoreload 2 . In the last lesson we had an outstanding question about PyTorch’s CNN default initialization. In order to answer it, Jeremy did a bit of research, and we start today’s lesson seeing how he went about that research, and what he learned. . Then we do a deep dive into the training loop, and show how to make it concise and flexible. First we look briefly at loss functions and optimizers, including implementing softmax and cross-entropy loss (and the logsumexp trick). Then we create a simple training loop, and refactor it step by step to make it more concise and more flexible. In the process we’ll learn about nn.Parameter and nn.Module, and see how they work with nn.optim classes. We’ll also see how Dataset and DataLoader really work. . Once we have those basic pieces in place, we’ll look closely at some key building blocks of fastai: callbacks, DataBunch, and Learner. We’ll see how they help, and how they’re implemented. Then we’ll start writing lots of callbacks to implement lots of new functionality and best practices! . Jeremy’s starting comments: how to do research and software development . Jeremy shows how he did research into why sqrt(5) was used in pytorch’s kaiming initialization. . The question is, does this initialization make nn.Conv2d work well. . Notebook: 02a_why_sqrt5 . Note: init.kaiming_normal_(weight, a) is designed to be used after a (leaky) ReLU layer. Here a is the “leak” of the leaky ReLU, i.e. the gradient for the side inputs &lt; 0. . Glossary: rec_fs, or receptive field size, is # elements in a convolution kernel. A 5x5 kernel has rec_fs == 25. . Going through the notebook, the results show that the variance keeps getting smaller as there are more layers added, which is a concerning issue. . Jeremy reached out to the pytorch team and got a response that it was a historical bug from the original torch implementation. Then they created an issue to fix it. . The moral of the story is that in deep learning, don’t assume everything in the library is right. It doesn’t take much to go digging up the code and try making sense of it. . If you find a problem, make your research into a gist and share with the community or the team maintaining the library. . Note: notebook 02b_initializing shows that a series of matrix multiplications can explode or diminish quickly if not properly initialized. Training deep networks require good initializations for this reason, because DNN is essentially a series of matmuls. . Recommended paper: All You Need is a Good Init . Fun fact 1 . A fun fact is that there is a Twitter handle @SELUAppendix that mocks the fact that Self-Normalizing Neural Networks had a 96-page appendix for the math it used to get good inits. If you add dropout or any change to the network you’ll need to go through that math again. . twitter: https://twitter.com/SELUAppendix/status/873882218774528003 . Fun fact 2 . Another fun fact is that pytorch’s linear layer does a transpose first because of historical reasons. We created a linear layer with input dimension 784 and output dimension 50 (hidden layer dimension), so the shape is (784, 50). The pytorch linear layer has shape (50, 784) because the old Lua couldn’t handle batch matrix multiplication without this transpose. . In this particular case, it doesn’t make things slower so it doesn’t matter. But in a lot of cases, these things do matter. . Recreate a modern CNN: the training loop . Notebook: 03_minibatch_training . Create the Cross-Entropy Loss Function . First we introduce softmax, log softmax, and negative log-likelihood (i.e. cross-entropy loss). . The cross entropy loss for some target $y$ and some prediction $ hat{y}$ is given by: . NLL=−∑0≤i≤n−1yi log⁡y^i text{NLL} = - sum_{0 leq i leq n-1} y_i , log hat{y}_iNLL=−0≤i≤n−1∑​yi​logy^​i​ . where . y^i=softmax(x)i=exi∑0≤j≤n−1exj hat{y}_i = text{softmax}( mathbf{x})_i = frac{e^{x_{i}}}{ sum_{0 leq j leq n-1} e^{x_{j}}}y^​i​=softmax(x)i​=∑0≤j≤n−1​exj​exi​​ . But since target $y$s are 1-hot encoded, this can be rewritten as $- log( hat{y}_i)$ where i is the index of the desired target. . . In the case of binary classification, . NLL=−ylog⁡(y^)−(1−y)log⁡(1−y^) text{NLL} = -y log( hat{y}) - (1-y) log(1 - hat{y})NLL=−ylog(y^​)−(1−y)log(1−y^​) . The coefficients before logs are just a way of selection, i.e. y = 1 then select the 1st term, y = 0 then select the 2nd term. . . Tip: multiplying with a one-hot encoded vector is equivalent to a selection where the vector is 1. Don’t do the actual multiplication. . Trick: . def nll(softmax_preds, targets): &quot;&quot;&quot; Use array indexing to select the corresponding values for cross entropy loss. &quot;&quot;&quot; log_sm = softmax_preds.log() return -log_sm[range(targets.shape[0]), targets].mean() # Example: smpred = torch.Tensor([[.01, .98, .01], [.001, .001, .998]]) # - - &quot;&quot;&quot; The negative log of the softmax predictions: very close to 0 at places that were close to 1 in the softmax output tensor([[4.6052, 2.0203e-02, 4.6052], [6.9078, 6.9078, 2.0020e-03]]) &quot;&quot;&quot; targets = torch.LongTensor([1, 2]) &quot;&quot;&quot; nll picks out the elements from each of row in smpred with the indices in targets &quot;&quot;&quot; nll(smpred, targets) &quot;&quot;&quot; This example has very good softmax prediction so the overall cross entropy loss is close to 0 tensor(0.0111) &quot;&quot;&quot; . Numerical Stability Considerations . exp() creates huge numbers, it creates big errors in floating point. To avoid this numerical stability problem, we use the LogSumExp trick. . log⁡(∑j=1nexj)=log⁡(ea∑j=1nexj−a)=a+log⁡(∑j=1nexj−a) log left ( sum_{j=1}^{n} e^{x_{j}} right ) = log left ( e^{a} sum_{j=1}^{n} e^{x_{j}-a} right ) = a + log left ( sum_{j=1}^{n} e^{x_{j}-a} right )log(j=1∑n​exj​)=log(eaj=1∑n​exj​−a)=a+log(j=1∑n​exj​−a) . where a is the maximum of the $x_{j}$. . In code, . # Avoid overflow caused by huge numbers from exp() def logsumexp(x): m = x.max(-1)[0] return m + (x-m[:,None]).exp().sum(-1).log() . pytorch also has logsumexp(). . . Note: in pytorch, . F.nll_loss(F.log_softmax(pred, -1), y_train) is equivalent to F.cross_entropy(pred, y_train). . . Now, we have implemented cross-entropy loss for multiclass classification from scratch. . For accuracy, do this . def accuracy(pred, yb): return (torch.argmax(pred, dim=1)==yb).float().mean() . Notice that pytorch tensor can only use mean() on float type. . Implement the Training Loop . We need to refactor our Module class to be able to get all the model parameters so that we can update them later. . class DummyModule(): def __init__(self, n_in, nh, n_out): self._modules = {} self.l1 = nn.Linear(n_in,nh) self.l2 = nn.Linear(nh,n_out) def __setattr__(self,k,v): &quot;&quot;&quot; This is a special Python dunder method. Every time __init__ is called, this is called to do something for the attributes. &quot;&quot;&quot; # Methods start with _ are internal. Need this condition to # avoid infinite recursion if not k.startswith(&quot;_&quot;): self._modules[k] = v # Set attribute for parent, in this case just the Python object super().__setattr__(k,v) def __repr__(self): return f&#39;{self._modules}&#39; def parameters(self): &quot;&quot;&quot;Returns a generator&quot;&quot;&quot; for l in self._modules.values(): for p in l.parameters(): yield p . Note that __setattr__(key, value) is used as a magical method to populate self._modules dictionary. key turns the attribute variable names into strings. In this case, keys are l1 and l2. . This is exactly the same as if we inherit from pytorch’s nn.Module. Pytorch does the __setattr__ thing to populate the modules dictionary for us when we call super().__init__() in our Model class. . Now the training loop is . def fit(): for epoch in range(epochs): for i in range((n-1)//bs + 1): start_i = i*bs end_i = start_i+bs xb = x_train[start_i:end_i] yb = y_train[start_i:end_i] loss = loss_func(model(xb), yb) loss.backward() with torch.no_grad(): for p in model.parameters(): p -= p.grad * lr model.zero_grad() . pytorch nn.ModuleList . With a list of layers we can init a model like this . class SequentialModel(nn.Module): def __init__(self, layers): super().__init__() self.layers = nn.ModuleList(layers) # The line above is equivalent to # self.layers = layers # for i,l in enumerate(self.layers): self.add_module(f&#39;layer_{i}&#39;, l) def __call__(self, x): for l in self.layers: x = l(x) return x . Note that the layers here are objects with forward and backward defined in the previous lesson, so nn.ModuleList can work. It doesn’t know how to implement forward and backward passes. But nn.Sequential does. . pytorch nn.Sequential . An even simpler way to init a model is . model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10)) . It even takes care of the definition of the forward backward passes. . Implement the Optimizer Class . To refactor the training loop further to be able to just use . opt.step() opt.zero_grad() . instead of . with torch.no_grad(): for p in model.parameters(): p -= p.grad * lr # For the case of Gradual Unfreezing, the user might want to include # only a subset of parameters, so we should avoid model.zero_grad() model.zero_grad() . We define the Optimizer class, . class Optimizer(): def __init__(self, params, lr=0.5): self.params,self.lr=list(params),lr def step(self): &quot;&quot;&quot; This is the purpose of grad computation. The update operations doesn&#39;t need grad itself. &quot;&quot;&quot; with torch.no_grad(): for p in self.params: p -= p.grad * lr def zero_grad(self): &quot;&quot;&quot; Only does zero_grad for the parameters passed in, not all model parameters in case the user wants gradual unfreezing. &quot;&quot;&quot; for p in self.params: p.grad.data.zero_() . Jeremy recommends using something like assert accuracy &gt; 0.7 to make sure the model is doing what it should do after training. It’s an indicator whether there’s a bug that makes the model wrong. . When developing models, we can embrace randomness by not setting the random seed. We need to see how it works with randomness, which bits are stable and which are not. . For research, in some cases we need reproducibility. We set the seeds in those cases. . Dataset and Dataloader . Dataset . With a Dataset class we do minibatches easier. . class Dataset(): def __init__(self, x, y): self.x, self.y = x, y def __len__(self): return len(self.x) def __getitem__(self, i): return self.x[i], self.y[i] train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid) assert len(train_ds)==len(x_train) assert len(valid_ds)==len(x_valid) . Now our training loop becomes . for epoch in range(epochs): for i in range((n-1)//bs + 1): &quot;&quot;&quot; # before: start_i = i*bs end_i = start_i+bs xb = x_train[start_i:end_i] yb = y_train[start_i:end_i] &quot;&quot;&quot; xb, yb = train_ds[i*bs : i*bs+bs] pred = model(xb) loss = loss_func(pred, yb) loss.backward() opt.step() opt.zero_grad() . Dataloader . Previously, our loop iterated over batches (xb, yb) like this: . for i in range((n-1)//bs + 1): xb, yb = train_ds[i*bs : i*bs+bs] ... . Let’s make our loop much cleaner, using a data loader: . for xb, yb in train_dl: ... . Define the Dataloader class that takes a Dataset and a batch size and produces the batches for us. . class DataLoader(): def __init__(self, dataset, bs): self.dataset, self.bs = dataset, bs def __iter__(self): &quot;&quot;&quot; When you call a for loop on something, it calls the __iter__ behind the scene &quot;&quot;&quot; for i in range(0, len(self.dataset), self.bs): yield self.dataset[i:i+self.bs] . Note: yield is a coroutine in Python. . TODO: Make note on Python coroutines and AsyncIO. . To use it, write next(iter(...)), . xb, yb = next(iter(train_dl)) . With data loader, our training loop becomes . &quot;&quot;&quot; We now have the cleanest form of a training loop. One iteration has 5 steps. &quot;&quot;&quot; def fit(): for epoch in range(epochs): for xb,yb in train_dl: # 1. Get predictions pred = model(xb) # 2. Calculate loss loss = loss_func(pred, yb) # 3. Calculate gradients loss.backward() # 4. Update the parameters opt.step() # 5. Reset the gradients opt.zero_grad() . This is quite neat and beautiful! . One problem that remains is that we are looping through the data in order. We need to do random sampling to let each batch be different. . Random Sampling . Define a Sampler class . class Sampler(): def __init__(self, dataset, bs, shuffle=False): self.n, self.bs, self.shuffle = len(dataset), bs, shuffle def __iter__(self): self.idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n) for i in range(0, self.n, self.bs): yield self.idxs[i:i+self.bs] def collate(b): xs,ys = zip(*b) return torch.stack(xs),torch.stack(ys) class DataLoader(): def __init__(self, dataset, sampler, collate_fn=collate): self.dataset, self.sampler, self.collate_fn = dataset,sampler, collate_fn def __iter__(self): for s in self.sampler: yield self.collate_fn([self.dataset[i] for i in s]) train_samp = Sampler(train_ds, bs, shuffle=True) valid_samp = Sampler(valid_ds, bs, shuffle=False) train_dl = DataLoader(train_ds, sampler=train_samp, collate_fn=collate) valid_dl = DataLoader(valid_ds, sampler=valid_samp, collate_fn=collate) xb,yb = next(iter(valid_dl)) plt.imshow(xb[0].view(28,28)) yb[0] . Pytorch’s Dataloader . from torch.utils.data import DataLoader, SequentialSampler, RandomSampler train_dl = DataLoader( train_ds, bs, sampler=RandomSampler(train_ds), collate_fn=collate) valid_dl = DataLoader( valid_ds, bs, sampler=SequentialSampler(valid_ds), collate_fn=collate) # Or omit the sampler and collate function, the ones we implemented are # the default train_dl = DataLoader(train_ds, bs, shuffle=True, drop_last=True) valid_dl = DataLoader(valid_ds, bs, shuffle=False) . Validation . In pytorch, model has a training attribute which is boolean. . Take this fitting loop for example, model.training is set by model.train() and model.eval(). . def fit(epochs, model, loss_func, opt, train_dl, valid_dl): for epoch in range(epochs): # Handle batchnorm / dropout model.train() # print(model.training) -&gt; True for xb,yb in train_dl: loss = loss_func(model(xb), yb) loss.backward() opt.step() opt.zero_grad() model.eval() # print(model.training) -&gt; False with torch.no_grad(): tot_loss,tot_acc = 0.,0. for xb,yb in valid_dl: pred = model(xb) tot_loss += loss_func(pred, yb) tot_acc += accuracy (pred,yb) nv = len(valid_dl) print(epoch, tot_loss/nv, tot_acc/nv) return tot_loss/nv, tot_acc/nv . This is useful because for some layers such as batch norm and dropout, they should do their thing in training but they are different during evaluation. This makes sure of that. . Also notice that the loss accumulation in the above code only works when batch sizes are equal. With varying batch sizes, we need weighted average. . Question: why zero_grad() in every iteration? . Answer: . We do batch gradient descent and it works by accumulating gradients in each batch. We would want to be able to stitch different components together for the gradients by not calling zero_grad() in some cases, so we make it a seperate method. . | Having a separate zero_grad() in the Optimizer class, rather than something like the code below where we zero out the gradients after each step, enables us to accumulate gradients. . | class Optimizer(): def __init__(self, params, lr=0.5): self.params,self.lr=list(params),lr def step(self): with torch.no_grad(): for p in self.params: p -= p.grad * lr p.grad.data.zero_() . For example, if we have big images to train with and can only fit a smaller number in the GPU, we can do . for epoch in range(epochs): for i in range((n-1)//bs + 1): xb,yb = train_ds[i*bs : i*bs+bs] pred = model(xb) loss = loss_func(pred, yb) loss.backward() # THIS EFFECTIVELY DOUBLED OUR BATCH SIZE! if i % 2: opt.step() opt.zero_grad() . Of course, we can have better API design by adding auto_zero into the Optimizer, e.g. . class Optimizer(): def __init__(self, params, lr=0.5, auto_zero=True): self.params, self.lr, self.auto_zero = list(params), lr, auto_zero def step(self): with torch.no_grad(): for p in self.params: p -= p.grad * lr if self.auto_zero: self.zero_grad() def zero_grad(self): for p in self.params: p.grad.data.zero_() . This removes the need to call zero_grad() in every batch iteration, which could potentially avoid bugs. But this is not something pytorch has done. . Callbacks . fast.ai docs on callbacks . Notebook: 04_callbacks . To recap, the training loop we implemented is . . . Different kinds of models have different training loops. It’s intractable to write each type of training loop and it’s bad code design. A better way is to insert callbacks at the right events. . . . Here are some other callback examples in fastai. . . This is the callbacks for a GAN training loop, . . Refactoring fit() . We start by refactoring the fit() function. . # Before fit(epochs, model, loss_func, opt, train_dl, valid_dl) # We get nervous when a function takes in too many parameters # Need to group relevant ones together. # E.g. the data loaders can be grouped together first into `DataBunch` class DataBunch(): def __init__(self, train_dl, valid_dl, c=None): self.train_dl, self.valid_dl, self.c = train_dl, valid_dl, c @property def train_ds(self): return self.train_dl.dataset @property def valid_ds(self): return self.valid_dl.dataset def get_model(data, lr=0.5, nh=50): m = data.train_ds.x.shape[1] model = nn.Sequential( nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,data.c) ) return model, optim.SGD(model.parameters(), lr=lr) class Learner(): # Notice the Learner class has no logic at all # It&#39;s just a useful device for storing things def __init__(self, model, opt, loss_func, data): self.model, self.opt, self.loss_func, self.data = model, opt, loss_func, data x_train, y_train, x_valid, y_valid = get_data() train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid) nh, bs = 50, 64 c = y_train.max().item() + 1 loss_func = F.cross_entropy data = DataBunch(*get_dls(train_ds, valid_ds, bs), c) learn = Learner(*get_model(data), loss_func, data) # After fit(epochs, learn) . Note: Python @property decorator helps create a getter method so that the property can be accessed by object dot the function name. For more info about it, check here. . Inside the fit() function, model becomes learn.model, data becomes learn.data. . def fit(epochs, learn): for epoch in range(epochs): learn.model.train() for xb,yb in learn.data.train_dl: loss = learn.loss_func(learn.model(xb), yb) loss.backward() learn.opt.step() learn.opt.zero_grad() learn.model.eval() with torch.no_grad(): tot_loss,tot_acc = 0.,0. for xb,yb in learn.data.valid_dl: pred = learn.model(xb) tot_loss += learn.loss_func(pred, yb) tot_acc += accuracy (pred,yb) nv = len(learn.data.valid_dl) print(epoch, tot_loss/nv, tot_acc/nv) return tot_loss/nv, tot_acc/nv loss, acc = fit(1, learn) . Add Callbacks . Implement the Callback class, . class Callback(): def begin_fit(self, learn): self.learn = learn return True def after_fit(self): return True def begin_epoch(self, epoch): self.epoch=epoch return True def begin_validate(self): return True def after_epoch(self): return True def begin_batch(self, xb, yb): self.xb,self.yb = xb,yb return True def after_loss(self, loss): self.loss = loss return True def after_backward(self): return True def after_step(self): return True . Then the CallbackHandler class, . class CallbackHandler(): def __init__(self,cbs=None): # cbs is a list of Callback objects self.cbs = cbs if cbs else [] def begin_fit(self, learn): self.learn,self.in_train = learn,True learn.stop = False res = True # Loops through callbacks, `res` means resume # In the later Runner implementation this is not needed for cb in self.cbs: res = res and cb.begin_fit(learn) return res def after_fit(self): res = not self.in_train for cb in self.cbs: res = res and cb.after_fit() return res def begin_epoch(self, epoch): self.learn.model.train() self.in_train=True res = True for cb in self.cbs: res = res and cb.begin_epoch(epoch) return res def begin_validate(self): self.learn.model.eval() self.in_train=False res = True for cb in self.cbs: res = res and cb.begin_validate() return res def after_epoch(self): res = True for cb in self.cbs: res = res and cb.after_epoch() return res def begin_batch(self, xb, yb): res = True for cb in self.cbs: res = res and cb.begin_batch(xb, yb) return res def after_loss(self, loss): res = self.in_train for cb in self.cbs: res = res and cb.after_loss(loss) return res def after_backward(self): res = True for cb in self.cbs: res = res and cb.after_backward() return res def after_step(self): res = True for cb in self.cbs: res = res and cb.after_step() return res def do_stop(self): try: return self.learn.stop finally: self.learn.stop = False . Callbacks in Action . To demonstrate the ways to use these callbacks, we have . def one_batch(xb, yb, cb): if not cb.begin_batch(xb,yb): return loss = cb.learn.loss_func(cb.learn.model(xb), yb) if not cb.after_loss(loss): return loss.backward() if cb.after_backward(): cb.learn.opt.step() if cb.after_step(): cb.learn.opt.zero_grad() def all_batches(dl, cb): for xb,yb in dl: one_batch(xb, yb, cb) if cb.do_stop(): return def fit(epochs, learn, cb): if not cb.begin_fit(learn): return for epoch in range(epochs): if not cb.begin_epoch(epoch): continue all_batches(learn.data.train_dl, cb) if cb.begin_validate(): with torch.no_grad(): all_batches(learn.data.valid_dl, cb) if cb.do_stop() or not cb.after_epoch(): break cb.after_fit() class TestCallback(Callback): def begin_fit(self,learn): super().begin_fit(learn) self.n_iters = 0 return True def after_step(self): self.n_iters += 1 print(self.n_iters) if self.n_iters&gt;=10: self.learn.stop = True return True fit(1, learn, cb=CallbackHandler([TestCallback()])) &quot;&quot;&quot; 1 2 3 4 5 6 7 8 9 10 &quot;&quot;&quot; . Note: pytorch hooks are a kind a callbacks that can be more granular than these ones, they can be inserted in model forward and backward passes, so we can do something between layers. . Runner: further cleaning it up . We can further refactor this since there are a lot of duplications. Refer to the notebook 04_callbacks here and check the Runner section. It contains some nice Python power user tricks such as enabling something like self(&#39;begin_fit&#39;) by . class Runner(): ... def __call__(self, cb_name): for cb in sorted(self.cbs, key=lambda x: x._order): f = getattr(cb, cb_name, None) if f and f(): return True return False . This part of the lecture video is worth revisiting for upleveling Python coding skills. . Annealing . Notebook: 05_anneal. . Note: Jeremy uses %debug in cells with pdb to debug. Check shapes, check the things an object contains, etc. . We define two new callbacks: the Recorder to save track of the loss and our scheduled learning rate, and a ParamScheduler that can schedule any hyperparameter as long as it’s registered in the state_dict of the optimizer. . It’s good to use parameter scheduling for everything. . class Recorder(Callback): def begin_fit(self): self.lrs,self.losses = [],[] def after_batch(self): if not self.in_train: return self.lrs.append(self.opt.param_groups[-1][&#39;lr&#39;]) self.losses.append(self.loss.detach().cpu()) def plot_lr (self): plt.plot(self.lrs) def plot_loss(self): plt.plot(self.losses) class ParamScheduler(Callback): _order=1 def __init__(self, pname, sched_func): self.pname,self.sched_func = pname,sched_func def set_param(self): # it&#39;s called param_groups in pytorch, and layer_groups in fastai for pg in self.opt.param_groups: pg[self.pname] = self.sched_func(self.n_epochs/self.epochs) def begin_batch(self): if self.in_train: self.set_param() . Trick: We use partial() from functools and decorators. . Python partial function . A partial function allows us to call a second function with fixed values in certain arguments. It avoids replicating code. . def power(base, exponent): return base**exponent def squared(base): return base ** 2 # The above is bad # Instead do this from functools import partial squared = partial(power, exponent=2) . Python decorators . A decorator is a function that returns another function. . def divide(a, b): return a/b # vs. def smart_divide(func): def inner(a,b): print(&quot;I am going to divide&quot;,a,&quot;and&quot;,b) if b == 0: print(&quot;Whoops! cannot divide&quot;) return return func(a,b) return inner @smart_divide def divide(a,b): return a/b # this is equivalent to smart_divide(divide) # Or make it work for any number of arguments with *args, **kargs def works_for_all(func): def inner(*args, **kwargs): print(&quot;I can decorate any function&quot;) return func(*args, **kwargs) return inner . Annealer decorator . def annealer(f): def _inner(start, end): return partial(f, start, end) return _inner @annealer def sched_lin(start, end, pos): return start + pos*(end-start) f = sched_lin(1, 2) f(0.3) # 1.3 . Jupyter has an advantage over an IDE that when you hit shift-tab to check what sched_lin takes in, it shows start, end because it runs a Python process and knows it’s decorated. . Now, using this approach we can define different schedulers . # sched_cos is the default for fastai @annealer def sched_cos(start, end, pos): return start + (1 + math.cos(math.pi*(1-pos))) * (end-start) / 2 @annealer def sched_no(start, end, pos): return start @annealer def sched_exp(start, end, pos): return start * (end/start) ** pos def cos_1cycle_anneal(start, high, end): return [sched_cos(start, high), sched_cos(high, end)] . Trick: pytorch tensors can’t be plotted directly because they don’t have ndim, but we can add it ourselves with the line below! . #This monkey-patch is there to be able to plot tensors torch.Tensor.ndim = property(lambda x: len(x.shape)) . In the next lesson, we will look at pytorch hooks and other advanced features. . Papers . Self-Normalizing Neural Networks (SELU) | Exact solutions to the nonlinear dynamics of learning in deep linear neural networks (orthogonal initialization) | All you need is a good init | Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification– 2015 paper that won ImageNet, and introduced ResNet and Kaiming Initialization. | Fixup Initialization: Residual Learning Without Normalization – paper highlighting importance of normalisation - training 10,000 layer network without regularisation | . Other helpful resources . Sylvain’s talk, An Infinitely Customizable Training Loop (from the NYC PyTorch meetup) and the slides that go with it | Why do we need to set the gradients manually to zero in pytorch? | What is torch.nn really? | Blog post explaining decorators | Primer on Python Decorators | Introduction to Mixed Precision Training, Benchmarks using fastai | Explanation and derivation of LogSumExp | Blog post about callbacks in fastai #1 | Blog post about callbacks in fastai #2 | Blog post about weight initialization | .",
            "url": "http://blog.logancyang.com/note/fastai/2020/05/25/fastai-lesson9.html",
            "relUrl": "/note/fastai/2020/05/25/fastai-lesson9.html",
            "date": " • May 25, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "FastAI Lesson 8: Backprop from the Foundations",
            "content": "Jeremy’s starting comments . “cutting-edge deep learning” now is more and more about engineering and not papers. It’s about who can make things in code that work properly. | Part II of fastai is bottom-up learning with code. It helps you understand the connections between algorithms, and make your own algorithm for your own problem, and debug, profile, maintain it. | Swift and Julia are the promising languages for high performance computing. | . Swift for TensorFlow vs. PyTorch . . Swift is a thin layer on top of LLVM. LLVM compiles Swift code to super fast machine code. . Python is the opposite. We write Python as an interface but things usually run in C++. It prevents doing deep dives as we shall see in this course. . Opportunity: join the Swift for TF community to contribute and be a pioneer in this field! . Recreate fastai Library from Scratch . . Benefit of doing this . Really experiment | Understand it by creating it | Tweak everything | Contribute | Correlate papers with code | . Opportunities . Make homework at the cutting edge | There are few DL practitioners that know what you know now | Experiment lots, especially in your area of expertise | Much of what you find will have not be written about before | Don’t wait to be perfect before you start communicating. Write stuff down for the person you were 6 months ago. | . How to Train a Good Model . . 5 steps of reducing overfitting . more data | data augmentation | generalization architectures | regularization | reducing architecture complexity (this should be the last step) | . Start Reading Papers . Get pass the fear of Greek letters! It’s just code. . Opportunity: there are blog posts that describing a paper better than the paper does. Write these blog posts! . Read blog posts and also the paper itself. . Goal: Recreating a Modern CNN Model . . For development, Jeremy recommends nbdev for library development in Jupyter notebook. . Tip: Python’s fire library lets you convert a function into CLI. . notebook: 01_matmul . Important: get familiar with PyTorch Tensors. It can do everything like a numpy array and it can run on GPU. . tensor.view() is equivalent to nparray.reshape(). . Creating Matrix Multiplication . Pure Python with 3 nested loops (speed lvl0) . Implement matrix multiplaction with 3 loops. . def matmul(a,b): ar,ac = a.shape # n_rows * n_cols br,bc = b.shape assert ac==br c = torch.zeros(ar, bc) for i in range(ar): for j in range(bc): for k in range(ac): # or br c[i,j] += a[i,k] * b[k,j] return c . This is super slow because it’s in Python. A (5, 784) by (784, 10) matrix multiplication took ~1s. MNIST needs ~10K of them, so it will take 10K seconds, that’s unacceptable. . The way to speed this up is to use something other than Python – use PyTorch where it uses ATen (C++) under the hood. . Tip: to get LaTeX formula, go to Wikipedia and click edit. Or go to Arxiv and do Download other format on the top right, then download source. . Elementwise vector operations with 2 nested loops (speed lvl1) . def matmul(a,b): ar,ac = a.shape br,bc = b.shape assert ac==br c = torch.zeros(ar, bc) for i in range(ar): for j in range(bc): # Any trailing &quot;,:&quot; can be removed # a[i, :] means the whole ith row # b[:, j] means the whole jth col # This is not really Python, it tells Python to call C c[i,j] = (a[i,:] * b[:,j]).sum() return c . This is hundreds of times faster. Next, broadcasting makes it even faster. . Tip: to test equal for floats, set a tolerance and use something like torch.allclose(). Float’s implementation gives small numerical errors. . Broadcasting with 1 loop (speed lvl2) . Broadcasting is the most powerful tool to speed things up. It gets rid of for loops and does implicit broadcast loops. . Any time we use broadcasting, we are using C speed (on CPU) or CUDA speed (on GPU). . Easiest example is a + 1 where a is a tensor. 1 is automatically turned into a tensor that matches the shape of a. This is scalar to tensor. . We can also broadcast vector to higher order tensors. . c = tensor([10.,20,30]) m = tensor([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]]) c + m &quot;&quot;&quot; tensor([[11., 22., 33.], [14., 25., 36.], [17., 28., 39.]]) We don&#39;t really copy the rows, the rows are given a *stride* of 1. &quot;&quot;&quot; # To check what c looks like after broadcasting t = c.expand_as(m); t &quot;&quot;&quot; tensor([[10., 20., 30.], [10., 20., 30.], [10., 20., 30.]]) Use t.storage() we can check the memory usage! It shows we only have one row in memory, not really making a full matrix during broadcasting. Use t.stride() shows (0, 1) meaning stride is 0 for rows, 1 for columns. This idea is used in all linear algebra libraries. &quot;&quot;&quot; . To add a dimension, use unsqueeze(axis), or, use None at that axis when indexing. Example: . # These are not in-place, c is not updated c.unsqueeze(0) # or c[None, :] &quot;&quot;&quot; tensor([[10., 20., 30.]]) &quot;&quot;&quot; c.unsqueeze(1) # or c[:, None] &quot;&quot;&quot; tensor([[10.], [20.], [30.]]) &quot;&quot;&quot; . Tip: always use None over unsqueeze because it’s more convenient and we can add more than one axis. . Trick: . We can omit trailing ,: as in c[None, :] == c[None] | We can use ... as in c[:, None] == c[..., None]. This is helpful especially when we don’t know the rank of the tensor. | . def matmul(a,b): ar,ac = a.shape br,bc = b.shape assert ac==br c = torch.zeros(ar, bc) for i in range(ar): # c[i,j] = (a[i,:] * b[:,j]).sum() # previous # Notice we got rid of loop j, that&#39;s why it&#39;s even faster # than the elementwise ops previously c[i] = (a[i].unsqueeze(-1) * b).sum(dim=0) # Or c[i] = (a[i][:, None] * b).sum(dim=0) return c . Concrete example for the above code: . a = torch.tensor([ [1, 2, 3], [4, 5, 6] ]) b = torch.tensor([ [10, 10], [20, 20], [30, 30] ]) # For i = 0 a0 = a[0][:, None] &quot;&quot;&quot; a0: tensor([[1], [2], [3]]) b: tensor([[10, 10], [20, 20], [30, 30]]) a0 * b: we rotate a0, a row vector to be a col vector, and broadcast into shape of b, and does elementwise * tensor([[10, 10], [40, 40], [90, 90]]) Then sum over axis=0 (rows) tensor([140, 140]) This is the result of matmul for a row vector a[0] and matrix b. Do the same for the 2nd row of a, we have a @ b: tensor([[140, 140], [320, 320]]) &quot;&quot;&quot; . Now we only have one level of loop for the matrix multiplication, and it’s 1000x times faster than the raw Python version of 3 nested loops. . Broadcasting Rule . c = tensor([10., 20., 30.]) # Add leading axis 0, row c[None,:], c[None,:].shape &quot;&quot;&quot; tensor([[10., 20., 30.]]), torch.Size([1, 3]) &quot;&quot;&quot; # Add trailing axis, col c[:,None], c[:,None].shape &quot;&quot;&quot; tensor([[10.], [20.], [30.]]) torch.Size([3, 1]) &quot;&quot;&quot; # How does this do broadcasting? # Here is where the BROADCASTING RULE comes in # Where there&#39;s a missing dimension, np/pytorch fills in a dimension # with size 1. A dim of size 1 can be broadcast into any size. # E.g. (1, 1, 3) * (256, 256, 3) -&gt; (256, 256, 3) c[None,:] * c[:,None] &quot;&quot;&quot; tensor([[100., 200., 300.], [200., 400., 600.], [300., 600., 900.]]) &quot;&quot;&quot; # Similarly c[None] &gt; c[:,None] &quot;&quot;&quot; tensor([[0, 1, 1], [0, 0, 1], [0, 0, 0]], dtype=torch.uint8) &quot;&quot;&quot; . When operating on two arrays/tensors, Numpy/PyTorch compares their shapes element-wise. It starts with the trailing dimensions, and works its way forward. Two dimensions are compatible when . they are equal, or | one of them is 1, in which case that dimension is broadcasted to make it the same size | . Arrays do not need to have the same number of dimensions. For example, if you have a 256*256*3 array of RGB values, and you want to scale each color in the image by a different value, you can multiply the image by a one-dimensional array with 3 values. Lining up the sizes of the trailing axes of these arrays according to the broadcast rules, shows that they are compatible: . Image (3d array): 256 x 256 x 3 Scale (1d array): 3 Result (3d array): 256 x 256 x 3 . The numpy documentation includes several examples of what dimensions can and can not be broadcast together. . Einstein Summation with no loops (speed lvl3) . Einstein summation notation: ik,kj-&gt;ij . ik,kj: input . ij: output . Each letter is the size of a dimension. Repeated letters indicate dot product. . # c[i,j] += a[i,k] * b[k,j] # c[i,j] = (a[i,:] * b[:,j]).sum() def matmul(a,b): return torch.einsum(&#39;ik,kj-&gt;ij&#39;, a, b) . This is even faster. And we can create new operations easily such as batch matrix multiplication: bik,bkj-&gt;bij. . But having a string as a language inside of a language is not a good idea, e.g. regex. We should be able to write Swift and Julia operating at this speed in a few years. . PyTorch op with no loops (speed lvl4) . PyTorch’s matmul or @ operation is even faster than einsum, it’s ~50K times faster than raw Python. Because to do really fast matrix multiplication on big matrices, it can’t fit in CPU cache and needs to be chopped down into smaller matrices. BLAS libraries do that. Examples are NVidia’s cuBLAS, and Intel’s mkl. . Fully Connected Nets . Forward pass . First, load the data and apply normalization. . Note: use the training set’s mean and std to normalize the validation set! Always make sure the validation set and the training set are normalized in the same way. . # num hidden nh = 50 # simplified kaiming init / he init: divide by sqrt(n_inputs). m is # examples w1 = torch.randn(m,nh)/math.sqrt(m) b1 = torch.zeros(nh) w2 = torch.randn(nh,1)/math.sqrt(nh) b2 = torch.zeros(1) # This should be ~ (0,1) (mean,std)... x_valid.mean(),x_valid.std() def lin(x, w, b): &quot;&quot;&quot;Linear layer&quot;&quot;&quot; return x@w + b t = lin(x_valid, w1, b1) # The effect of Kaiming init: makes the linear output have # ~0 mean and 1 std t.mean(),t.std() . Kaiming init is a very important factor to train deep networks. Some researchers trained a 10K-layer network without normalization layers just with careful initialization. . def relu(x): return x.clamp_min(0.) . Tip: if there’s a function for some calculation in pytorch, such as clamp_min, it’s generally written in C and it’s faster than your implementation in Python. . Kaiming Init . From pytorch docs: a: the negative slope of the rectifier used after this layer (0 for ReLU by default) . std=2(1+a2)×fan_in text{std} = sqrt{ frac{2}{(1 + a^2) times text{fan _in}}}std=(1+a2)×fan_in2​ . ​ . This was introduced in the paper that described the Imagenet-winning approach from He et al: Delving Deep into Rectifiers, which was also the first paper that claimed “super-human performance” on Imagenet (and, most importantly, it introduced resnets!) . Simply put, for ReLU, if the inputs are mean 0 and std 1, the numbers below 0 are clipped so we lose half the variance. The way to fix it is to time it by 2, proposed in the paper. . # kaiming init / he init for relu w1 = torch.randn(m,nh)*math.sqrt(2/m) w1.mean(),w1.std() # (tensor(0.0001), tensor(0.0508)) t = relu(lin(x_valid, w1, b1)) t.mean(),t.std() # (tensor(0.5678), tensor(0.8491)) . Conv layers can also be looked at as linear layers with a special weight matrix where there are a lot of 0s for the pixels outside the filter, so this initialization does the same thing for them. . In pytorch, . init.kaiming_normal_(w1, mode=&#39;fan_out&#39;) # check doc by `init.kaiming_normal_??` . Then we can write the model and the loss function. We use MSE for now for simplicity. . def model(xb): l1 = lin(xb, w1, b1) l2 = relu(l1) l3 = lin(l2, w2, b2) return l3 def mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean() y_train,y_valid = y_train.float(),y_valid.float() preds = model(x_train) mse(preds, y_train) . Backward pass . All you need to know about matrix calculus from scratch: https://explained.ai/matrix-calculus/index.html . def mse_grad(inp, targ): # grad of loss with respect to output of previous layer inp.grad = 2. * (inp.squeeze() - targ).unsqueeze(-1) / inp.shape[0] def relu_grad(inp, out): # grad of relu with respect to input activations inp.grad = (inp&gt;0).float() * out.grad def lin_grad(inp, out, w, b): # grad of matmul with respect to input inp.grad = out.grad @ w.t() w.grad = (inp.unsqueeze(-1) * out.grad.unsqueeze(1)).sum(0) b.grad = out.grad.sum(0) def forward_and_backward(inp, targ): # forward pass: l1 = inp @ w1 + b1 l2 = relu(l1) out = l2 @ w2 + b2 # we don&#39;t actually need the loss in backward! # this is just here if we want to print it out! loss = mse(out, targ) # backward pass: mse_grad(out, targ) lin_grad(l2, out, w2, b2) relu_grad(l1, l2) lin_grad(inp, l1, w1, b1) . Layers as Classes . Observe the above code, we see each function for grad can take in inputs, weight and bias. We can make layer classes to have inputs, weight and bias, and define a forward() to calculate outputs, and backward() to calculate the gradients. Refactor the previous code, . class Relu(): # Notice this Relu() class does not have __init__ # Instantiating an instance is just `Relu()` # dunder call means we can use the class name as a function! def __call__(self, inp): self.inp = inp self.out = inp.clamp_min(0.)-0.5 return self.out def backward(self): self.inp.g = (self.inp&gt;0).float() * self.out.g class Lin(): def __init__(self, w, b): self.w, self.b = w, b def __call__(self, inp): self.inp = inp self.out = inp@self.w + self.b return self.out def backward(self): self.inp.g = self.out.g @ self.w.t() # Creating a giant outer product, just to sum it, is inefficient! self.w.g = (self.inp.unsqueeze(-1) * self.out.g.unsqueeze(1)).sum(0) self.b.g = self.out.g.sum(0) class Mse(): def __call__(self, inp, targ): self.inp = inp self.targ = targ self.out = (inp.squeeze() - targ).pow(2).mean() return self.out def backward(self): self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0] class Model(): def __init__(self, w1, b1, w2, b2): self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)] self.loss = Mse() def __call__(self, x, targ): for l in self.layers: x = l(x) return self.loss(x, targ) def backward(self): self.loss.backward() for l in reversed(self.layers): l.backward() . Layers as Modules . We see that all layers have outputs, forward and backward passes. Further refactoring, we introduce the Module class (similar to pytorch nn.Module). . class Module(): def __call__(self, *args): self.args = args self.out = self.forward(*args) return self.out def forward(self): raise Exception(&#39;not implemented&#39;) def backward(self): self.bwd(self.out, *self.args) class Relu(Module): def forward(self, inp): return inp.clamp_min(0.) - 0.5 def bwd(self, out, inp): inp.g = (inp&gt;0).float() * out.g class Lin(Module): def __init__(self, w, b): self.w,self.b = w,b def forward(self, inp): return inp@self.w + self.b def bwd(self, out, inp): inp.g = out.g @ self.w.t() self.w.g = inp.t() @ out.g self.b.g = out.g.sum(0) class Mse(Module): def forward (self, inp, targ): return (inp.squeeze() - targ).pow(2).mean() def bwd(self, out, inp, targ): inp.g = 2 * (inp.squeeze()-targ).unsqueeze(-1) / targ.shape[0] class Model(): def __init__(self): self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)] self.loss = Mse() def __call__(self, x, targ): for l in self.layers: x = l(x) return self.loss(x, targ) def backward(self): self.loss.backward() for l in reversed(self.layers): l.backward() . Equivalent Code in PyTorch . from torch import nn class Model(nn.Module): def __init__(self, n_in, nh, n_out): super().__init__() self.layers = [ nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out) ] self.loss = mse def __call__(self, x, targ): for l in self.layers: x = l(x) return self.loss(x.squeeze(), targ) model = Model(m, nh, 1) %time loss = model(x_train, y_train) %time loss.backward() . In the next lesson, we will get the training loop, the optimizer, and other loss functions. . Homework . Read Kaiming’s paper: Delving Deep into Rectifiers. Focus on section 2.2. | Xavier init paper is also really readable, we will implement a lot from it. | . My Random Thoughts . The career path after fast.ai should be something that mixes engineering and research: . Applied scientist or research engineer. It’s different from the usual “data scientist”, which focuses on analytics, business metrics and non-DL work (lacks in engineering and research in DL); it’s also different from ML engineer, which focuses on productionizing and maintaining models (lacks in research). | Open source contribution to key DL projects. Swift for Tensorflow is one advocated by Jeremy. | . Some one who can implement DL frameworks from scratch and grasp key DL research shouldn’t be a “data scientist” or “ML engineer” in a non-research organization. There are tons of data scientists and ML engineers out there, but those who can reach high level of fast.ai competence are rare. . Demonstrate the knowledge by blogging and making a great project. .",
            "url": "http://blog.logancyang.com/note/fastai/2020/05/22/fastai-lesson8.html",
            "relUrl": "/note/fastai/2020/05/22/fastai-lesson8.html",
            "date": " • May 22, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "FastAI Lesson 7: Resnets from scratch; U-net; Generative adversarial networks; RNNs",
            "content": "Notebook: lesson7-resnet-mnist . PyTorch puts channel at the 1st dimension by default. An image in MNIST is a (1,28, 28) rank 3 tensor. . In fastai, there is a difference between validation and test set. Test set doesn’t have label. Use validation set for model development. If you want to do inference on many things at a time and not one at a time, set the data as test instead of valid. . Initial steps: . Create ItemList from image folders | Split into train and valid. | . Tip: for data augmentation, MNIST can&#39;t have much transformation, you can&#39;t flip it because it changes the meaning of the number, you can&#39;t zoom because it&#39;s low res. The only transform is to add random padding. Do this transform on training set but not validation set. If not using a pretrained model, don’t pass in stat in normalize() for databunch, it grabs a subset of the data at random and figures out how to normalize. . plot_multi(_plot, nrow, ncol, figsize=()) is a fastai function that plots multiple examples in a grid. Define _plot first for what to show. . # Showing a batch of data using the DataBlock API xb,yb = data.one_batch() xb.shape,yb.shape # DataBunch has .show_batch() data.show_batch(rows=3, figsize=(5,5)) . Then we define the convolution function with fixed kernel size, stride and padding. . # ni is # input channels, nf is # output filters (kernels) def conv(ni,nf): return nn.Conv2d(ni, nf, kernel_size=3, stride=2, padding=1) model = nn.Sequential( # 1 channel in, 8 channels out (picked by us), output size 8*14*14 conv(1, 8), nn.BatchNorm2d(8), nn.ReLU(), # 8 channel in, 16 channels out (picked by us), output size 16*7*7 conv(8, 16), # 7 nn.BatchNorm2d(16), nn.ReLU(), # 16 channel in, 32 channels out (picked by us), output size 32*4*4 conv(16, 32), # 4 nn.BatchNorm2d(32), nn.ReLU(), # 32 channel in, 16 channels out (picked by us), output size 16*2*2 conv(32, 16), # 2 nn.BatchNorm2d(16), nn.ReLU(), # 16 channel in, 10 channels out (picked by us), output size 10*1*1 conv(16, 10), # 1 nn.BatchNorm2d(10), Flatten() # remove (1,1) grid ) . . Trick: Flatten() gets rid of all the unit axes (the axes with 1s)! (10, 1, 1) becomes just (10,), a flat vector of dim 10! # create learner learn = Learner(data, model, loss_func = nn.CrossEntropyLoss(), metrics=accuracy) # print learner summary print(learn.summary()) # pop data onto GPU xb = xb.cuda() # check model output shape model(xb).shape . This is a model we built from scratch with a simple CNN architecture, it takes 12s to train on my GPU and got 98.8% accuracy! Already super good. . Refactor . fastai has conv_layer so we can skip writing all the batch norm and relu’s. . def conv2(ni,nf): return conv_layer(ni,nf,stride=2) model = nn.Sequential( conv2(1, 8), # 14 conv2(8, 16), # 7 conv2(16, 32), # 4 conv2(32, 16), # 2 conv2(16, 10), # 1 Flatten() # remove (1,1) grid ) learn = Learner(data, model, loss_func = nn.CrossEntropyLoss(), metrics=accuracy) learn.fit_one_cycle(10, max_lr=0.1) . It’s the same as previous code, just looks better. Train 10 epochs, we can get to 99%+ accuracy. . Introduce the Residual Block . The residual block is a revolutionary technique in computer vision. . Kaiming He et. al. at Microsoft Research initially found that a 56-layer CNN was performing worse than a 20-layer CNN which made no sense. He created an architecture where a 56-layer CNN contains the 20-layer CNN, by adding some skip connections that skipped some conv layers. That way, it must be as least as good as the 20-layer CNN because the deeper CNN could just set the skipped conv layers to 0 and only keep the identity links. . . Instead of having . output = conv( conv (x) ) . The residual block is . output = conv( conv (x) ) + x . The result was that he won ImageNet that year (2015). . . Trick: if an NN or GAN doesn&#39;t work so well, try replacing the conv layers with residual blocks! Check out the fantastic paper Visualizing the Loss Landscape. This is 3 years later since ResNet, and people started to realize why it worked. With the skip connections, the loss landscape is much smoother. . The batch norm had the same story. This reminds us innovation usually comes from intuition. Intuition comes first, people realize what’s going on and why it works much later. . . fastai has res_block. We add a res_block after every conv2 layer from previous code, we get . model = nn.Sequential( conv2(1, 8), res_block(8), conv2(8, 16), res_block(16), conv2(16, 32), res_block(32), conv2(32, 16), res_block(16), conv2(16, 10), Flatten() ) . Further refactoring it, . def conv_and_res(ni,nf): return nn.Sequential(conv2(ni, nf), res_block(nf)) model = nn.Sequential( conv_and_res(1, 8), conv_and_res(8, 16), conv_and_res(16, 32), conv_and_res(32, 16), conv2(16, 10), Flatten() ) learn = Learner(data, model, loss_func = nn.CrossEntropyLoss(), metrics=accuracy) learn.lr_find(end_lr=100) learn.recorder.plot() learn.fit_one_cycle(12, max_lr=0.05) print(learn.summary()) . . Tip: when you try out new architectures, keep refactor the code and reuse more to avoid mistakes. Resnet is quite good and can reach SOTA accuracy for a lot of tasks. More modern techniques such as group convolutions don’t train as fast. . DenseNet is another architecture, its only difference from Resnet is that instead of a x + conv(conv(x)), it does concat(x, conv(conv(x))) (the channel gets a little bigger). It is called a DenseBlock instead of ResBlock. The paper of DenseNet seems complicated but it’s really very similar to Resnet. . DenseNet is very memory intensive because it maintains all previous features, BUT it has much fewer parameters. It works really well for small datasets. . U-Net . Use resnet34 and half-stride. What half-stride is really doing is nearest-neighbor interpolation or a bilinear interpolation with stride 1, it up samples the patch and increases the size, as shown below. . . Fantastic paper for convolution: A Guide to Convolution Arithmetic for Deep Learning . Nowadays we use a pretrained resnet34 as the encoder in U-net. . . Trick: if you see two convs in a row, probably should use a resnet block instead. A skip connection with &quot;+&quot; or &quot;concat&quot; usually works great. U-net came before resnet and densenet but it had a lot of the similar ideas and worked great for segmentation tasks. . . Tip: don&#39;t use U-net for classification, because you only need the down-sampling part, not the up-sampling part. Use U-net for generative purposes such as image segmentation because the output resolution is the same as input resolution. Image Restoration with U-Net and GAN . Notebook: superres-gan . We use the U-net architecture to train a super-resolution model. This is a model which can increase the resolution of a low-quality image. Our model won’t only increase resolution—it will also remove jpeg artifacts, and remove unwanted text watermarks. . In order to make our model produce high quality results, we need to create a custom loss function which incorporates feature loss (also known as perceptual loss), along with gram loss. These techniques can be used for many other types of image generation task, such as image colorization. . Traditionally, the GAN is hard to train because the initial generator and critic are bad. Fastai uses pretrained generator and critic, so they are already pretty good. After that the training of GAN is much easier. . . To train a fastai version GAN, we need two folders, one with high-res original images, one with generated images. . . Trick: to free GPU memory without restarting notebook, run the code below. To free GPU memory without restarting notebook, run . my_learner = None gc.collect() . Running nvidia-smi won’t show it freed because pytorch has pre-allocated cache, but it’s available. . # need to wrap the loss with AdaptiveLoss for GAN to work # Will revisit in Part II loss_critic = AdaptiveLoss(nn.BCEWithLogitsLoss()) # Use gan_critic() and not resnet here def create_critic_learner(data, metrics): return Learner(data, gan_critic(), metrics=metrics, loss_func=loss_critic, wd=wd) # GAN version of accuracy: accuracy_thresh_expand learn_critic = create_critic_learner(data_crit, accuracy_thresh_expand) learn_critic.fit_one_cycle(6, 1e-3) learn_critic.save(&#39;critic-pre2&#39;) . . Tip: for GAN, fastai&#39;s GANLearner figures out the back and forth training of the generator and the critic for us. Use the hyperparameters like this below. switcher = partial(AdaptiveGANSwitcher, critic_thresh=0.65) learn = GANLearner.from_learners( learn_gen, learn_crit, weights_gen=(1.,50.), show_img=False, switcher=switcher, opt_func=partial(optim.Adam, betas=(0.,0.99)), wd=wd ) learn.callback_fns.append(partial(GANDiscriminativeLR, mult_lr=5.)) lr = 1e-4 learn.fit(40,lr) # NOTE: the train_loss and gen_loss should stay around the same values # because when the generator and critic both get better, the loss is relative. # The only way to tell how it&#39;s doing is by looking at the image results # Use show_img=True to check . WGAN . Notebook wgan is briefly mentioned for the task of generating image from pure noise without pretraining. Jeremy mentioned it’s a relatively old approach and the task isn’t particularly useful, but it’s good research exercise. . Perceptual Loss (Feature Loss) . Notebook: lesson7-superres . Paper: Perceptual Losses for Real-Time Style Transfer and Super-Resolution . Jeremy didn’t like the name “perceptual loss” and he named it “feature loss” in fastai library. . Convention: in U-net shaped architecture, the down-sampling part is called encoder and the up-sampling part is called decoder. . The paper’s idea is to compare the generated image with the target image using a new loss function, that is, the activation from a middle layer in an ImageNet pretrained VGG network. Use the two images and pass them through this network up to that layer and check the difference. The intuition for this is that, each pixel in that activation should be capturing some feature of ImageNet images, such as furriness, round shaped, has eyeballs, etc. If the two images agree on these features they should have small loss with this loss function. . . With 1 GPU and 1-2hr time, we can generate medium res images from low res images, or high res from medium res using this approach. . A fastai student Jason in 2018 cohort created the famous deOldify project. He crappified color images to black and white, and trained a GAN with feature loss to color 19th century images! . Recap . . Watch the videos again and go through notebooks in detail to understand better. . Recurrent Neural Network . Notebook: lesson7-human-numbers . Toy example dataset with numbers in English, the task is to predict the next word – language model. . xxbos: beginning of stream, meaning start of document. . data.bptt: bptt is backprop thru time . Basic NN with 1 hidden layer: . . One step toward RNN . . Basic RNN: . . Refactor, make it a loop –&gt; RNN . . There is nothing new for an RNN, it is just a fully connected NN with maintained states. . What GRU or LSTM is basically doing is to determine how much of the green arrow to keep and how much of the brown arrow to keep. Will see more in Course Part II. . . Homework . Read papers and write blog posts in plain language. | Build visualizations and apps. Just finish something. | . Answers and Comments from Jeremy . Reinforcement learning is far from being useful for average real world problems. Transfer learning is the most promising. | If you can build the notebooks using fastai from blank, it’s already really rare and very competent in DL. | .",
            "url": "http://blog.logancyang.com/note/fastai/2020/04/30/fastai-lesson7.html",
            "relUrl": "/note/fastai/2020/04/30/fastai-lesson7.html",
            "date": " • Apr 30, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "FastAI Lesson 6: Regression on Tabular Time Series; Regularization; Convolution; Data Ethics",
            "content": "Tip: Use platform.ai to find clusters of unlabeled data to label manually. It uses pretrained models so you can pick a middle layer and pick some specific projections from it. It’s a way of clustering similar images for labeling and it helps you build your better model on top of it. There is a thread for more info. . Regression on Tabular Time Series; Regularization cont. . Look at the rossmann dataset from Kaggle. It uses historical sales data to predict a small period in its future. . Most of times, the approach of embeddings and tabular data is more effective than RNNs for time series forecasting because we have useful metadata like day of week, day of month, locations, etc. RNN is better for pure sequences. So, in most business settings, use tabular approach for time series! . Tip: grab a small part of data, say 2000 rows to explore first. Split to training and test. . The two notebooks for rossmann data are great examples of pandas data preprocessing. There are preprocessors Categortify (-1 for NaN), Fillmissing (replace NaN with median), etc. You can apply them by simply assigning them to a list and pass into databunch creation. . Next, identify categorical and continuous variables. Think carefully. For example, Day may be a number but it’s categorical because in some cases the number of sales for a day is independent of surrounding days. . Don’t forget to make the validation set with the same time range as the test set! . procs=[FillMissing, Categorify, Normalize] cat_vars = [&#39;Store&#39;, &#39;DayOfWeek&#39;, &#39;Year&#39;, &#39;Month&#39;, &#39;Day&#39;, &#39;StateHoliday&#39;, &#39;CompetitionMonthsOpen&#39;, &#39;Promo2Weeks&#39;, &#39;StoreType&#39;, &#39;Assortment&#39;, &#39;PromoInterval&#39;, &#39;CompetitionOpenSinceYear&#39;, &#39;Promo2SinceYear&#39;, &#39;State&#39;, &#39;Week&#39;, &#39;Events&#39;, &#39;Promo_fw&#39;, &#39;Promo_bw&#39;, &#39;StateHoliday_fw&#39;, &#39;StateHoliday_bw&#39;, &#39;SchoolHoliday_fw&#39;, &#39;SchoolHoliday_bw&#39;] cont_vars = [&#39;CompetitionDistance&#39;, &#39;Max_TemperatureC&#39;, &#39;Mean_TemperatureC&#39;, &#39;Min_TemperatureC&#39;, &#39;Max_Humidity&#39;, &#39;Mean_Humidity&#39;, &#39;Min_Humidity&#39;, &#39;Max_Wind_SpeedKm_h&#39;, &#39;Mean_Wind_SpeedKm_h&#39;, &#39;CloudCover&#39;, &#39;trend&#39;, &#39;trend_DE&#39;, &#39;AfterStateHoliday&#39;, &#39;BeforeStateHoliday&#39;, &#39;Promo&#39;, &#39;SchoolHoliday&#39;] dep_var = &#39;Sales&#39; df = train_df[cat_vars + cont_vars + [dep_var,&#39;Date&#39;]].copy() data = (TabularList.from_df(df, path=path, cat_names=cat_vars, cont_names=cont_vars, procs=procs,) .split_by_idx(valid_idx) # NOTE: fastai assumes classification if label_cls is not float, # here we need regression, so make it a FloatList! # log=True takes log of y, use this for percent error because # it turns ratio into difference, RMSPE -&gt; RMSE .label_from_df(cols=dep_var, label_cls=FloatList, log=True) .add_test( TabularList.from_df( test_df, path=path, cat_names=cat_vars, cont_names=cont_vars)) .databunch()) . Tip: for target variable as population, sales etc. where we care more about change rather than absolute differences, we use Root Mean Squared Percent Error (RMSPE) rather than RMSE. Take the log of y with log=True above to make it RMSE. . . Trick: set y_range a bit wider than actual to get better result. In this case 1.2 * ymax. For this problem, the architecture is a fully connected NN. This Kaggle competition is 3 years old but there is no significant better model than it. . We use a weight matrix of 1000 by 500, which is 500K parameters on a few 100K dataset. It is going to overfit. Use regularization to counter overfitting, NOT reducing the parameters manually. . ps=[0.001, 0.01] is DROPOUT. The dropout paper. . Dropout is not dropping weights, but dropping activations! . . Each minibatch we throw away a difference subset of the activations with probability p. A common value is 0.5. . Hinton mentioned where this idea came from. He’s a neural scientist by training so he used dropout to imitate the effect of spiking neurons given that we don’t exact know how neurons spike. . “If you have noisy activations, you can afford to use a much bigger model” – Hinton. . These math ideas almost never come from math but from physical intuitions. . Jeremy advice for research: the original dropout paper is one of the most influential papers in the last decade but was rejected by NIPS. The research community is poor at recognizing important work. Find what interests you and don’t just follow what most people say. . Dropout works really really well! . learn = tabular_learner(data, layers=[1000,500], ps=[0.001,0.01], emb_drop=0.04, y_range=y_range, metrics=exp_rmspe) . ps is a list of dropout probabilities. All fastai learners have this parameter. . In PyTorch, dropout is applied at training time and not test time. You don’t need to do anything at test time. . emb_drop is to randomly drop activations in the embedding. . Reminder: what is embeddings in this case? It’s the vector for each feature, e.g. stores, DayOfWeek, Year, etc. The data matrix started with them as columns and each entry in time as row. Do a matrix factorization and we get these embeddings for the features. . . BatchNorm1d is for continuous variables. . Batch normalization paper. Interestingly, the attempt to explain why it helps NN training (by reducing covariate shifts) was wrong in this paper. Usually researchers have intuitions and did the experiment, it worked, and tried to find math explanations post-hoc. Another paper found it is not why it works. . . With batch norm, the loss landscape is not as bumpy as without batch norm! You can increase your learning rate! . Explanation: the reason it works is activation range shift. people used to think the normalization by mean and std is the important reason why it worked. But it’s actually the two extra parameters. We use activations * g + b to shift the activations from one range to another, just like when we have output predictions in 0-1 but we need to shift to 1-5 for movie ratings. g and b are the biases to directionly shift the range rather than relying on the weights. The weights have nonlinear relationships because of the nonlinear activations in between so they are hard to tune for range shifting (that’s why the loss landscape is bumpy). With 2 more direct controlling parameters, the range shift is much easier. . A much newer method is called Weight Norm and is used by fastai. . . Note: for batch norm, we don&#39;t actually use the exact mean and std for each minibatch, that will be too bumpy. We use the exponentially weighted moving average of mean and std. That&#39;s why there is a momentum=0.1 in batch norm layer, it is not the momentum as in optimization, it is for moving average. . A smaller momentum means less variations from minibatch to minibatch ==&gt; less regularization . . A larger momentum means more variations from minibatch to minibatch ==&gt; more regularization . . In the regularization techniques, we . always want batch norm | weight decay is more preferable than L2 regularization (are they not the same thing? yes and no? ) | weight decay vs. dropout? No definitive answer. Use a bit of both. Try it out. | . The next regularization technique is data augmentation. Jeremy is most excited about this approach. . CNN, Data Augmentation; Regularization cont. . Now use the pets-more notebook. . # check this list of transformations doc(get_transforms) . Tip: for border transform, “reflection” works the best! Better than black borders. And warping works great. . Apply the kind of transform that will match test data. . Research: how to do data augmentation outside CV, e.g. in NLP. . Jeremy goes on explaining convolution and image kernel with visual explanation here, developed by ex-Uber colleague Victor Powell. . . Since 3-by-3 kernels can only reach the second to last rim of the image, we need padding. 0-padding is fine but reflective padding is better. . How CNN works: . We have RGB color image of size w * h * 3, apply k 3 * 3 * 3 kernels to it (without hard definitions such as left right sobel, just randomly init them). Each kernel outputs 1 number, so we end up with a w * h * k output of the image. . | We can have stride 2 by 2 so the w * h gets shrinked by a factor of 2. We also apply 2x more kernels, so the output gets squashed into a longer stick shape with smaller cross section.. . | We repeatedly stack a lot of these conv layers. . | . . . One interesting trick is that we use a 7 by 7 kernel (and more padding) for the first layer to handle the input image, then use 3 by 3 ones in hidden layers. Jeremy will talk about why in Course Part II. . Tip: learn.summary() and learn.model prints out the info and architecture of the NN. . Heatmap for CNN . Next, find the heatmap for CNN’s focus in images. . # Make the kernel for 3 channels k = tensor([ [0. ,-5/3,1], [-5/3,-5/3,1], [1. ,1 ,1], ]).expand(1,3,3,3)/6 . In PyTorch, the shape of the tensor in the case is (# kernels, # channels, height, width). . . Trick: indexing None into a tensor in both pytorch and numpy gives to a new unit axis!! (shown below) t.shape = (3, 352, 352) # Very handy trick t[None].shape # (1, 3, 352, 352) . How the last layers work . Say we have a (w=11, h=11, ch=512) shape layer, that is the last layer of the conv part of the network. The output expects a vector of shape (class=37, 1), for that we take the mean of every slice in the 512 slices, each (w, h) slice only outputs 1 number. This is called AVERAGE POOLING. Now, we have a (512, 1) vector. We then need a weight matrix of (37, 512). This is a linear layer that has input size 512 and output size 37. . W1 -- W2 -- W3 * (x1 | x2 | x3 | ... | x512)_T -- ... -- W37 Each weight vector W has length 512, it dots the (512, 1) vector, produces a weighted sum, activation A1 for class 1. Same for other Ws. We then have an output of (A1 ... A37). The different setting of each W, a shape (1, 512) row vector is the deciding factor that maps the 512 features from average pooling into the final 37 classes. . This is how the classification is done. Each averaged number from a slice indicates a “feature”, it could be “how fluffy it is”, “does it have pointy ears”, etc. The average is equivalent to a sum of the activations in each slice indicating how activated collectively they are in one slice. . Interestingly, if you squash the 512 channels to 1 and average over all the (w, h) slices, the output is one (w, h) matrix. This is the average of all the pixels in the same position in each channel. It then indicates the relevance of “position”, not individual features. With this, we can create a heatmap that shows where it is most relevant to decide whether the cat is a maine coone. . Tip: fastai has a great advanced feature called a “hook”. It allows you to tap into the pytorch code, e.g. a forward pass and manipulate the values in the middle layers. . Tip: if you use hook, don’t forget to remove it after, because it will store the intermediate values everytime you call the model and it is memory intensive. . # To get the conv part of the network, do this m = learn.model.eval() m[0] # Then it shows the conv part of the network # Create a minibatch with just one image in it xb,_ = data.one_item(x) xb_im = Image(data.denorm(xb)[0]) # Pop it into a GPU xb = xb.cuda() # To hook into the output of m[0] from fastai.callbacks.hooks import * def hooked_backward(cat=y): # `with` in Python is called context manager, # at the end of `with` it will remove the hook with hook_output(m[0]) as hook_a: with hook_output(m[0], grad=True) as hook_g: # pytorch allows us to use the model as a function m() preds = m(xb) preds[0,int(cat)].backward() # we don&#39;t care about pred, we care about the hook return hook_a,hook_g hook_a, hook_g = hooked_backward() # fastai hook has .stored for the things you want in the hook acts = hook_a.stored[0].cpu() acts.shape def show_heatmap(hm): _,ax = plt.subplots() xb_im.show(ax) ax.imshow(hm, alpha=0.6, extent=(0,352,352,0), interpolation=&#39;bilinear&#39;, cmap=&#39;magma&#39;); show_heatmap(avg_acts) . Tip: it’s very important to frequently print out the shapes of the tensors, and think about why, think about the pictures in CNN architecture, the learn.summary() and learn.model. . Data Ethics . Check out Rachel’s TED talk here. . . One potential solution: put human in the loop! . Don’t be slaves of algorithm, avoid run-away feedback loops (bad recommendations feed bad behaviors). .",
            "url": "http://blog.logancyang.com/note/fastai/2020/04/22/fastai-lesson6.html",
            "relUrl": "/note/fastai/2020/04/22/fastai-lesson6.html",
            "date": " • Apr 22, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "FastAI Lesson 5: Collaborative Filtering cont.; Backpropagation; Accelerated SGD; NN from Scratch",
            "content": "Discussion on basic NN . PyTorch tip: anything with an underscore after, means in-place, for example a.sub_(val) means a -= val. . SGD: . in code: a.sub_(lr * a.grad) | in formula | . . . . There two types of tensors in NN . parameters | activations | . Input data is one special case in activation. . Activation functions are element-wise functions. Most of the time just use ReLU. . Universal Approximation Theorem: once you stack these tensors together, you can get arbitrarily close approximation of any function. . . Finetuning: what happens when we do transfer learning on resnet-34? . resnet-34 is trained on imagenet where the task was to output probabilities for 1000 classes, so the last layer of weights has 1000 columns. . You probably don’t have 1000 classes, or don’t have the same 1000 classes. So that last layer is of no use to you. . . When you do create_cnn in fastai, it deletes the last layer and puts in two new weight matrices and a ReLU in between, the one to the right has # columns = databunch.c. All previous layers are frozen. — . It’s faster, needs less memory when we freeze early layers. . . After unfreeze, we split our model into a few sections. We think the earlier layers are pretty good, so we give them a smaller learning rate. For example, the first section we give it lr = 1e-5 and the second section we give it lr = 1e-3, etc. This is called Discriminative Learning Rate. — . We don’t want to mess with already good layers with big learning rate. . When you call fit, . # All layers use the same learning rate fit(num_epoch, lr=1e-3) # The last layers (added by create_cnn) use 1e-3, and ALL OTHER LAYERS USE THIS VALUE / 3. # Will discuss the /3 in part II. It&#39;s a quirk of batch normalization fit(num_epoch, lr=slice(1e-3)) # The last layers added by create_cnn will use 1e-3, the first layers will use 1e-5, # the middle layers will have equally spaced lr between the two fit(num_epoch, lr=slice(1e-5, 1e-3)) . We use different lr for different layer groups. . For Collaborative Filtering, there is only one layer, so use just one lr is fine. . . . Terminology: Matrix multiplication is one form of Affine Functions. Convolution is another form. When we talk about affine functions we mean linear function like matrix multiplication. . Collaborative Filtering cont. . Tip: Multiplying with a one-hot encoded matrix (identity matrix) is equivalent to a row lookup. Therefore, never actually do the matrix multiplication, do row lookup instead for better time and space performance. . Terminology: embedding means look something up in an array, which is equivalent to multiplying with an identity matrix, or one-hot encoded matrix. . Once we train a collaborative filtering model, the result user embeddings and movie embeddings contain interesting property of users and movies. . For example, the 1st number in the user embedding could mean whether that user likes a movie with Tom Hanks in it, and the 1st number in the movie embedding is an indicator whether the movie has Tom Hanks. Then, the dot product has a bigger value if both numbers are big, meaning the user will like that movie. . Each dimension in the embedding is a kind of latent factor. . Bias term: there are genuine bad movies, and there are users who generally rate movies low or high. That is when the bias term is useful. It is the value that’s not relevant to all the latent factors in the embedding. . learn = collab_learner(data, n_factors=40, y_range=y_range, wd=1e-1) . . Trick: to get more accuracy on the movie ratings data ranging from 0.5 to 5, set the y_range (which controls the sigmoid asymptotes) to [0, 5.5] n_factors for collab_learner is the # columns, or the # embedding dimensions. They are the “latent factors”. This learner is actually doing matrix factorization. n_factors = 40 is an experiment result that works best. . One good trick to identify best and worst movies regardless of latent factors (they are good or bad on average and not affected by user attributes) is to look at bias. For example, these are some generally bad movies, . [(tensor(-0.3807), &#39;Children of the Corn: The Gathering (1996)&#39;, 1.3157894736842106), (tensor(-0.2987), &#39;Mortal Kombat: Annihilation (1997)&#39;, 1.9534883720930232), (tensor(-0.2976), &#39;Striptease (1996)&#39;, 2.2388059701492535), (tensor(-0.2973), &#39;Lawnmower Man 2: Beyond Cyberspace (1996)&#39;, 1.7142857142857142), (tensor(-0.2699), &#39;Cable Guy, The (1996)&#39;, 2.339622641509434), (tensor(-0.2676), &#39;Free Willy 3: The Rescue (1997)&#39;, 1.7407407407407407), (tensor(-0.2578), &#39;Grease 2 (1982)&#39;, 2.0), (tensor(-0.2526), &#39;Barb Wire (1996)&#39;, 1.9333333333333333), (tensor(-0.2502), &#39;Bio-Dome (1996)&#39;, 1.903225806451613), (tensor(-0.2438), &#39;Island of Dr. Moreau, The (1996)&#39;, 2.1578947368421053), (tensor(-0.2407), &#39;Crow: City of Angels, The (1996)&#39;, 1.9487179487179487), (tensor(-0.2275), &quot;Joe&#39;s Apartment (1996)&quot;, 2.2444444444444445), (tensor(-0.2187), &#39;Leave It to Beaver (1997)&#39;, 1.8409090909090908), (tensor(-0.2173), &#39;Lawnmower Man, The (1992)&#39;, 2.4461538461538463), (tensor(-0.2076), &quot;Stephen King&#39;s The Langoliers (1995)&quot;, 2.413793103448276)] . And these are some generally good movies, . (tensor(0.5738), &quot;Schindler&#39;s List (1993)&quot;, 4.466442953020135), (tensor(0.5580), &#39;Titanic (1997)&#39;, 4.2457142857142856), (tensor(0.5491), &#39;Silence of the Lambs, The (1991)&#39;, 4.28974358974359), (tensor(0.5480), &#39;Shawshank Redemption, The (1994)&#39;, 4.445229681978798), (tensor(0.5439), &#39;Star Wars (1977)&#39;, 4.3584905660377355), (tensor(0.5278), &#39;L.A. Confidential (1997)&#39;, 4.161616161616162), (tensor(0.5112), &#39;As Good As It Gets (1997)&#39;, 4.196428571428571), (tensor(0.5078), &#39;Rear Window (1954)&#39;, 4.3875598086124405), (tensor(0.4927), &#39;Good Will Hunting (1997)&#39;, 4.262626262626263), (tensor(0.4855), &#39;Apt Pupil (1998)&#39;, 4.1), (tensor(0.4810), &#39;Casablanca (1942)&#39;, 4.45679012345679), (tensor(0.4728), &#39;Usual Suspects, The (1995)&#39;, 4.385767790262173), (tensor(0.4705), &#39;Close Shave, A (1995)&#39;, 4.491071428571429), (tensor(0.4539), &#39;Boot, Das (1981)&#39;, 4.203980099502488), (tensor(0.4514), &#39;Vertigo (1958)&#39;, 4.251396648044692) . It’s likely that recommending the ones with biggest positive biases is good for new users we don’t know much about. . To understand the latent factors, 40 is too much to look at. We can do PCA and reduce to 3 dimensions. . . Note: checking activation layers in NN with PCA is often a good idea, because we have way too many activations to have intuition. . Trick: for image similarity, if we run similarity functions over the image activations directly it can be too large. A better idea is to run PCA on image activations and then use the reduced dimensions to run similarity! Weight Decay wd in learner . Weight decay is a way of regularization, it controls the model complexity. It is the coefficient of the sum of squares of parameters, effectively reduce the complexity by making unnecessary parameters smaller. The value of weight decay wd is usually 1e-1 (0.1). This is the result of a lot of experiments on different dataset. . . . . Weight decay wd is a parameter for all learners in fastai, even if you don’t see it in the signature of a particular learner, it is there because it’s in the parent learner class. The default value is 0.01 and not 0.1 because in rare cases, too big a wd caps the model performance, but too small a wd just makes the model easy to overfit and doesn’t cap the performance, the solution is to stop early. . Reason for having weight decay . We don’t want too much complexity, but we DO want many parameters to capture the potential curvy bits of reality. The answer is to have many parameters but penalize their values. . . Note: complexity != # parameters. Because some parameters can be very small and close to 0, even though we have many of them, they are barely there which means not much complexity. Entity Embeddings of Categorical Variables . Paper. Used NN and entity embedding layers for tabular data and got great results. . . MAPE: mean average percentage error. SGD with MNIST . Subclassing is very very common in pytorch. Override the constructor. . class Mnist_Logistic(nn.Module): def __init__(self): super().__init__() self.lin = nn.Linear(784, 10, bias=True) # xb means a minibatch of x def forward(self, xb): return self.lin(xb) . Then create the logistic regression model, . # Create the model and put it on GPU model = Mnist_Logistic().cuda() model # Mnist_Logistic( # (lin): Linear(in_features=784, out_features=10, bias=True) # ) model.lin # Linear(in_features=784, out_features=10, bias=True) model(x).shape # torch.Size([64, 10]) [p.shape for p in model.parameters()] # [torch.Size([10, 784]), torch.Size([10])] # means it takes 784 dim input and output 10 dim, # then there&#39;s the 10d bias . The SGD update with weight decay, . lr=2e-2 loss_func = nn.CrossEntropyLoss() def update(x,y,lr): wd = 1e-5 y_hat = model(x) # weight decay w2 = 0. for p in model.parameters(): w2 += (p**2).sum() # add to regular loss # THIS IS L2 REGULARIZATION! loss = loss_func(y_hat, y) + w2*wd loss.backward() with torch.no_grad(): for p in model.parameters(): p.sub_(lr * p.grad) p.grad.zero_() # Tensor.item() -&gt; a normal python number return loss.item() losses = [update(x,y,lr) for x,y in data.train_dl] . In this case, weight decay is equivalent to L2 regularization because the gradient of the regularization term wd * w^2 gives 2 wd * w. . . Later there&#39;s a case where weight decay != L2 regularization!! Weight decay helps prevent overfitting, so . we can have a giant model without overfitting | or we can have a smaller dataset | . Too much weight decay can also hurt training. . The above is a logistic regression by pytorch from scratch. Here we make an NN with pytorch from scratch. . class Mnist_NN(nn.Module): def __init__(self): super().__init__() self.lin1 = nn.Linear(784, 50, bias=True) self.lin2 = nn.Linear(50, 10, bias=True) def forward(self, xb): x = self.lin1(xb) x = F.relu(x) return self.lin2(x) model = Mnist_NN().cuda() def update(x,y,lr): # Instead of SGD we can use `optim.Adam` opt = optim.Adam(model.parameters(), lr) y_hat = model(x) loss = loss_func(y_hat, y) loss.backward() opt.step() opt.zero_grad() return loss.item() losses = [update(x,y,1e-3) for x,y in data.train_dl] . Optimizer: SGD with Momentum . Apply exponentially weighted moving average on the last several derivatives (steps). The more recent steps are exponentially higher weighted. . . . . Alpha is the momentum, S are the steps, g is the gradient. . To use it in pytorch, just use optim.SGD with a momentum param. Set it to 0.9. . Optimizer: RMSprop . Geoff Hinton first mentioned RMSprop in his Coursera NN course! That is the correct way to cite it. . It’s very similar to SGD with momentum. It uses squared gradient in the denominator. If it’s small the step will be big. . If the gradient is consistently small, it will be a small number. If gradient is volatile or consistently big, it will be a big number. This is because if the gradient is always small we need to take bigger steps. . Optimizer: Adam . Adam is Momentum + RMSprop. It keeps track of the exponentially weighted moving average and the squared gradient. . We still need to set the learning rate, and do learning rate annealing! . In fastai, use learner, and it sets the optimizer (Adam or a slight variation of Adam by default) and you don’t need to worry. . fit_one_cycle helps you get “super convergence”, i.e. train 10x faster than plain SGD. It has smaller lr and large momentum in the beginning, and then lr increases, momentum decreases. When it’s close to optimum, lr decreases again and momentum increases. . . This is research result by Leslie Smith. . Cross-Entropy Loss . If we have a classifier for Cats and Dogs . Cat Dog Pred(Cat) Pred(Dog) X-entropy Comment . 1 | 0 | 0.5 | 0.5 | 0.3 | unsure | . 1 | 0 | 0.98 | 0.02 | 0.01 | confident, right | . 0 | 1 | 0.9 | 0.1 | 1 | confident, wrong | . 0 | 1 | 0.5 | 0.5 | 0.3 | unsure | . 1 | 0 | 0.9 | 0.1 | 0.05 | confident, right | . The unsure cases have moderate loss, confident and right has the lowest loss, confident but wrong has the highest loss. . The cross-entropy formula is . -y log(y_hat) - (1-y) log(1-y_hat) . Keep in mind that now Cat is 1. This basically means, . If Cat (y=1), then look at log of Pred(Cat); . If Dog (y=0), then look at log of 1-Pred(Cat), i.e. Pred(Dog); . MUST make sure the preds to plug into cross-entropy add to one. To make sure of that, use the softmax activation. . For multi-class classification, use cross-entropy as loss and softmax as activation. . . Note: in PyTorch, calling nn.CrossEntropyLoss() actually calculates the softmax behind the scene so there is no need to add a softmax layer manually. If you use a custom loss and want softmax output, make sure to add the softmax layer at the end. . . Note: Regularization include weight decay, batch norm, dropout",
            "url": "http://blog.logancyang.com/note/fastai/2020/04/19/fastai-lesson5.html",
            "relUrl": "/note/fastai/2020/04/19/fastai-lesson5.html",
            "date": " • Apr 19, 2020"
        }
        
    
  
    
        ,"post18": {
            "title": "FastAI Lesson 4: NLP; Tabular Data; Collaborative Filtering and Embeddings",
            "content": "NLP Continued from lesson 3 . 2018 was the year that transfer learning started working well with NLP. You train a language model on a very large dataset to “learn to speak English”, and then do transfer learning for other purposes such as text classification, NER, etc. . Language model on all of Wikipedia (Wikitext 103, ~1B tokens) ==&gt; Fine-tune this language model using your target corpus (in this case, IMDb movie reviews) ==&gt; Extract the encoder from this fine tuned language model, and pair it with a classifier. Then fine-tune this model for the final classification task (in this case, sentiment analysis) . We would think that an LM trained on Wikipedia wouldn’t work well for slangs and informal language, but actually when it gets finetuned with your target corpus, it works. . For the imdb notebook, the sample data has columns label, text, is_valid (in validation set). . For tokenization, most often a token is a word, sometimes it can be &#39;s or a punctuation or symbol. . The full unique set of tokens is vocabulary. Here a limit of 60K tokens and a frequency threshold of 2 are applied to the vocab. . xxunk means unknown token, it means that word was not common enough to be in the vocab. . # Using the data block API is the better way to create the DataBunch data = (TextList.from_csv(path, &#39;texts.csv&#39;, cols=&#39;text&#39;) # how to split out validation set based on bool column .split_from_df(col=2) # which column is the label .label_from_df(cols=0) .databunch()) . The reviews are in a training and test set following an imagenet structure. The only difference is that there is an unsup folder on top of train and test that contains the unlabelled data. . Training an LM on Wiki data takes 2-3 days on a decent GPU, no need to do that, just download the pretrained model. . Even if we have a large target corpus, we still prefer to start from the pretrained model on wikitext-103, there is no reason to start from random. . We are going to use that ‘knowledge’ of the English language to build our classifier, but first, like for computer vision, we need to fine-tune the pretrained model to our particular dataset. Because the English of the reviews left by people on IMDB isn’t the same as the English of wikipedia, we’ll need to adjust the parameters of our model by a little bit. Plus there might be some words that would be extremely common in the reviews dataset but would be barely present in wikipedia, and therefore might not be part of the vocabulary the model was trained on. . This is where the unlabelled data is going to be useful to us, as we can use it to fine-tune our model. Let’s create our data object with the data block API (next line takes a few minutes). . . Trick: make sure to train the LM on all of the data including the test set, because it doesn&#39;t matter, there is no label for LM. #Inputs: all the text files in path data_lm = (TextList.from_folder(path) #We may have other temp folders that contain text files so # we only keep what&#39;s in train and test .filter_by_folder(include=[&#39;train&#39;, &#39;test&#39;, &#39;unsup&#39;]) #We randomly split and keep 10% (10,000 reviews) for validation .split_by_rand_pct(0.1) #We want to do a language model so we label accordingly .label_for_lm() .databunch(bs=bs)) data_lm.save(&#39;data_lm.pkl&#39;) data_lm = load_data(path, &#39;data_lm.pkl&#39;, bs=bs) data_lm.show_batch() # This is an RNN learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3) #Downloading https://s3.amazonaws.com/fast-ai-modelzoo/wt103-fwd . It takes very long to even train one epoch on an average GPU. Mine took ~20min for 1 epoch before unfreeze. It can easily take overnight to train a good model. . For LM on Wikipedia, ~30% accuracy is quite good. For more specific documents like medical or legal, it can be higher. . After training the LM on the target corpus, we save the encoder . learn.save_encoder(&#39;fine_tuned_enc&#39;) . Next, we can build the classifier. . #grab all the text files in path, MUST use the same vocab and order data_clas = (TextList.from_folder(path, vocab=data_lm.vocab) #split by train and valid folder (that only keeps &#39;train&#39; and #&#39;test&#39; so no need to filter) .split_by_folder(valid=&#39;test&#39;) #label them all with their folders .label_from_folder(classes=[&#39;neg&#39;, &#39;pos&#39;]) .databunch(bs=bs)) data_clas.save(&#39;data_clas.pkl&#39;) learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5) learn.load_encoder(&#39;fine_tuned_enc&#39;) learn.lr_find() learn.recorder.plot() # moms is the momentum for the optimizer learn.fit_one_cycle(1, 2e-2, moms=(0.8,0.7)) learn.save(&#39;first&#39;) learn.load(&#39;first&#39;) # NOTE: This only unfreezes the last 2 layers learn.freeze_to(-2) # NOTE: The 2.6**4 stuff is called discriminative learning rate # the range controls the lr for different layers since they learn # best at different rate learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2), moms=(0.8,0.7)) learn.save(&#39;second&#39;) learn.load(&#39;second&#39;) # Unfreeze last 3 layers learn.freeze_to(-3) learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3), moms=(0.8,0.7)) learn.save(&#39;third&#39;) learn.load(&#39;third&#39;) # Unfreeze whole model to finetune learn.unfreeze() learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7)) learn.predict(&quot;I really loved that movie, it was awesome!&quot;) . NOTE: When creating the classifier learner, we MUST have the same vocab as the pretrained language model. The line . data_clas = (TextList.from_folder(path, vocab=data_lm.vocab) ... . is very important. Have the right data_lm in memory and create data_clas with its vocab, or there will be an error when loading the pretrained model via learn.load_encoder(&#39;fine_tuned_enc&#39;) and it will say . Error(s) in loading state_dict for AWD_LSTM: size mismatch... . . Trick: for text classification, unfreezing one layer at a time and train some more is an effective strategy. . Trick: Jeremy created a random forest to find best hyperparameter setting. The best number for discriminative learning rate is 2.6^4. This is similar to AutoML for hyperparam search. For training Chinese language models, search the forum for more info. . Tabular Data using Deep Learning, Embeddings for Categorical Variables . . People were skeptical about using neural nets on tabular data, they often use logistic regression, random forest, gradient boosting machines to do it. In fact, NN is extremely useful for tabular data. . With NN, you don’t need to hand engineer features as much as before. It’s more accurate and requires less maintenance. Jemery used to use Random Forest 99% of the time for tabular data, now he uses NN 90% of the time. . Nobody else created a library for NN on tabular data, fastai has fastai.tabular. In the notebook lesson4-tabular there is a detailed example. . It assumes the data is in a pandas dataframe. Pandas can read from csv, relational db, Spark and Hadoop. . The independent variables (features) can be continuous or categorical. With NN, we use embeddings for categorical variables. . Instead of having “transform”s as in CV such as brightening, flipping, normalization etc., we have “processor”s for tabular data. The difference is that transforms are for data augmentation and are different each time, but processors are run once ahead of time. . dep_var = &#39;salary&#39; cat_names = [ &#39;workclass&#39;, &#39;education&#39;, &#39;marital-status&#39;, &#39;occupation&#39;, &#39;relationship&#39;, &#39;race&#39; ] cont_names = [&#39;age&#39;, &#39;fnlwgt&#39;, &#39;education-num&#39;] # 1. Deal with missing values in some way # 2. Use pandas categorical variables # 3. Normalize continuous variables by mean 0 and std 1 procs = [FillMissing, Categorify, Normalize] data = (TabularList.from_df( df, path=path, cat_names=cat_names, cont_names=cont_names, procs=procs) # when split validation set, must have contiguous indices # think time periods, video frames, or other structure in data .split_by_idx(list(range(800, 1000))) .label_from_df(cols=dep_var) .add_test(test) .databunch() ) data.show_batch(rows=10) learn = tabular_learner(data, layers=[200, 100], metrics=accuracy) learn.fit(1, 1e-2) # Inference row = df.iloc[0] learn.predict(row) . For time series tabular data, you generally don’t use RNN for them. Instead, you can time box them into day_of_week, time_of_day, etc. and it will give you state of the art result. . Collaborative Filtering and Embeddings . When you have data about who-bought-what, who-liked-what, you can have two columns like [userId, productId] in the most basic form. Other metadata can be added, like timestamp, review, etc. . This matrix is very sparse because most users didn’t buy most products / watched most movies. . In this example our data has userId, movieId, rating, timestamp. . ratings = pd.read_csv(path/&#39;ratings.csv&#39;) data = CollabDataBunch.from_df(ratings, seed=42) y_range = [0,5.5] learn = collab_learner(data, n_factors=50, y_range=y_range) . For recommender systems, a big challenge is the Cold Start problem. It means that we particularly care about recommending new movies or recommend relevant movies to new users which we don’t have any data for. The solution is to have a second model on user or movie metadata to quatify the similarities. . Netflix fixed the cold start problem by UX. It asks a new user whether they like the movies they show as a survey. For new movies, they just need to let some hundreds of people watch it and rate them. It wasn’t quite a cold start problem for Netflix. . But for selling products, you might not want people to look at your range of products. You could for example find the metadata of the users such as what geography they are from, their age, gender, and other features to predict whether they would like something. . Collaborative filtering is specifically for when you already have some data about the preferences of the users. . A user has an embedding vector. A movie has an embedding vector. A bias term needs to be added in the user embedding and can be interpretted as the user’s tendency to like movies in general regardless of what movie. Similarly, a bias term in the movie embedding is like the likeability of a movie regardless of users. . The target value is the rating in the range 0 to 5. We dot the user embedding and the movie (item) embedding along with the weights, and pass it through a sigmoid (and times 5) to get a numbder between 0 - 5. Notice that this is actually a “logistic regression” (linear layer on inputs and a sigmoid) but with MSE loss and target variables as numbers between 0 - 5. . . Question: Sigmoid with MSE is interesting, is there a maximum likelihood explanation for this? Note that this mapping from the product of embeddings to the range [0, 5] is still regression and not classification, so the loss used is MSE and not cross entropy. . Why pass through the sigmoid? It makes the model learn easier and let the weights converge to relevant results. It is very common to use sigmoid or softmax as the last layer to produce the output. . . Question: it seems this is a sigmoid(linear model) with MSE, the optimization is nonconvex. How is it done?",
            "url": "http://blog.logancyang.com/note/fastai/2020/04/13/fastai-lesson4.html",
            "relUrl": "/note/fastai/2020/04/13/fastai-lesson4.html",
            "date": " • Apr 13, 2020"
        }
        
    
  
    
        ,"post19": {
            "title": "FastAI Lesson 3: Data blocks; Multi-label classification; Segmentation",
            "content": "Must know: Dataset class in PyTorch . class Dataset(object): &quot;&quot;&quot;An abstract class representing a dataset All other datasets should subclass it and override these methods &quot;&quot;&quot; def __getitem__(self, index): &quot;&quot;&quot;Allow [] indexing&quot;&quot;&quot; raise NotImplementedError def __len__(self): raise NotImplementedError . . Note: Pythonistas call special magic methods __xxx__(): &quot;dunder&quot; xxx. PyTorch has another class called the DataLoader for making minibatches. Then, fastai’s DataBunch uses DataLoaders to create a training DataLoader and a validation DataLoader. fastai has the data block API to customize the creation of DataBunch by isolating the underlying parts of that process in separate blocks, mainly: . Where are the inputs and how to create them? | How to split the data into a training and validation sets? | How to label the inputs? | What transforms to apply? | How to add a test set? | How to wrap in dataloaders and create the DataBunch? | Check the fastai docs for function signatures and types. . To find the corresponding notebooks for the docs, go to the fastai repo . https://github.com/fastai/fastai/tree/master/docs_src/ . For multi-label image classification such as this one, to put this in a DataBunch while using the data block API, we need to use ImageList (and not ImageDataBunch). This will make sure the model created has the proper loss function to deal with the multiple classes. . # This does image data augmentation by flipping them horizontally by default. # Here we enable vertical flipping as well so it rotates every 90 degrees left and right, so 8 possible settings. # warp: fastai has fast perspective warping. For satellite image we don&#39;t need warping tfms = get_transforms(flip_vert=True, max_lighting=0.1, max_zoom=1.05, max_warp=0.) . We often want to call the same function but with different values of a parameter. For example, . # We want to call with different thresh def acc_02(inp, targ): return accuracy_thresh(inp, targ, thresh=0.2) # Equivalent: a CS concept called &quot;partial&quot; or partial function application, # pass in the original function and the param, returns a new wrapper function (py3) acc_02 = partial(accuracy_thresh, thresh=0.2) . This is really common thing to do! . Question: How to use online feedback to retrain model? Answer: Add the labeled new data into the training set, load the old model, unfreeze, use a slightly larger learning rate and more epochs, train (fine-tune) the model some more. . Before unfreeze, we train the model’s last layer. The learning rate should look like this. . . Note that do not set the learning rate at the bottom, set it at the steepest place. . After unfreeze, we finetune the full model. The learning rate should look like this. . . For this shape, we find where it starts to go up, and set it to be 10x smaller than that point as the left of the lr range, and the old learning rate for the frozen model divided by 5 or 10. Will talk about this (called discriminative learning rate) in the future. . In the notebook example for the planet data challenge, Jeremy first trained a model on size 128 by 128 image. This is for faster experimentation and can be used as a pretrained model for the actual 256 by 256 image next. . The process is to create the DataBunch with new size . learn.load(&#39;stage-2-rn50&#39;) # Note, set bs=32 and restart kernel after loading the saved model, # or GPU can run out of memory data = (src.transform(tfms, size=256) .databunch(bs=32).normalize(imagenet_stats)) learn.data = data data.train_ds[0][0].shape # Output: torch.Size([3, 256, 256]) # Freeze means we go back to train the last few layers for transfer learning learn.freeze() learn.lr_find() learn.recorder.plot() # Check the image below for output lr=1e-2/2 learn.fit_one_cycle(5, slice(lr)) learn.save(&#39;stage-1-256-rn50&#39;) learn.unfreeze() learn.fit_one_cycle(5, slice(1e-5, lr/5)) learn.recorder.plot_losses() # Note: save() is used for stages. Data used is also saved learn.save(&#39;stage-2-256-rn50&#39;) # export() returns a pickle file for inference. It saves all # transforms, weights but not data. # Check https://docs.fast.ai/tutorial.inference.html learn.export() . . New Task: Segmentation . Example: . . In segmentation, every pixel needs to be classified. . The training data needs to have images with all pixels labeled. It’s hard to create such datasets, so usually we download them. . Every time we use the datasets, we should find the citation and credit the creators appropriately. . Question: what to do if training loss &gt; validation loss Answer: This means underfitting. Try 1. Train more epochs 2. Smaller learning rate 3. Decrease regularization: weight decay, dropout, data augmentation . The model for segmentation used is U-Net . . Note: what does fit_one_cycle() do? fit_one_cycle helps you get &quot;super convergence&quot;, i.e. train 10x faster than plain SGD. It has smaller lr and large momentum in the beginning, and then lr increases, momentum decreases. When it&#39;s close to optimum, lr decreases again and momentum increases. Paper by Leslie Smith. Towards Data Science article here. You pass in the max learning rate, and it uses a range of learning rates as the picture shows below, it goes up first and down after. The downward part is called annealing which is well known, but the upward part is quite new. The motivation is to avoid the optimization being stuck in a local minimum. The loss surface is usually quite bumpy at some areas and flat in other areas. . . The approach was proposed by Leslie Smith. Read about it more here. . The fastai version of unet is better than the state-of-the-art result published which is a model called hundred-layer-tiramisu! . . Trick: if GPU memory runs out very frequently, use half-precision (16-bit) rather than single-precision (32-bit) float in training. Just add .to_fp16() to any learner. learn = unet_learner( data, models.resnet34, metrics=metrics).to_fp16() . Make sure the GPU driver is update-to-date to use this feature. . Head Pose Estimation: A Regression Task . This is a regression task and the output is a set of (x, y) coordinates. We train a CNN. Instead of using a cross-entropy loss, use MSE. . Preview next lesson: IMDB Review Sentiment, an NLP Task . For texts, we create TextDataBunch from csv. Texts need to be tokenized and numericalized. . When we do text classifications, we actually create 2 models: one is a language model (pretrain, for transfer learning later), the other is a classification model. . The SOTA accuracy for this dataset is ~95% and this notebook achieves that level. . . Note: in deep learning, we don&#39;t care about n-grams, that&#39;s for old time NLP&#39;s feature engineering. Extra: Jeremy mentioned activations and pointed to one great resource . A Visual Proof that NN can approximate any shape, or, universal approximation theorem: . http://neuralnetworksanddeeplearning.com/chap4.html | . (This is an online book, need to check it out!) . What really is deep learning from a math perspective: It&#39;s a series of matrix multiplications with max(x, 0) (ReLU) in between, and we use gradient descent to adjust the weights in these matrices to reduce the final error. The forward pass is something like E = loss_func(W3.dot( max(W2.dot( max( W1.dot(X), 0)), 0), 0)) The only thing is the matrices are large. That&#39;s it. . Usually, the hardest part is to create the DataBunch, the rest is straightforward in fastai. .",
            "url": "http://blog.logancyang.com/note/fastai/2020/04/11/fastai-lesson3.html",
            "relUrl": "/note/fastai/2020/04/11/fastai-lesson3.html",
            "date": " • Apr 11, 2020"
        }
        
    
  
    
        ,"post20": {
            "title": "FastAI Lesson 2: Data Cleaning and Production. SGD from Scratch",
            "content": "Lesson 2 Data Cleaning and Production. SGD from Scratch . The notebook “Lesson 2 Download” has code for downloading images from Google images search result in parallel. It is a great way to create my own image dataset. . . Note: ImageDataBunch can create validation set with valid_pct. ALWAYS SET RANDOM SEED BEFORE CREATING THE VALIDATION SET. We need the same validation set for each run. Other times, randomness is good because we need to know the model runs stably with randomness. . DL models are pretty good at randomly noisy data. But “biased noisy” data is not good. . # Fastai way of opening an image for inspection img = open_image(path/&#39;black&#39;/&#39;00000021.jpg&#39;) . Facilitate experiments with notebook GUI widgets . There is an in-notebook GUI widget called fastai.widgets.ImageCleaner for cleaning the dataset. It has buttons to remove problematic examples and creates a new cleaned.csv from where you can create a new ImageDataBunch with the corrected labels to continue training your model. . These things are called ipywidgets. . Put models in production . Inference on CPU is good enough for the vast majority of use cases. Inference on GPU is a big hassle, you have to deal with batching and queueing, etc. Unless the website has extremely high traffic, inferece on CPU is a much better choice, and it can be horizontally scaled easily. . defaults.device = torch.device(&#39;cpu&#39;) # Open image for inspection img = open_image(path/&#39;black&#39;/&#39;00000021.jpg&#39;) img . The code below loads the trained model and is run at web app starting time once, should run fairly quickly. . # We create our Learner in production enviromnent like this, # just make sure that path contains the file &#39;export.pkl&#39; from before learn = load_learner(path) . Then we can do inference in the web application, . # inference pred_class, pred_idx, outputs = learn.predict(img) pred_class . The endpoint should look like (this example is for Starlette app) . @app.route(&quot;/classify-url&quot;, methods=[&quot;GET&quot;]) async def classify_url(request): bytes = await get_bytes(request.query_params[&quot;url&quot;]) img = open_image(BytesIO(bytes)) _,_,losses = learner.predict(img) return JSONResponse({ &quot;predictions&quot;: sorted( zip(cat_learner.data.classes, map(float, losses)), key=lambda p: p[1], reverse=True ) }) . . Note: **Starlette** is similar to Flask but supports modern async and await for Python 3. **FastAPI** is another option. Common Training Problems . Learning rate | Number of epochs | . If validation loss explodes, learning rate is too high. Decrease the learning rate. . If training loss is higher than validation loss, it means underfitting, either learning rate is too low or number of epochs too low. Try higher learning rate or more epochs. . If the error rate goes down and up again, it’s probably overfitting. But it’s pretty hard to overfit with the settings in fastai here. In this case, if the learning rate is already good and the model is trained a long time but we are still not satisfied with the error rate (note fastai error rate is always validation error), it can mean that we need more data. There is no shortcut to know how much data we need in advance. But sometimes we don’t need that much data we thought we needed. . . Note: Some say if training loss is lower than validation loss, it&#39;s overfitting. IT IS NOT TRUE!! Any model that&#39;s trained correctly always have training loss lower than validation loss. That is right.. . Note: unbalanced classes is often NOT a problem, it just works. Jeremy spent years experimenting and thinks it&#39;s not a problem, no need to do oversampling for less common classes, it just works. Rule of thumb for training flow and setting learning rate . default_lr = 3e-3 learn.fit_one_cycle(4, default_lr) learn.unfreeze() learn.fit_one_cycle(4, slice(steepest_pt_in_plot, default_lr/10)) . In Lesson 2 SGD notebook, we generate some data points for a linear regression. . n=100 # tensor n by 2, all ones x = torch.ones(n,2) # In PyTorch, any function that ends with _ means no return and modify in-place! # With ., int becomes float x[:,0].uniform_(-1.,1) x[:5] a = tensor(3.,2) # Note in Python &#39;@&#39; is matmult. It&#39;s more general tensor product in pytorch y = x@a + torch.rand(n) . The above is 95% we need to know about PyTorch . create a tensor | update a tensor | tensor product | . . Note: The `rank` of a tensor is the number of dimensions, or axes. An RGB image is a rank 3 tensor. A vector is a rank 1 tensor. Tensor is just high dimensional arrays. . Trick: for matplotlib animations, set rc(&#39;animation&#39;, html=&#39;jshtml&#39;) instead of %matplotlib notebook. Vocab . learning rate: step size to multiply gradient by | epoch: one complete run on all data. A batch size of 100 for 1000 datapoints means 1 epoch = 10 iterations. In genral we don’t want to do too many epochs, it lets the model see the same data too many times and overfit | minibatch: random bunch of data points to do weight update | SGD: gradient descent using minibatches | Model architecture: the math function to fit the data with | Parameters: or weights or coefficients. The knobs to tune for a model | Loss function: how far away to the target | . Homework . Make logistic regression optimization animation and blog post | Make a minimal web app for a model using FastAPI | .",
            "url": "http://blog.logancyang.com/note/fastai/2020/04/08/fastai-lesson2.html",
            "relUrl": "/note/fastai/2020/04/08/fastai-lesson2.html",
            "date": " • Apr 8, 2020"
        }
        
    
  
    
        ,"post21": {
            "title": "FastAI Lesson 1: Pets",
            "content": "Lesson 1 . # Shows the module imported from, function signature with types # such as url:str, fname:Union[pathlib.Path, str]=None # Union[...] means it can be any of the type in this list help(func) # python3 path object can be used like this path_img = path/&#39;image&#39; # ImageDataBunch in fastai has a factory method from_name_re() to use regex to get the data and labels # Normalize to square image, size 224 is very commonly used data = ImageDataBunch.from_name_re( path_img, fnames, pat, ds_tfms=get_transforms(), size=224, bs=bs ).normalize(imagenet_stats) . A DataBunch object in fastai contains everything about data. It has training, validation, test data, all the labels. . # normalize the data in the DataBunch object to 0 mean 1 std # in this case, RGB channels with be 0 mean and 1 std data.normalize(imagenet_stats) # Always take a look at the data data.show_batch(rows=3, figsize=(7,6)) print(data.classes) # data.c is the number of classes len(data.classes), data.c . Learner in fastai is a model object, it takes in the DataBunch object and model architecture. It also let you specify the metric to look at for training. . # Note that models.resnet34 is pretrained on imagenet # error_rate is for validation set, because creating a DataBunch object automatically creates the validation set learn = cnn_learner(data, models.resnet34, metrics=error_rate) learn.model # fit_one_cycle() is much better than fit() (paper in 2018), always use it. # 4 is the # epoch to run, it&#39;s a good place to start learn.fit_one_cycle(4) # This can saves the model trained on different DataBunch to different places learn.save(&#39;stage-1&#39;) . To interpret the output of a learner, . # ClassificationInterpretation has a factory method from_learner, # it takes in a learner object and returns a ClassificationInterpretation object interp = ClassificationInterpretation.from_learner(learn) losses,idxs = interp.top_losses() len(data.valid_ds)==len(losses)==len(idxs) # This is the most useful tool, it plots the examples the model got wrong # with biggest losses, and show titles as # `predicted / actual / score for predicted class / score for actual class` interp.plot_top_losses(9, figsize=(15,11)) # Use doc() to check how to use, click `show in doc` to documentation website for more details and source code doc(interp.plot_top_losses) # Another useful tool, confusion matrix interp.plot_confusion_matrix(figsize=(12,12), dpi=60) # Yet another, the most useful, prints the most wrong ones (predicted, actual, # times) interp.most_confused(min_val=2) . Previously we called fit_one_cycle(4), it only trains the last layer!! The whole model is frozen by default! . . Note: transfer learning is always a two-step process, train the last layer for some epochs first, and then unfreeze the whole model for some more training/fine-tuning # After training the last layer, we unfreeze the whole model learn.unfreeze() . When we unfreeze the whole model and retrain, the error rate is actually worse than before. . Now, load the stage 1 model back on again, we need lr_find to find a good learning rate. The default learning rate is something like 0.003. We need to change it. . The learning rate is the most important hyperparameter. . Use learn.recorder.plot() to plot learning rate vs loss graph and set the learning rate range to the steepest downward slope. . Next, unfreeze the model and use that learning rate . learn.unfreeze() # Pass a range of learning rate from 1e-6 to 1e-4 # means first layers use 1e-6 and last layers use 1e-4 # Good rule of thumb, set the right side of slice to be 10x smaller # than default lr, and left to be at least 10x smaller than the lowest # point in the lr vs loss graph learn.fit_one_cycle(2, max_lr=slice(1e-6,1e-4)) . Now it gets better than stage 1! . With these two stages, the model can be already really good. It is at least average Kaggle practitioner level. . Next, we can use a bigger model - resnet50. . If GPU runs out of memory, use smaller batch size. It can be set in the DataBunch creation. . Again, the two-stage workflow . data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(), size=299, bs=bs//2).normalize(imagenet_stats) learn = cnn_learner(data, models.resnet50, metrics=error_rate) # Stage 1 learn.fit_one_cycle(8) learn.save(&#39;stage-1-50&#39;) # Stage 2 learn.unfreeze() learn.lr_find() learn.recorder.plot() # Pick learning rate range around the steepest downward slope learn.fit_one_cycle(3, max_lr=slice(1e-6,1e-4)) # Evaluation interp = ClassificationInterpretation.from_learner(learn) interp.most_confused(min_val=2) . . For an imagenet-style dataset, it has data in folders named with the labels. MNIST for example, has images for 3 in a folder named 3. Use ImageDataBunch.from_folder(...) | For a csv file with columns [filepath, label], use ImageDataBunch.from_csv(...) | For data where labels are in the filename, use regex and ImageDataBunch.from_name_re(...) | For more complex cases, construct any function and pass into ImageDataBunch.from_name_func(...) | What’s really cool is that the documentation of fastai is actually a collection of Jupyter Notebooks with working example code! . Success story: a fastai alumnus created a security anomaly detection software by using the exact code for this lesson on mouse trajectories. . There are examples where problems can be converted into an image problem and apply CNN. Such as audio to spectral form. . Homework 1 . Use my own dataset to train an image classifier. .",
            "url": "http://blog.logancyang.com/note/fastai/2020/04/07/fastai-lesson1.html",
            "relUrl": "/note/fastai/2020/04/07/fastai-lesson1.html",
            "date": " • Apr 7, 2020"
        }
        
    
  
    
        ,"post22": {
            "title": "[Nature of Code] Part I: Creating Physics Engine",
            "content": "Topics in Nature of Code . Part I: Creating Physics Engine . Vectors | Forces | Oscillations | Particle Systems Inheritance | Polymorphism | . | Box2D | Steering Forces | . Part II: Complexity . Flocking | Cellular Automata | Fractals | . Part III: Intelligence . Evolution | Neural Networks | . 1. Introduction: Random Walker, Gaussian, Custom Distributions, Perlin Noise . For a function that yields a standard Gaussian distribution (mean = 0, std = 1), we just add our mean and multiply by our stddev to everything the standard Gaussian yields. . random() gives us uniform distribution. . To get a custom distribution, there are 2 main approaches. . The “bucket” approach | . Draw from [0, 0, 0, 0, 1], we have 80% chance picking 0. . 2-number approach (Rejection Sampling: a Monte Carlo Method) | . let vals, norms; let width, height; let drawLoop = 0; function monteCarlo() { let foundOne = false; let iter = 0; let r1, r2; while (!foundOne &amp;&amp; iter &lt; 10000) { r1 = random(1); r2 = random(1); // target function: y = x^2 target_y = r1 * r1; if (r2 &lt; target_y) { foundOne = true; return r1; } iter++; } // If there&#39;s a problem, not found return 0; } function setup() { width = 600; height = 600; canvas = createCanvas(width, height); canvas.position(10, 10); canvas.style(&quot;outline&quot;, &quot;black 3px solid&quot;); vals = Array(width).fill(0); norms = Array(width).fill(0); } function draw() { background(255); stroke(148,0,211); strokeWeight(4); // Draw a sample between (0, 1) sampleNumber = monteCarlo(); bin = int(sampleNumber * width); vals[bin] += 1 * 10; let normalization = false; maxBinCount = 0; for (let x = 0; x &lt; vals.length; x++) { line(x, height, x, height-norms[x]); if (vals[x] &gt; height) { normalization = true; } if (vals[x] &gt; maxBinCount) maxBinCount = vals[x]; } for (let x = 0; x &lt; vals.length; x++) { if (normalization) norms[x] = vals[x] / maxBinCount * height; else norms[x] = vals[x]; } textSize(24); noStroke(); text(`Monte Carlo Iteration: ${drawLoop}`, 50, 50); drawLoop++; if (drawLoop &gt; 5000) { text(&quot;Done!&quot;, 50, 100); noLoop(); } } . Dan Shiffman called this approach the “2-number approach”. I found on Wikipedia that it is actually one method under the umbrella of Monte Carlo simulations called Rejection Sampling. In that article, there is a visual description that is helpful to understand why this works: . To visualize the motivation behind rejection sampling, imagine graphing the density function of a random variable onto a large rectangular board and throwing darts at it. Assume that the darts are uniformly distributed around the board. Now remove all of the darts that are outside the area under the curve. The remaining darts will be distributed uniformly within the area under the curve, and the x-positions of these darts will be distributed according to the random variable&#39;s density. This is because there is the most room for the darts to land where the curve is highest and thus the probability density is greatest. . This is a method for simulating a custom continuous distribution. . TODO: Make this visualization along side the bar chart distribution in a synchronous way, put it on my website learning-automata.co. . Additional note: in pseudo-random number sampling, there is a method to generate discrete random variables. The method is to use CDF. For example, r.v. X = 0, 1, 2. P(X) = 0.2, 0.7, 0.1 accordingly. Then divide [0, 1) into . Uniformly draw from [0, 1), return 0, 1, 2 depending on which interval it falls into. 0.2 0.9 1 |========|============================|====| return: 0 1 2 . Perlin Noise . Perlin Noise is developed by Prof. Perlin at NYU in the 80s. On a high level, it is a technique to make smoothness and achieve natural looking motions or textures. Will use it more in the future. . 2. Vectors and Forces . In Processing, there is a class PVector. In P5.js, the class is p5.Vector and can be created using . let someVector = createVector(some_x, some_y); . The class has vector math methods such as add(), dot(), cross(), mag(), magSq(), dist(), rotate(), angleBetween(), etc. . The constructor takes 2 or 3 arguments, depending on 2D or 3D. . Static method for PVector in Processing (Java) . PVector f = new PVector(0, 1); float mass = 2; // If we want to calculate acceleration, we need A = f/mass // But we can&#39;t do it directly passing in the f object, because // it will be updated in-place. We need static function in the // PVector class to make a copy of f PVector a = PVector.div(f, mass); . Applying force with draw() . Since it’s sometimes unnecessary to keep track time or # draw loops in a sketch, we can re-apply force every frame. In this case, DO NOT forget to set acceleration to 0 (mult 0) after every frame udpate! . // Inside the Mover class void update() { vel.add(acc); location.add(vel); // Note, reset acc acc.mult(0); } . If we do choose to apply the force with a parameter time, then we don’t have to do it every frame. But that can be a rare use case. . Takeaway, location and velocity are cumulative between frames, but always calculate force fresh every frame! . Simulate friction . friction = - mu * || N || * vel_hat . where ||N|| is the magnitude of the normal force from the surface, and vel_hat the unit velocity vector. . Don’t forget when calculating the friction, copy the velocity vector and do not change it in-place. . 3. Oscillations . Rotation in Processing, . // PI = 180 degrees, this is 45 degrees float angle = PI/4; float aVelocity = 0; float aAcceleration = 0.0001; void setup() { size(800, 200); smooth(); } void draw() { background(255); fill(127); stroke(0); // Note that `rotate` has center at origin (0, 0), // aka top left corner of the canvas, we need translate to // make it centered translate(width/2, height/2); rectMode(CENTER); rotate(); rect(0, 0, 64, 36); angle += aVelocity; aVelocity += aAcceleration; } . Polar coordinate . x = r * cos(a); y = r * sin(a); . Simple harmonic motion . // Say period = 200, it means 200 frames are one period float x = amplitude * sin((frameCount / period) * TWO_PI); // But really we can just use one &quot;angle&quot; variable in sin() float x = amplitude * sin(angle); . Springs . // Bob: Mover class object class Mover { PVector position; PVector velocity; PVector acceleration; void applyForce() {}; } class Spring { float k; float restLength; // If we need moving anchor, we can have it as a Mover object PVector anchor; // This is powerful, Spring directly modifies Mover object void connect(Mover m) { // Calculates displacement, force float force = ...; // and then apply the force to m m.applyForce(force); }; } . It’s good to use physics engine libraries to simulate complex spring systems. . 4. Particle Systems . Java ArrayList: add(), get(), remove(), size() . ArrayList&lt;Particle&gt; particles = new ArrayList&lt;Particle&gt;(); particles.add(new Particle()); particles.get(&lt;index&gt;); particles.remove(&lt;index&gt;); // Enhanced Java loop // Con: can&#39;t modify the arraylist while in the loop for (Particle p: particles) { p.update(); p.display(); } // PROBLEM MODIFYING ARRAYLIST WHILE LOOPING: // If we want to remove from arraylist in the middle // the indices change, e.g. removing c from a, b, c, d, e // we do remove(2), and it // gives a, b, d, e, with d occupying index 2 now. // The loop goes on to i=3 and d is skipped // SOLUTION: // To avoid this when removing in the loop, LOOP BACKWARD for (int i = particles.size(); i &gt;= 0; i--) { if (particle.isDead()) { particles.remove(i); } particles[i].update(); particles[i].display(); } . JavaScript Array: .push(), [i], .splice(i, numToBeRemoved), .length . Note: .splice() can also add items. ref . Organize Particles into one ParticleSystem class . class ParticleSystem { ArrayList&lt;Particle&gt; particles; ParticleSystem() { particles = new ArrayList&lt;Particle&gt;(); } void addParticle(Particle p) { particles.add(p); } void run() { ... } } . A ParticleSystem class is essentially an ArrayList or Particles. Its constructor should be initializing that ArrayList. It can also have a centerPosition where all its particles initialize at. It should be able to addParticle(), display all particles via a loop in run(), and more depending on the use cases. . We can also have a system of ParticleSystems where it is an ArrayList of ParticleSystems. We can assign properties to the system of systems and separate the functionality for each level of system. . Inheritance . Inherit everything from super class | Add data or funtionality | Override functions | super ! Call its parent’s function | // extends is the keyword for inheritance class Kitten extends Mammal { int numWhiskers; void sleep() { println(&quot;purrrr&quot;); super.sleep(); } void meow() {} } . Another example . // extends is the keyword for inheritance class SquareParticle extends Particle { SquareParticle(PVector l) { super(l); } void display() { fill(127); stroke(0); rectMode(CENTER); rect(location.x, location.y, 16, 16); } } . Polymorphism . Polymorphism allows us to use child classes as type parent class, e.g. put child classes into one array of type parent class. . Suppose we have a class Animal, and Dog, Cat extends it. . Animal[] kingdom = new Animal[100]; Animal spot = new Dog(); kingdom[0] = new Dog(); kingdom[1] = new Cat(); ... for (Animal a: kingdom) { // This is powerful! These call their own subclass functions! a.sleep(); a.eat(); } . JavaScript ES6 has the same syntax extends and super as Java! . Note that JS has differences between ES5 class syntax and ES6 ones. . ES5 has . function MyClass(params) { this.params = params; } . while ES6 has . class MyClass { constructor(params) { this.params = params; } } . Inheritance with overriding constructor (must call super in a child constructor before using this, or it won’t work) . class ParentClass { constructor(params) { this.params = params; } } class ChildClass extends ParentClass { constructor(childParams) { super(parentParams); this.params = otherChildParams; } } . When overriding another method: We can use super.method() in a Child method to call Parent method. . Here is a good reference. . To use code for base classes in different folders, put . &lt;script src=&quot;/absolute_path_to_js_file&quot;&gt;&lt;/script&gt; . in the header of html files, NOT import because it is not a module. . TODO: Need to take note here for modules in the future. . Use Image Textures with Particles . Processing uses PImage for images. Preload an image and don’t load it in the constructor. Processing has P2D mode to render things much faster. . There is this blendMode concept, with blendMode(ADD) all the RGB values are added on top of each other which creates a brighter effect. . Ref video .",
            "url": "http://blog.logancyang.com/note/natureofcode/simulation/processing/p5js/2020/03/13/nature-of-code-1.html",
            "relUrl": "/note/natureofcode/simulation/processing/p5js/2020/03/13/nature-of-code-1.html",
            "date": " • Mar 13, 2020"
        }
        
    
  
    
        ,"post23": {
            "title": "Flask Full Stack Part 1: Frontend Quick Walkthrough",
            "content": "This is the note for the Flask course by Jose Portilla on Udemy. . 1. Overview . Flask is a super simple Python web framework and is also scalable with a lot of 3rd party libraries. Choosing Flask rather than NodeJS because Python has nice ecosystem in machine learning. What Flask does in general: . Connect with UI | Connect to database and handle CRUD (create, read, update, delete) | . For example, it can handle HTML forms via library WTForms. SQLite is sufficient as a database for a small website. . Flask is a middle man between the frontend UI and the database backend. . We will use Jinja templates to grab information from Python and Flask to send as HTML. . . 2. HTML quick reference . &lt;!DOCTYPE html&gt; tells it’s an html file. &lt;head&gt; contains metadata, title on the tab and links to javascript, &lt;body&gt; contains content such as forms, styles, headers, etc. . 2.1. Basic tags, list, div, span, attribute . &lt;h1&gt;: heading one . &lt;h6&gt;: heading six . &lt;p&gt;: paragraph . &lt;strong&gt;: bold . &lt;em&gt;: italics . &lt;br&gt;: line break . For full reference, go to Mozilla HTML elements reference. . CodePen and JSFiddle are good online test grounds. . &lt;ol&gt;: ordered list . &lt;li&gt;: list item . &lt;ul&gt;: unordered list . Lists can be nested. . &lt;div&gt; stands for division. . &lt;div&gt; and &lt;span&gt; can separate the HTML page into sections. . &lt;div&gt; is for larger division/block of elements, &lt;span&gt; is for substring such as . &lt;p&gt;Here is &lt;span class=&#39;myclass&#39;&gt;some text&lt;/span&gt;. woohoo! &lt;/p&gt; . for doing styling on myclass. . 2.1.1. HTML Attributes . &lt;img src=&quot;&lt;link to image&gt;&quot; alt=&quot;Uh oh! No image&quot;&gt; . &lt;link to image&gt; can be an url online or a path to local file. Next sections will show how to organize static files in Flask. . &lt;a href=&quot;&lt;some url&gt;&quot;&gt;My link here&lt;/a&gt; . Again, &lt;some url&gt; can be an URL online or a path to another html file locally. . Note that &lt;img&gt; is a self closing tag but &lt;a&gt; is not. . 2.2. HTML Forms . Consist of &lt;form&gt; and &lt;input&gt; tags. . Example 1 (from code example Forms Basics) . &lt;form&gt; &lt;h1&gt;Log In&lt;/h1&gt; &lt;h2&gt;Please Input your Password and Email&lt;/h2&gt; &lt;input type=&quot;email&quot; name=&quot;useremail&quot; value=&quot;Email Here&quot;&gt; &lt;input type=&quot;password&quot; name=&quot;password&quot; value=&quot;Password&quot;&gt; &lt;input type=&quot;submit&quot; name=&quot;&quot; value=&quot;Enter&quot;&gt; &lt;h1&gt;Choose a Color!&lt;/h1&gt; &lt;h2&gt;Click on the Button when ready&lt;/h2&gt; &lt;input type=&quot;color&quot; &gt; &lt;h2&gt;Enter some Text!&lt;/h2&gt; &lt;input type=&quot;text&quot; name=&quot;&quot; value=&quot;Text goes here&quot;&gt; &lt;/form&gt; . . The email input type will let the browser check if it’s a valid email with @. value is prefilled. . The password type hides the input in the box. value is what’s prefilled and hidden. . The submit type is a button where value has the text shown on the button. . The color type is interesting but not commonly used, it lets you select from a color palette. . GET will send back the info to our action URL. . POST submits data to be processed. . Forms must set label for each text box in order to let the user see which field is which in the UI. The for in &lt;label&gt; must match the id in &lt;input&gt; to label the input properly. . Example: (from example Form Labels) . &lt;!-- Upon submitting the form will perform the action (a redirect) --&gt; &lt;form action=&quot;http://www.google.com&quot; method=&quot;get&quot;&gt; &lt;label for=&quot;email&quot;&gt;EMAIL:&lt;/label&gt; &lt;input type=&quot;email&quot; id=&quot;email&quot; name=&quot;useremail&quot; value=&quot;Email Here&quot;&gt; &lt;label for=&quot;pass&quot;&gt;PASSWORD:&lt;/label&gt; &lt;input type=&quot;password&quot; id=&quot;pass&quot; name=&quot;password&quot; placeholder=&quot;Password&quot;&gt; &lt;!-- Validation --&gt; &lt;!-- Usually do a lot more validation with Backend--&gt; &lt;!-- Use the attribute: required to require input--&gt; &lt;label for=&quot;userinput&quot;&gt;TEXT:&lt;/label&gt; &lt;input type=&quot;text&quot; id=&quot;userinput&quot; name=&quot;input&quot; placeholder=&quot;Enter Text Here&quot; required&gt; &lt;input type=&quot;submit&quot; &gt; &lt;/form&gt; . . action is the action that gets triggered upon form submission. Making it an URL is a redirect. . value in the text input type tag is a pre-populated string that is shown in the text input box before typing. It is also the value that actually gets submitted for the field. . placeholder is a hint to the user when the field is empty and it is greyed out. . For type “password”, value is prefilled and hidden, placeholder is a hint without actual filled value and is not hidden. . 2.2.1. Form Selections . When two input radio buttons share the same name, only one can be selected. . Example: (from example form seletions) . &lt;form method=&quot;get&quot;&gt; &lt;h3&gt;Do you already own a dog?&lt;/h3&gt; &lt;label for=&quot;yes&quot;&gt;Yes&lt;/label&gt; &lt;input type=&quot;radio&quot; id=&quot;yes&quot; name=&quot;dog_choice&quot; value=&quot;yes&quot;&gt; &lt;label for=&quot;no&quot;&gt;No:&lt;/label&gt; &lt;input type=&quot;radio&quot; id=&quot;no&quot; name= &quot;dog_choice&quot; value=&quot;no&quot;&gt; &lt;p&gt;How clean is your house (Rated 1-3 with 3 being cleanest))&lt;/p&gt; &lt;select name=&quot;stars&quot;&gt; &lt;option value=&quot;Great&quot;&gt;3&lt;/option&gt; &lt;option value=&quot;Okay&quot;&gt;2&lt;/option&gt; &lt;option value=&quot;Bad&quot;&gt;1&lt;/option&gt; &lt;/select&gt; &lt;p&gt;Any other comments?&lt;/p&gt; &lt;textarea name=&quot;name&quot; rows=&quot;8&quot; cols=&quot;80&quot;&gt;&lt;/textarea&gt; &lt;input type=&quot;submit&quot; name=&quot;&quot; value=&quot;Submit Feedback&quot;&gt; &lt;/form&gt; . . &lt;select&gt; gives a dropdown selection of &lt;option&gt;s. Each option has a value. The value of the option selected will be assigned to name (variable name) of the &lt;select&gt; and the backend can see name = value for this dropdown. . &lt;textarea&gt; is a big text box which can be set with # rows and columns. . Note that &lt;submit&gt;’s value is just the string shown on the submit button. . Once hit submit, the URL will be updated and a part in the format ?name=value&amp;name=value&amp;name=value will be appended. . 3. CSS Crash Course . CSS = Cascading Style Sheet . CSS controls the color, background, borders and much more. . Create a .css file | Use CSS syntax to link element tags | Add style name-value pairs | Connect CSS to HTML | 3.1. Colors . Example: (from Part1_master/css) . /*Colors can be names, or codes*/ h1{ color: blue; } li { color: rgb(0,200,0); } /*Search Google for hex color, it has a hex color picker*/ p{ color: #000000; } /*a is alpha, controls transparency, 1 is fully opaque, 0 fully transparent*/ h4{ color: rgba(13,90,140,0.5) } . The general format is shown below, don’t forget the ; . Selected Tag { property: value; } . To link the CSS file to HTML, add the following in the &lt;head&gt; section to the HTML. . &lt;link rel=&quot;stylesheet&quot; href=&quot;Part1_master.css&quot;&gt; . rel is the relationship attribute of the link, it says the CSS is a stylesheet of the HTML. . href points to the path of the CSS file. . The final result is shown below. . . 3.2. Backgrounds and Borders . Example: (from Part2_master.css) . . . background can be an url to an image, set no-repeat to avoid tiling. background-repeat can be repeat-x or repeat-y for x and y axis only. . For border, border-style and border-width are required attributes. Use one line to avoid 3 . border: orange 10px dashed; . Final result: . . 3.3. class and id: CSS Selector . This is the most important one for CSS. We can select by id or class. . Every HTML element can accept a class or id attribute. CSS can link to them by . . for class | # for id | . classs are for styling multiple different elements. . ids are for a single and unique element. . Example: from CSS Part3 . . . Later, Bootstap will define classes for us. . To summarize, CSS can style the HTML based on tags, classes and ids. . 3.4. Inspect Elements in Browser . In Chrome we can inspect the HTML and CSS in the developer tool. We can even edit it locally to see changes. For example, open Google and change it’s styling locally. . To go back to the original site, just hit refresh. . 3.5. Fonts . Not every font is available on each OS. Mac, Windows and Linux have different fonts. . Use Google Fonts API to change fonts. . Add a link to Google fonts API in the HTML | Add the name for the font-family from Google Fonts API. | You can get the link and name from the Google Fonts webpage. . Example: look at CSS Part5 files . 4. Bootstrap 4 . Bootstrap is a CSS framework originally developed at Twitter for internal use. It was open sourced in 2011 and became one of the most starred projects on Github. . What is Bootstrap? . Conceptually, Bootstrap is a really large CSS file + a really large JS file. . Check out the documentation and templates . This is a template for a dashboard. . . Key concepts: bootstrap components and classes . Linking Bootstrap | Containers | Jumbotrons | Buttons | . 4.1. Buttons . In the &lt;head&gt; section in the HTML, . Copy and paste the CSS link | . &lt;link rel=&quot;stylesheet&quot; href=&quot;https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css&quot; integrity=&quot;sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T&quot; crossorigin=&quot;anonymous&quot;&gt; . Copy and paste the jQuery link | . &lt;script src=&quot;https://code.jquery.com/jquery-3.3.1.slim.min.js&quot; integrity=&quot;sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt; &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js&quot; integrity=&quot;sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt; &lt;script src=&quot;https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js&quot; integrity=&quot;sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt; . Add containers next. The containers are responsive and can self-adjust the position based on browser size on different devices. . Then add a button. Go to Bootstrap -&gt; components -&gt; buttons, copy and paste the code there, mainly need the class names e.g. btn btn-primary. . &lt;button class=&quot;btn btn-success btn-lg active&quot; type=&quot;button&quot; name=&quot;button&quot;&gt;Button&lt;/button&gt; . Same for other components. Be comfortable searching the Component section and copy paste around. . . 4.1.1. Class jumbotron . A showcase message for the website. . Example: . &lt;!-- JumboTron --&gt; &lt;div class=&quot;container&quot;&gt; &lt;div class=&quot;jumbotron&quot;&gt; &lt;!-- &lt;div class=&quot;container&quot;&gt; --&gt; &lt;h1 class=&quot;display-3&quot;&gt;Hello, world!&lt;/h1&gt; &lt;p class=&quot;lead&quot;&gt;This is a simple hero unit, a simple jumbotron-style component for calling extra attention to featured content or information.&lt;/p&gt; &lt;hr class=&quot;my-2&quot;&gt; &lt;p&gt;It uses utility classes for typography and spacing to space content out within the larger container.&lt;/p&gt; &lt;p class=&quot;lead&quot;&gt; &lt;a class=&quot;btn btn-primary btn-lg &quot; href=&quot;#&quot; role=&quot;button&quot;&gt;Learn more&lt;/a&gt; &lt;/p&gt; &lt;!-- &lt;/div&gt; --&gt; &lt;/div&gt; &lt;/div&gt; . . 4.2. Forms . In &lt;head&gt; include . &lt;!-- Bootstrap CSS, JS, and jQuery --&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;https://stackpath.bootstrapcdn.com/bootstrap/4.1.1/css/bootstrap.min.css&quot; integrity=&quot;sha384-WskhaSGFgHYWDcbwN70/dfYBj47jz9qbsMId/iRN3ewGhXQFZCSftd1LZCfmhktB&quot; crossorigin=&quot;anonymous&quot;&gt; &lt;script src=&quot;https://code.jquery.com/jquery-3.3.1.slim.min.js&quot; integrity=&quot;sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt; &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js&quot; integrity=&quot;sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt; &lt;script src=&quot;https://stackpath.bootstrapcdn.com/bootstrap/4.1.1/js/bootstrap.min.js&quot; integrity=&quot;sha384-smHYKdLADwkXOn1EmN1qk/HfnUcbVRZyYmZ4qpPea6sjB/pTJ0euyQp0Mk8ck+5T&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt; . Bootstrap 4 makes forms look really well. . . All of these form components such as email submission, password, dropdown select, multiple select, text area, file upload, radio button, check button should use proper div, select and input with bootstrap class names. Refer to Part2_Forms.html for actual HTML code for each component. . 4.3. Navbar . HTML tag &lt;nav&gt; creates a navigation bar. Bootstrap classes can be added to it to add styling and functionality. It even makes it a dropdown menu on small screens of mobile devices. . Refer to Part3_Navbar.html for code examples. One thing to note is that we need jQuery uncompressed or minified, not slim or slim minified as in Bootstrap. Go to the jQuery website and get the link for minified . &lt;script src=&quot;https://code.jquery.com/jquery-3.4.1.min.js&quot; integrity=&quot;sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt; . The reason is that the slim version of jQuery from Bootstrap sometimes has problems for the collapse functionality for certain browsers. . Check the documentation for further customizations as needed. . The End . This is a quick crash course for HTML, CSS, and Bootstrap. . For further frontend knowledge, check out the courses below . The Complete Web Developer in 2019: Zero to Mastery by Andrei Neagoie | React - The Complete Guide (incl Hooks, React Router, Redux) by Maximilian Schwarzmüller | .",
            "url": "http://blog.logancyang.com/note/fullstack/flask/2019/07/08/flask-frontend.html",
            "relUrl": "/note/fullstack/flask/2019/07/08/flask-frontend.html",
            "date": " • Jul 8, 2019"
        }
        
    
  
    
        ,"post24": {
            "title": "Flask Full Stack Part 3: Flask and SQL Database Walkthrough",
            "content": "This is the note for the Flask course by Jose Portilla on Udemy. . Python and Flask can connect to a variety of SQL database engines, including PostgreSQL, MySQL, SQLite, and a lot more. . SQLite is a simple SQL database engine that comes with Flask, usually it can handle all the needs for a small application with daily hits around 100K. It can also handle more, it’s not a hard cap. . To connect Python code to SQL code, use ORM (Object Relational Mapper). The most common is SQLAlchemy. Flask-SQLAlchemy is an extension to connect Flask to SQLAlchemy. . pip install Flask-SQLAlchemy . Getting Started . We need the following steps . Set up SQLite database in a Flask app | Create a model in the Flask app | Perform basic CRUD on our model | . Here we will show the script for manual CRUD to help understand the concepts. In practice, Flask automates all these. . Use CLI tool to make database. Refer to BasicModelApp.py to check the model definition, and SetUpDatabase.py to see db creation script (usually db creation is through CLI). . A summary of model class definition: . define __tablename__ | define column variables as class variables such as id, name, age, etc. | define __init__(self, ...) method. Note that id is automatically set and it does not need to be in __init__() | define __repr__(self) method | (optional) if there is a relationship to another model/table, use db.relationship(&#39;&lt;another_model&gt;&#39;, backref=&#39;&lt;this_model&gt;&#39;, lazy=&#39;dynamic&#39;) | . To add a row to the db, . db.session.add(&lt;row_obj&gt;) db.session.commit() . For SQLite and SQLAlchemy, integer id gets added automatically and starts at 1. The .sqlite file is the database, it’s encoded and is not readable by a text editor. . CRUD example: . from BasicModelApp import db,Puppy ########################### ###### CREATE ############ ######################### my_puppy = Puppy(&#39;Rufus&#39;,5) db.session.add(my_puppy) db.session.commit() ########################### ###### READ ############## ######################### # Note lots of ORM filter options here. # filter(), filter_by(), limit(), order_by(), group_by() # Also lots of executor options # all(), first(), get(), count(), paginate() all_puppies = Puppy.query.all() # list of all puppies in table print(all_puppies) print(&#39; n&#39;) # Grab by id puppy_one = Puppy.query.get(1) print(puppy_one) print(puppy_one.age) print(&#39; n&#39;) # Filters puppy_sam = Puppy.query.filter_by(name=&#39;Sammy&#39;) # Returns list print(puppy_sam) print(&#39; n&#39;) ########################### ###### UPDATE ############ ######################### # Grab your data, then modify it, then save the changes. first_puppy = Puppy.query.get(1) first_puppy.age = 10 db.session.add(first_puppy) db.session.commit() ########################### ###### DELETE ############ ######################### second_pup = Puppy.query.get(2) db.session.delete(second_pup) db.session.commit() # Check for changes: all_puppies = Puppy.query.all() # list of all puppies in table print(all_puppies) . Flask Migrate: Database Migration . When there are new columns to be added, only updating the model.py file won’t update the database. We need database migration. . To install, . pip install Flask-Migrate . To import, . from flask_migrate import Migrate . To use Flask Migrate, the first thing is to set the FLASK_APP variable. On Linux/Mac it goes, . export FLASK_APP=&lt;myapp.py&gt; . then import Migrate from flask_migrate, and add the line below before the class definition code. . # Add on migration capabilities in order to run terminal commands Migrate(app,db) . Migration steps: . # Set up the migration directory flask db init # Add some changes to the model class, e.g. add a column # Set up the migration file. Similar to git commit flask db migrate -m &quot;&lt;some message&gt;&quot; # Update the database with the migration flask db upgrade . Flask Relationships . For multiple models (tables), we need “relationships” to link them together. Some concepts: . Primary Key: unique identifier for a row | Foreigh Key: the column that’s another table’s primary key | . The key is to define db.relationship(...) to point a class variable x in &lt;parent_table&gt; (column) to another table. x is defined as db.Column(db.&lt;var_type&gt;, db.ForeignKey(&#39;&lt;parent_table.column&gt;&#39;)) in the other table. &lt;parent_table&gt; does not reference to other tables, other tables reference to it by having a column of &lt;parent_table&gt;’s primary key as db.ForeignKey. . Code example below . class Puppy(db.Model): __tablename__ = &#39;puppies&#39; id = db.Column(db.Integer,primary_key = True) name = db.Column(db.Text) # This is a one-to-many relationship # A puppy can have many toys toys = db.relationship(&#39;Toy&#39;,backref=&#39;puppy&#39;,lazy=&#39;dynamic&#39;) # This is a one-to-one relationship # A puppy only has one owner, thus uselist is False. # Strong assumption of 1 dog per 1 owner and vice versa. owner = db.relationship(&#39;Owner&#39;,backref=&#39;puppy&#39;,uselist=False) def __init__(self,name): # Note how a puppy only needs to be initalized with a name! self.name = name def __repr__(self): if self.owner: return f&quot;Puppy name is {self.name} and owner is {self.owner.name}&quot; else: return f&quot;Puppy name is {self.name} and has no owner assigned yet.&quot; def report_toys(self): print(&quot;Here are my toys!&quot;) for toy in self.toys: print(toy.item_name) class Toy(db.Model): __tablename__ = &#39;toys&#39; id = db.Column(db.Integer,primary_key = True) item_name = db.Column(db.Text) # Connect the toy to the puppy that owns it. # We use puppies.id because __tablename__=&#39;puppies&#39; puppy_id = db.Column(db.Integer,db.ForeignKey(&#39;puppies.id&#39;)) def __init__(self,item_name,puppy_id): self.item_name = item_name self.puppy_id = puppy_id class Owner(db.Model): __tablename__ = &#39;owners&#39; id = db.Column(db.Integer,primary_key= True) name = db.Column(db.Text) # We use puppies.id because __tablename__=&#39;puppies&#39; puppy_id = db.Column(db.Integer,db.ForeignKey(&#39;puppies.id&#39;)) def __init__(self,name,puppy_id): self.name = name self.puppy_id = puppy_id . Note that one-to-one relationship has uselist=False. Default is uselist=True i.e. one-to-many, if not specified. . lazy=&#39;dynamic&#39; is a more advanced usage. Leave it as it for now. . . Schema: . Table puppies: id, name (parent table) . Table toys: id, item_name, puppy_id (foreign key, refers to puppies.id) . Table owners: id, name, puppy_id (foreign key, refers to puppies.id) . . Now we experiment with adding some records with relationships. . from models import db,Puppy,Owner,Toy # Create 2 puppies rufus = Puppy(&quot;Rufus&quot;) fido = Puppy(&quot;Fido&quot;) # Add puppies to database db.session.add_all([rufus,fido]) db.session.commit() # Check with a query, this prints out all the puppies! print(Puppy.query.all()) # Grab Rufus from database # Grab all puppies with the name &quot;Rufus&quot;, returns a list, so index [0] # Alternative is to use .first() instead of .all()[0] rufus = Puppy.query.filter_by(name=&#39;Rufus&#39;).all()[0] # Create an owner to Rufus # Owner __init__ takes in name and foreign key Puppy id which is unique jose = Owner(&quot;Jose&quot;,rufus.id) # Give some Toys to Rufus # Toy __init__ takes in item name and foreign key Puppy id which is unique toy1 = Toy(&#39;Chew Toy&#39;,rufus.id) toy2 = Toy(&quot;Ball&quot;,rufus.id) # Commit these changes to the database # Note that all_all can take different types of objects at once!! db.session.add_all([jose,toy1,toy2]) db.session.commit() # Let&#39;s now grab rufus again after these additions rufus = Puppy.query.filter_by(name=&#39;Rufus&#39;).first() print(rufus) # Output: # Puppy name is Rufus and owner is Jose # Show toys print(rufus.report_toys()) # Output: # Chew Toy # Ball # You can also delete things from the database: find_pup = Puppy.query.get(1) db.session.delete(find_pup) db.session.commit() # But note that if deleted, do not try to delete again, or it will error . Connect to Flask Template - Databases in Views . Refer to “03-Databases-in-Views” folder for a complete puppy adoption site example from scratch. . A view function is just a function with app.route in Flask, it responds to client requests. . First, create the site.py or app.py, then the necessary files such as forms.py and models.py. Next, create the templates folder and base.html in it, add necessary html files such as home.html, add.html, delete.html, etc. . Then add the model classes for the database same as in the last section. . The next step is to add @app.routes functions to the form view, such as . @app.route(&#39;/add&#39;, methods=[&#39;GET&#39;, &#39;POST&#39;]) def add_pup(): form = AddForm() if form.validate_on_submit(): name = form.name.data new_pup = Puppy(name) db.session.add(new_pup) db.session.commit() return redirect(url_for(&#39;list_pup&#39;)) return render_template(&#39;add.html&#39;, form=form) . and other views such as /list and /delete. Refer to example in the code folder. .",
            "url": "http://blog.logancyang.com/note/fullstack/flask/2019/07/08/flask-db.html",
            "relUrl": "/note/fullstack/flask/2019/07/08/flask-db.html",
            "date": " • Jul 8, 2019"
        }
        
    
  
    
        ,"post25": {
            "title": "How to Create Keyword List Entities in spaCy (v2.1)",
            "content": "Sometimes there is a need to create keyword entities from a list of known keywords, e.g. country names, species, brands, etc. The list can be large. This note shows how to create such entities in spaCy and make it work with a trained NER model. . Rule Based Matcher . PhraseMatcher is useful if you already have a large terminology list or gazetteer consisting of single or multi-token phrases that you want to find exact instances of in your data. As of spaCy v2.1.0, you can also match on the LOWER attribute for fast and case-insensitive matching. . Matcher is about individual tokens. For example, you can find a noun, followed by a verb with the lemma “love” or “like”, followed by an optional determiner and another token that’s at least ten characters long. . PhraseMatcher is what we need. . Say we have several brand names, . [u&quot;Armani&quot;, u&quot;Ralph Lauren&quot;, u&quot;Monique Lhuillier&quot;, u&quot;Norma Kamali&quot;] . Assume we have some text messages in which we find these brand names. We apply the trained NER model on these messages to make predictions. To make this case insensitive, use attr=&quot;LOWER&quot;, . from spacy.lang.en import English from spacy.matcher import PhraseMatcher nlp = English() matcher = PhraseMatcher(nlp.vocab, attr=&quot;LOWER&quot;) patterns = [ nlp.make_doc(name) for name in [u&quot;Armani&quot;, u&quot;Ralph Lauren&quot;, u&quot;Monique Lhuillier&quot;, u&quot;Norma Kamali&quot;] ] matcher.add(&quot;Brands&quot;, None, *patterns) doc = nlp(u&quot;armani and monique Lhuillier are both brands&quot;) for match_id, start, end in matcher(doc): print(&quot;Matched based on lowercase token text:&quot;, doc[start:end]) # output: # Matched based on lowercase token text: armani # Matched based on lowercase token text: monique Lhuillier . It can even match number entities by shape, attr=&quot;SHAPE&quot;, e.g. IP addresses. . Combine with Model Prediction: Use Entity Ruler and Pattern File (v2.1) . PhraseMatcher doesn’t address the need to combine rules with statistical models. The rules must have influence on the prediction process or they will have conflicts. . Citing the spaCy docs, “The entity ruler is designed to integrate with spaCy’s existing statistical models and enhance the named entity recognizer. If it’s added before the &quot;ner&quot; component, the entity recognizer will respect the existing entity spans and adjust its predictions around it. This can significantly improve accuracy in some cases. If it’s added after the &quot;ner&quot; component, the entity ruler will only add spans to the doc.ents if they don’t overlap with existing entities predicted by the model. To overwrite overlapping entities, you can set overwrite_ents=True on initialization.” . from spacy.lang.en import English from spacy.pipeline import EntityRuler # Before training nlp = English() &quot;&quot;&quot;This is the hard-coded ruler ruler = EntityRuler(nlp) patterns = [{&quot;label&quot;: &quot;ORG&quot;, &quot;pattern&quot;: &quot;Apple&quot;}, {&quot;label&quot;: &quot;GPE&quot;, &quot;pattern&quot;: [{&quot;lower&quot;: &quot;san&quot;}, {&quot;lower&quot;: &quot;francisco&quot;}]}] ruler.to_disk(&quot;./patterns.jsonl&quot;) ruler.add_patterns(patterns) &quot;&quot;&quot; # Loading ruler from jsonl file ruler = EntityRuler(nlp).from_disk(&quot;./patterns.jsonl&quot;) nlp.add_pipe(ruler) # Add NER training / transfer learning code here... # At prediction time doc = nlp(u&quot;Apple is opening its first big office in San Francisco.&quot;) print([(ent.text, ent.label_) for ent in doc.ents]) . Question: Since the pattern file is a list of patterns, it must be slow to go through the list every time to check whether something is a brand. What’s the solution? . Case Study: Brand Entity . Brand is an example where the keyword list / pattern file can be really large. There are already many labeled brand entities in the training data so the model may or may not find correct brand entities at prediction time. In the case of an incorrect prediction, how do we leverage the rule-based method to correct it? . Note that we prefer adding the EntityRuler before the &quot;ner&quot; component to let the model respect the keyword list and adjust its predictions. . Case 1: Predicted entity is not in the keyword list and has no word overlap with any item in the list. . In this case, it is either a wrong prediction or a new brand entity correctly predicted but is not in the training data. These cases need to be logged and checked by a human. If confirmed it IS a correct new brand entity, it should be added to the brand keyword list. . Case 2: Predicted entity is not in the keyword list BUT has overlap with one or more items in the list. . If EntityRuler is used, the model prediction should be able to find the complete brand name in the text, so any such overlap should be the case where only part of the brand name is there in the text but no complete name from the brand list is present. This is sometimes OK, people don’t necessarily call out the complete brand name but only refer to it with a short form. In other cases, this is a wrong prediction. Again, human check is preferred. . Case 3: Predicted entity is in the keyword list . This is the trivial case where the model is doing a perfect job. . (For more advanced usages involving dependency parsing, check here for examples. This is beyond the scope of this post about keyword list entities.) .",
            "url": "http://blog.logancyang.com/note/spacy/nlp/2019/06/07/keyword-entity.html",
            "relUrl": "/note/spacy/nlp/2019/06/07/keyword-entity.html",
            "date": " • Jun 7, 2019"
        }
        
    
  
    
        ,"post26": {
            "title": "How to setup local GPU server for fast.ai",
            "content": "Fast.ai is the best coder’s guide to practical deep learning. This is a guide to its environment setup on a Linux server with NVIDIA GPU. . Server setup: GPU driver and CUDA . If you have an Nvidia GPU on your Linux machine, great! Let’s install the necessary components for deep learning. . Preparation . Install some useful packages . sudo apt-get update sudo apt-get install aptitude freeglut3-dev g++-4.8 gcc-4.8 libglu1-mesa-dev libx11-dev libxi-dev libxmu-dev nvidia-modprobe python-dev python-pip python-virtualenv vim . Install CUDA . Download CUDA installation file: https://developer.nvidia.com/cuda-downloads . Choose Linux -&gt; x86_64 -&gt; Ubuntu -&gt; 14.04 -&gt; deb (local) -&gt; Download . Install CUDA in terminal (use the specific .deb file you’ve downloaded): . cd ~/Downloads sudo dpkg -i cuda-repo-ubuntu1404-8-0-local-ga2_8.0.61-1_amd64.deb sudo apt-get update sudo apt-get install cuda . Restart the computer to activate CUDA driver. Now your screen resolution should be automatically changed to highest resolution for the display! . Install cuDNN . The NVIDIA CUDA® Deep Neural Network library (cuDNN) is aGPU-accelerated library of primitives for deep neural networks with optimizations for convolutions etc. . Register an (free) acount on NVIDIA website and login to download the latest cuDNN library: https://developer.nvidia.com/cudnn . Choose the specific version of cuDNN (denpending on support of your prefered deep learning framework) . Choose Download cuDNN v5.1 (Jan 20, 2017), for CUDA 8.0 -&gt; cuDNN v5.1 Library for Linux . Install cuDNN (by copying files :) in terminal: . cd ~/Downloads tar xvf cudnn-8.0-linux-x64-v5.1.tgz cd cuda sudo cp lib64/* /usr/local/cuda/lib64/ sudo cp include/cudnn.h /usr/local/cuda/include/ sudo chmod a+r /usr/local/cuda/lib64/libcudnn* . Update your .bashrc . Add the following lines to your ~/.bashrc file (you can open it by gedit ~/.bashrc in terminal) . export PATH=/usr/local/cuda/bin:$PATH export MANPATH=/usr/local/cuda/man:$MANPATH export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH source ~/.bashrc . To check the installation, print some GPU and driver information by: . nvidia-smi nvcc --version . Note: Haven’t been using my gpu server for a while and when I started Ubuntu in Jan 2020, the NVidia driver stopped working. First of all, the display resolution was messed up and I had to perform actions from my other machine via ssh. I had to update the driver but there were some hoops to jump through. There were some broken dependencies and I needed . sudo aptitude install &lt;name_of_package_with_conflicts&gt; . to be able to run . sudo ubuntu-drivers autoinstall . reference: https://askubuntu.com/questions/1077493/unable-to-install-nvidia-drivers-on-ubuntu-18-04 . Server Setup: Conda . Install Anaconda . cd /tmp curl -O https://repo.anaconda.com/archive/Anaconda3-2019.03-Linux-x86_64.sh sha256sum Anaconda3-2019.03-Linux-x86_64.sh # output # 45c851b7497cc14d5ca060064394569f724b67d9b5f98a926ed49b834a6bb73a Anaconda3-2019.03-Linux-x86_64.sh bash Anaconda3-2019.03-Linux-x86_64.sh . You’ll receive the following output to review the license agreement by pressing ENTER until you reach the end. . # Output Welcome to Anaconda3 2019.03 In order to continue the installation process, please review the license agreement. Please, press ENTER to continue &gt;&gt;&gt; ... Do you approve the license terms? [yes|no] . When you get to the end of the license, type yes as long as you agree to the license to complete installation. . Once you agree to the license, you will be prompted to choose the location of the installation. You can press ENTER to accept the default location, or specify a different location. . Once installation is complete and ran conda init, the following code is added to .bashrc. If you use zsh, add it to .zshrc. . # &gt;&gt;&gt; conda initialize &gt;&gt;&gt; # !! Contents within this block are managed by &#39;conda init&#39; !! __conda_setup=&quot;$(&#39;/home/loganyc/anaconda3/bin/conda&#39; &#39;shell.bash&#39; &#39;hook&#39; 2&gt; /dev/null)&quot; if [ $? -eq 0 ]; then eval &quot;$__conda_setup&quot; else if [ -f &quot;/home/loganyc/anaconda3/etc/profile.d/conda.sh&quot; ]; then . &quot;/home/loganyc/anaconda3/etc/profile.d/conda.sh&quot; else export PATH=&quot;/home/loganyc/anaconda3/bin:$PATH&quot; fi fi unset __conda_setup # &lt;&lt;&lt; conda initialize &lt;&lt;&lt; . Use the conda command to test the installation and activation: . conda list . To setup conda environment, . conda create --name my_env python=3 conda activate my_env . To deactivate conda env . conda deactivate . (Note if you use zsh, there can be a (base) shown as the default conda environment.) . Server Setup: fast.ai . git clone https://github.com/fastai/course-v3 conda update conda conda install -c pytorch -c fastai fastai pytorch torchvision cuda92 jupyter cd course-v3/nbs/dl1 jupyter notebook . In your terminal, and you can access the notebook at localhost:8888. . If going to localhost:8888 doesn’t work, or asks for a password/token return to your terminal window and look for this message after you typed ‘jupyter notebook’: “Copy/paste this URL into your browser when you connect for the first time, to login with a token:” . Copy and paste that URL into your browser, and this should connect you to your jupyter notebook. . Go back to the first page to see how to use this jupyter notebook and run the jupyter notebook tutorial. Come back here once you’re finished and don’t forget to stop your instance with the next step. . If you have any problem while using the fastai library try running . conda install -c fastai fastai . Note that in Ubuntu terminal, use ctrl+ to stop the notebook server. . GPU vs. CPU for Deep Learning Test . Try running this inside a Jupyter Notebook: . Cell [1]: . import torch t_cpu = torch.rand(500,500,500) %timeit t_cpu @ t_cpu # Output # 785 ms ± 14.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) . Cell [2]: . t_gpu = torch.rand(500,500,500).cuda() %timeit t_gpu @ t_gpu # Output # 18.7 ms ± 376 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) . Set up SSH connection . Server side, install SSH server: . sudo apt-get install openssh-server . Edit SSH configuration to whitelist users: . sudo vim /etc/ssh/sshd_config . Change root login permission line to: PermitRootLogin no . Add allow users: AllowUsers &lt;your_username&gt; . Then restart SSH server: . sudo /etc/init.d/ssh restart sudo ufw allow 22 . Client side, to connect with the workstation, you need to firstly know the server’s IP (or hostname if it has one). Use ifconfig -a on the server to check IP address (look for that in eth0). . Client side (Mac OS), you need to whitelist the server IP in /etc/hosts: . sudo vim /etc/hosts . Add line &lt;server IP&gt; &lt;server hostname&gt; . Port Forwarding to Acess Jupyter Notebook (LAN) . Within the home network, access the Jupyter Notebook on the Linux GPU server from a client machine’s browser by running . ssh -fNL 8888:local:8888 &lt;username_on_server&gt;@&lt;server_ip&gt; . and go to localhost:8888/tree in your browser. . Setup Remote SSH Access (WAN/Internet) . In my case, my Ubuntu machine with GPU sits at home behind a Verizon Fios router. I can directly ssh into it in my home network (LAN) but doing so from outside requires several additional steps. . Configure Router for Port Forwarding . For Verison Fios routers, go to 192.168.1.1 to access the router setting. In my case, the username is admin and the password is printed on my router. . In the page, find port forwarding, set source port to be ANY, destination port to be your custom port, say 2222, and the port forward to is 22 which is the port on the box at which ssh is listening. Then click add. Done! . SSH from Remote . Now, to log onto the box from outside, run . ssh &lt;username&gt;@&lt;your_router_ip&gt; -p 2222 . To simplify this, add a blob in ~/.ssh/config. . Host deeplearningbox HostName &lt;router_public_ip&gt; User &lt;username_on_deeplearningbox&gt; . Now you can run ssh deeplearningbox -p 2222 . Setup Jupyter Notebook Server for Remote Access . With remote ssh setup, now we setup Jupyter Notebook. . jupyter notebook --generate-config . which will generate jupyter_notebook_config.py in ~/.jupyter. . Then generate certfile and key as: . $ cd ~/.jupyter $ openssl req -x509 -nodes -days 365 -newkey rsa:1024 -keyout mykey.key -out mycert.pem . Launch Python and run the following lines: . In [1]: from notebook.auth import passwd In [2]: passwd() Enter password: Verify password: Out[2]: &#39;sha1:67c9e60bb8b6:9ffede0825894254b2e042ea597d771089e11aed&#39; . After that, edit the jupyter_notebook_config.py as following: . c.NotebookApp.password = u&#39;sha1:67c9e60bb8b6:9ffede0825894254b2e042ea597d771089e11aed&#39; # Set options for certfile, ip, password, and toggle off browser auto-opening c.NotebookApp.certfile = u&#39;/absolute/path/to/your/certificate/mycert.pem&#39; c.NotebookApp.keyfile = u&#39;/absolute/path/to/your/certificate/mykey.key&#39; c.NotebookApp.ip = &#39;0.0.0.0&#39; c.NotebookApp.open_browser = False # It is a good idea to set a known, fixed port for server access c.NotebookApp.port = 8888 . Now, start jupyter notebook on the deep learning workstation and log in by . ssh -fNL 8888:localhost:8888 deeplearningbox -p 2222 . Note that visit https://localhost:8888 instead of http on the client computer. . Side Note: Github HTTPS . Since I have 2FA authentication for my github account, using https as remote url for my repo needs “personal access token” which serves as password when pushing. Refer to this for setup. . Using SSH instead of HTTPS is another option. But when the computer has two or more github account, setting up more key pairs and making things work is just too much. Since I prefer using my MBP (the machine with multiple Github accounts) over the Linux box for coding, sticking with HTTPS for the mac. . Side Note: SSH Port Forwarding Concepts . I’m a visual person. I found the answer here helpful for explaining different types of port forwarding. . References . https://course.fast.ai/start_aws.html | https://github.com/charlesq34/DIY-Deep-Learning-Workstation | https://unix.stackexchange.com/questions/115897/whats-ssh-port-forwarding-and-whats-the-difference-between-ssh-local-and-remot | https://superuser.com/questions/869417/port-forwarding-not-working-on-verizon-fios-router | https://fzhu.work/blog/python/remote-ipython-notebook-setup.html | https://tuatini.me/part-2-how-to-setup-your-own-environment-for-deep-learning-for-remote/ | https://help.github.com/en/articles/creating-a-personal-access-token-for-the-command-line | .",
            "url": "http://blog.logancyang.com/note/fastai/2019/05/27/fastai-gpu-setup.html",
            "relUrl": "/note/fastai/2019/05/27/fastai-gpu-setup.html",
            "date": " • May 27, 2019"
        }
        
    
  
    
        ,"post27": {
            "title": "[CS231n] Overview of Neural Networks",
            "content": "What is a neural network mathematically . Linear model . pred=Wx text{pred} = W xpred=Wx . A 2-layer NN . pred=W2∗max⁡(0,W1x) text{pred} = W_2 * max(0, W_1 x)pred=W2​∗max(0,W1​x) . A 3-layer NN . pred=W3∗max⁡(0,W2∗max⁡(0,W1x)) text{pred} = W_3 * max(0, W_2 * max(0, W_1 x))pred=W3​∗max(0,W2​∗max(0,W1​x)) . Note that when we count layers, we exclude the input layer and include the output layer. So a logistic regression is a 1-layer neural net (only the output layer is counted). . Single neuron as a linear classifier (Logistic Regression) . As we saw with linear classifiers, a neuron has the capacity to “like” (activation near one) or “dislike” (activation near zero) certain linear regions of its input space. Hence, with an appropriate loss function on the neuron’s output, we can turn a single neuron into a linear classifier: . Binary Softmax classifier (Logistic Regression). For example, we can interpret . σ(∑iwixi+b) sigma( sum_iw_ix_i + b)σ(i∑​wi​xi​+b) . to be the probability of one of the classes . P(yi=1∣xi;w)P(y_i = 1 mid x_i; w)P(yi​=1∣xi​;w) . The probability of the other class would be . P(yi=0∣xi;w)=1−P(yi=1∣xi;w)P(y_i = 0 mid x_i; w) = 1 - P(y_i = 1 mid x_i; w)P(yi​=0∣xi​;w)=1−P(yi​=1∣xi​;w) . since they must sum to one. . With this interpretation, we can formulate the cross-entropy loss as we have seen in the Linear Classification section, and optimizing it would lead to a binary Softmax classifier (also known as logistic regression). Since the sigmoid function is restricted to be between 0-1, the predictions of this classifier are based on whether the output of the neuron is greater than 0.5. . Binary SVM classifier. Alternatively, we could attach a max-margin hinge loss to the output of the neuron and train it to become a binary Support Vector Machine. . Regularization interpretation. The regularization loss in both SVM/Softmax cases could in this biological view be interpreted as gradual forgetting, since it would have the effect of driving all synaptic weights w towards zero after every parameter update. . A single neuron can be used to implement a binary classifier (e.g. binary Softmax or binary SVM classifiers) . Activation functions . TLDR: . Q: What neuron type should I use? . A: Use the ReLU non-linearity, be careful with your learning rates and possibly monitor the fraction of “dead” units in a network. If this concerns you, give Leaky ReLU or Maxout a try. Never use sigmoid. Try tanh, but expect it to work worse than ReLU/Maxout. . It is very rare to mix and match different types of neurons in the same network, even though there is no fundamental problem with doing so. . Use the ReLU non-linearity, be careful with your learning rates and possibly monitor the fraction of “dead” units in a network. . Never use sigmoid (except in RNN/LSTM) . Every activation function (or non-linearity) takes a single number and performs a certain fixed mathematical operation on it. There are several activation functions you may encounter in practice: . Sigmoid non-linearity squashes real numbers to range between [0,1] . . The tanh non-linearity squashes real numbers to range between [-1,1]. . . Sigmoid . σ(x)=11+e−x sigma(x) = frac{1}{1 + e^{-x}}σ(x)=1+e−x1​ . The sigmoid function has seen frequent use historically since it has a nice interpretation as the firing rate of a neuron: from not firing at all (0) to fully-saturated firing at an assumed maximum frequency (1). In practice, the sigmoid non-linearity has recently fallen out of favor and it is rarely ever used. It has two major drawbacks: . Sigmoids saturate and kill gradients. A very undesirable property of the sigmoid neuron is that when the neuron’s activation saturates at either tail of 0 or 1, the gradient at these regions is almost 0. Recall that during backpropagation, this (local) gradient will be multiplied to the gradient of this gate’s output for the whole objective. Therefore, if the local gradient is very small, it will effectively “kill” the gradient and almost no signal will flow through the neuron to its weights and recursively to its data. Additionally, one must pay extra caution when initializing the weights of sigmoid neurons to prevent saturation. For example, if the initial weights are too large then most neurons would become saturated and the network will barely learn. | Sigmoid outputs are not zero-centered. This is undesirable since neurons in later layers of processing in a Neural Network (more on this soon) would be receiving data that is not zero-centered. This has implications on the dynamics during gradient descent, because if the data coming into a neuron is always positive . e.g. x &gt; 0 elementwise in $f = w^Tx + b$ . then the gradient on the weights w will during backpropagation become either all be positive, or all negative (depending on the gradient of the whole expression f). This could introduce undesirable zig-zagging dynamics in the gradient updates for the weights. However, notice that once these gradients are added up across a batch of data the final update for the weights can have variable signs, somewhat mitigating this issue. Therefore, this is an inconvenience but it has less severe consequences compared to the saturated activation problem above. . | . Sigmoids saturate and kill gradients . Tanh . It squashes a real-valued number to the range [-1, 1]. Like the sigmoid neuron, its activations saturate, but unlike the sigmoid neuron its output is zero-centered. Therefore, in practice the tanh non-linearity is always preferred to the sigmoid nonlinearity**. Also note that the **tanh neuron is simply a scaled sigmoid neuron, in particular the following holds: . tanh⁡(x)=2σ(2x)−1 tanh(x) = 2 sigma(2x) -1tanh(x)=2σ(2x)−1 . in practice the tanh non-linearity is always preferred to the sigmoid nonlinearity . Rectified Linear Unit (ReLU) activation function, which is zero when x &lt; 0 and then linear with slope 1 when x &gt; 0. . . A plot from Krizhevsky et al. (pdf) paper indicating the 6x improvement in convergence with the ReLU unit compared to the tanh unit. . . ReLU . It computes the function . f(x)=max⁡(0,x)f(x) = max(0, x)f(x)=max(0,x) . In other words, the activation is simply thresholded at zero. There are several pros and cons to using the ReLUs: . (+) It was found to greatly accelerate (e.g. a factor of 6 in Krizhevsky et al.) the convergence of stochastic gradient descent compared to the sigmoid/tanh functions. It is argued that this is due to its linear, non-saturating form. | (+) Compared to tanh/sigmoid neurons that involve expensive operations (exponentials, etc.), the ReLU can be implemented by simply thresholding a matrix of activations at zero. | (-) Unfortunately, ReLU units can be fragile during training and can “die”. For example, a large gradient flowing through a ReLU neuron could cause the weights to update in such a way that the neuron will never activate on any datapoint again. If this happens, then the gradient flowing through the unit will forever be zero from that point on. That is, the ReLU units can irreversibly die during training since they can get knocked off the data manifold. For example, you may find that as much as 40% of your network can be “dead” (i.e. neurons that never activate across the entire training dataset) if the learning rate is set too high. With a proper setting of the learning rate this is less frequently an issue. | . ReLU: . 👍 Faster convergence than sigmoid/tanh. . 👍 Easier to implement, faster to evaluate. . 👎 Dying ReLU caused by big learning rate during training, the neuron won’t ever activate again . Some other activation functions . Leaky ReLU. Leaky ReLUs are one attempt to fix the “dying ReLU” problem. Instead of the function being zero when x &lt; 0, a leaky ReLU will instead have a small negative slope (of 0.01, or so). That is, the function computes . f(x)=1(x&lt;0)(αx)+1(x&gt;=0)(x)f(x) = mathbb{1}(x &lt; 0) ( alpha x) + mathbb{1}(x&gt;=0) (x)f(x)=1(x&lt;0)(αx)+1(x&gt;=0)(x) . where alpha is a small constant. The results are not always consistent. The slope in the negative region can also be made into a parameter of each neuron, as seen in PReLU neurons, introduced in Delving Deep into Rectifiers, by Kaiming He et al., 2015. However, the consistency of the benefit across tasks is presently unclear. . Maxout . The Maxout neuron (introduced recently by Goodfellow et al.) that generalizes the ReLU and its leaky version. The Maxout neuron computes the function . max⁡(w1Tx+b1,w2Tx+b2) max(w_1^Tx+b_1, w_2^Tx + b_2)max(w1T​x+b1​,w2T​x+b2​) . Notice that both ReLU and Leaky ReLU are a special case of this form (for example, for ReLU we have $w_1, b_1 = 0$). The Maxout neuron therefore enjoys all the benefits of a ReLU unit (linear regime of operation, no saturation) and does not have its drawbacks (dying ReLU). However, unlike the ReLU neurons it doubles the number of parameters for every single neuron, leading to a high total number of parameters. . This concludes our discussion of the most common types of neurons and their activation functions. As a last comment, it is very rare to mix and match different types of neurons in the same network, even though there is no fundamental problem with doing so. . Data preprocessing . Normalization . In ML, it’s common practice to normalize the data for each feature (zero-center and standardize). . However, for images, standardize is not needed, zero center is good. . Normalize variances: apply PCA and whitening* . In ML it’s also common to apply PCA to the data to “decorrelate” them. A step further is to squash the covariance matrix to identity matrix so the two variances are equal. . . However, these are not used for images. . Image-specific preprocessing . Just do mean centering. Find the “mean image” for all images (or for each channel across all images), subtract it out from all images. . Neural network architectures . Layer-wise organization . Neural Networks as neurons in graphs. . . . Left: A 2-layer Neural Network (one hidden layer of 4 neurons (or units) and one output layer with 2 neurons), and three inputs. Right: A 3-layer neural network with three inputs, two hidden layers of 4 neurons each and one output layer. . Naming conventions. Notice that when we say N-layer neural network, we do not count the input layer. Therefore, a single-layer neural network describes a network with no hidden layers (input directly mapped to output). In that sense, you can sometimes hear people say that logistic regression or SVMs are simply a special case of single-layer Neural Networks. You may also hear these networks interchangeably referred to as “Artificial Neural Networks” (ANN) or “Multi-Layer Perceptrons” (MLP). . Output layer. Unlike all layers in a Neural Network, the output layer neurons most commonly do not have an activation function (or you can think of them as having a linear identity activation function). This is because the last output layer is usually taken to represent the class scores (e.g. in classification), which are arbitrary real-valued numbers, or some kind of real-valued target (e.g. in regression). . Sizing neural networks = number of parameters. Working with the two example networks in the above picture: . The first network (left) has 4 + 2 = 6 neurons (not counting the inputs), [3 x 4] + [4 x 2] = 20 weights and 4 + 2 = 6 biases, for a total of 26 learnable parameters. | The second network (right) has 4 + 4 + 1 = 9 neurons, [3 x 4] + [4 x 4] + [4 x 1] = 12 + 16 + 4 = 32 weights and 4 + 4 + 1 = 9 biases, for a total of 41 learnable parameters. | . To give you some context, modern Convolutional Networks contain on orders of 100 million parameters and are usually made up of approximately 10-20 layers (hence deep learning). However, as we will see the number of effective connections is significantly greater due to parameter sharing. More on this in the Convolutional Neural Networks module. . Repeated matrix multiplications interwoven with activation function. One of the primary reasons that Neural Networks are organized into layers is that this structure makes it very simple and efficient to evaluate Neural Networks using matrix vector operations. Working with the example three-layer neural network in the diagram above, the input would be a [3x1] vector. All connection strengths for a layer can be stored in a single matrix. For example, the first hidden layer’s weights W1 would be of size [4x3], and the biases for all units would be in the vector b1, of size [4x1]. Here, every single neuron has its weights in a row of W1, so the matrix vector multiplication np.dot(W1,x) evaluates the activations of all neurons in that layer. Similarly, W2 would be a [4x4] matrix that stores the connections of the second hidden layer, and W3 a [1x4] matrix for the last (output) layer. The full forward pass of this 3-layer neural network is then simply three matrix multiplications, interwoven with the application of the activation function: . # forward-pass of a 3-layer neural network: f = lambda x: 1.0/(1.0 + np.exp(-x)) # activation function (use sigmoid) x = np.random.randn(3, 1) # random input vector of three numbers (3x1) h1 = f(np.dot(W1, x) + b1) # calculate first hidden layer activations (4x1) h2 = f(np.dot(W2, h1) + b2) # calculate second hidden layer activations (4x1) out = np.dot(W3, h2) + b3 # output neuron (1x1) . In the above code, W1, W2, W3, b1, b2, b3 are the learnable parameters of the network. Notice also that instead of having a single input column vector, the variable x could hold an entire batch of training data (where each input example would be a column of x) and then all examples would be efficiently evaluated in parallel. Notice that the final Neural Network layer usually doesn’t have an activation function (e.g. it represents a (real-valued) class score in a classification setting). . The forward pass of a fully-connected layer corresponds to one matrix multiplication followed by a bias offset and an activation function. . One way to look at Neural Networks with fully-connected layers is that they define a family of functions that are parameterized by the weights of the network. A natural question that arises is: What is the representational power of this family of functions? In particular, are there functions that cannot be modeled with a Neural Network? . It turns out that Neural Networks with at least one hidden layer are universal approximators. That is, it can be shown (e.g. see Approximation by Superpositions of Sigmoidal Function from 1989 (pdf), or this intuitive explanation from Michael Nielsen) that given any continuous function f(x) and some $ epsilon &gt; 0$, there exists a Neural Network g(x) with one hidden layer (with a reasonable choice of non-linearity, e.g. sigmoid) such that $ forall x, mid f(x) - g(x) mid &lt; epsilon$ . In other words, the neural network can approximate any continuous function. . If one hidden layer suffices to approximate any function, why use more layers and go deeper? The answer is that the fact that a two-layer Neural Network is a universal approximator is, while mathematically cute, a relatively weak and useless statement in practice. . In one dimension, the “sum of indicator bumps” function . g(x)=∑ici1(ai&lt;x&lt;bi)g(x) = sum_i c_i mathbb{1}(a_i &lt; x &lt; b_i)g(x)=i∑​ci​1(ai​&lt;x&lt;bi​) . where a,b,c are parameter vectors is also a universal approximator, but no one would suggest that we use this functional form in Machine Learning. Neural Networks work well in practice because they compactly express nice, smooth functions that fit well with the statistical properties of data we encounter in practice, and are also easy to learn using our optimization algorithms (e.g. gradient descent). Similarly, the fact that deeper networks (with multiple hidden layers) can work better than a single-hidden-layer networks is an empirical observation, despite the fact that their representational power is equal. . Q: If one hidden layer suffices to approximate any function, why use more layers and go deeper? . A: The fact that deeper networks (with multiple hidden layers) can work better than a single-hidden-layer networks is an empirical observation, despite the fact that their representational power is equal. . As an aside, (for fully-connected nets) in practice it is often the case that 3-layer neural networks will outperform 2-layer nets, but going even deeper (4,5,6-layer) rarely helps much more. This is in stark contrast to Convolutional Networks, where depth has been found to be an extremely important component for a good recognition system (e.g. on order of 10 learnable layers). One argument for this observation is that images contain hierarchical structure (e.g. faces are made up of eyes, which are made up of edges, etc.), so several layers of processing make intuitive sense for this data domain. . For fully-connected nets, 2 or 3 layers are good enough, going deeper (4, 5, 6 layers) is rarely more helpful. . For CNNs, deeper nets are more powerful because images contain hierarchical structure. . The full story is, of course, much more involved and a topic of much recent research. If you are interested in these topics we recommend for further reading: . Deep Learning book in press by Bengio, Goodfellow, Courville, in particular Chapter 6.4. | Do Deep Nets Really Need to be Deep? | FitNets: Hints for Thin Deep Nets | . Setting number of layers and their sizes . TLDR: Use large networks with regularization, do not use small networks! . As we increase the size and number of layers in a Neural Network, the capacity of the network increases. That is, the space of representable functions grows since the neurons can collaborate to express many different functions. For example, suppose we had a binary classification problem in two dimensions. We could train three separate neural networks, each with one hidden layer of some size and obtain the following classifiers: . . Larger Neural Networks can represent more complicated functions. . ConvNetsJS demo. . Neural Networks with more neurons can express more complicated functions. However, this is both a blessing (since we can learn to classify more complicated data) and a curse (since it is easier to overfit the training data). Overfitting occurs when a model with high capacity fits the noise in the data instead of the (assumed) underlying relationship. For example, the model with 20 hidden neurons fits all the training data but at the cost of segmenting the space into many disjoint red and green decision regions. The model with 3 hidden neurons only has the representational power to classify the data in broad strokes. It models the data as two blobs and interprets the few red points inside the green cluster as outliers (noise). In practice, this could lead to better generalization on the test set. . Based on our discussion above, it seems that smaller neural networks can be preferred if the data is not complex enough to prevent overfitting. However, this is incorrect - there are many other preferred ways to prevent overfitting in Neural Networks that we will discuss later (such as L2 regularization, dropout, input noise). In practice, it is always better to use these methods (regularization) to control overfitting instead of the number of neurons. . The subtle reason behind this is that smaller networks are harder to train with local methods such as Gradient Descent: It’s clear that their loss functions have relatively few local minima, but it turns out that many of these minima are easier to converge to, and that they are bad (i.e. with high loss). Conversely, bigger neural networks contain significantly more local minima, but these minima turn out to be much better in terms of their actual loss. Since Neural Networks are non-convex, it is hard to study these properties mathematically, but some attempts to understand these objective functions have been made, e.g. in a recent paper The Loss Surfaces of Multilayer Networks. In practice, what you find is that if you train a small network the final loss can display a good amount of variance - in some cases you get lucky and converge to a good place but in some cases you get trapped in one of the bad minima. On the other hand, if you train a large network you’ll start to find many different solutions, but the variance in the final achieved loss will be much smaller. In other words, (for large networks) all solutions are about equally as good, and rely less on the luck of random initialization. . To reiterate, the regularization strength is the preferred way to control the overfitting of a neural network. We can look at the results achieved by three different settings: . . The effects of regularization strength: Each neural network above has 20 hidden neurons, but changing the regularization strength makes its final decision regions smoother with a higher regularization. You can play with these examples in this . ConvNetsJS demo. . The takeaway is that you should not be using smaller networks because you are afraid of overfitting. Instead, you should use as big of a neural network as your computational budget allows, and use other regularization techniques to control overfitting. . Summary . We discussed several types of activation functions that are used in practice, with ReLU being the most common choice | We saw that this layered architecture enables very efficient evaluation of Neural Networks based on matrix multiplications interwoven with the application of the activation function. | We saw that that Neural Networks are universal function approximators, but we also discussed the fact that this property has little to do with their ubiquitous use. They are used because they make certain “right” assumptions about the functional forms of functions that come up in practice. | We discussed the fact that larger networks will always work better than smaller networks, but their higher model capacity must be appropriately addressed with stronger regularization (such as higher weight decay, dropout, etc.). | . Reference: . https://cs231n.github.io/neural-networks-1/ .",
            "url": "http://blog.logancyang.com/note/theory/2018/06/23/cs231n-note.html",
            "relUrl": "/note/theory/2018/06/23/cs231n-note.html",
            "date": " • Jun 23, 2018"
        }
        
    
  
    
        ,"post28": {
            "title": "Hinton Neural Network course",
            "content": "Regression . The word regression comes from “regression to the mean” like the parent-children height example. Later it actually means using functional form to approximate a bunch of data points. . i.i.d.: Independent and identically distributed, a fundamental assumption for the data. . Linear Regression | Polynomial Regression | Cross Validation Split the training set into training and validation set. K-fold cross validation. | To determine the complexity of the model (degree of polynomial in the case of polynomial regression), choose the complexity with the lowest validation error | . | . . Neural Networks . Perceptron: $ mathbf{wx} = y $, threshold $ y $ to get $ hat{y} $ If linearly separable, it will find a solution | If not, it won’t stop. But there’s no way to know when to stop and declare it’s not linearly separable. | . | Gradient Descent: $ mathbf{wx} = a $, activation. There’s no thresholding. | Perceptron vs. Gradient descent Perceptron: Δwi=η∗(y−y^)xi Delta w_i = eta * (y - hat y) x_iΔwi​=η∗(y−y^​)xi​ . | Gradient descent: Δwi=η∗(y−a)xi Delta w_i = eta * (y - a) x_iΔwi​=η∗(y−a)xi​ . | Difference: activation with or without thresholding. Gradient descent is more robust, it needs a differentiable loss. The 1-0 loss of perceptron is not differentiable. | To get a differentiable / softer thresholding function, we have sigmoid (literally means s-like). | . | Sigmoid unit in NN is just a softer thresholding version of Perceptron. | Back-propagation for error: computationally beneficial organization of the chain rule. | Local optima: For a single sigmoid unit, the error looks like a parabola because its quadratic. For many sigmoid units as in NN, there will be a lot of local optima. | Learning = optimization | Note that a 1-layer NN with sigmoid activation is equivalent to Logistic Regression! | NN complexity More nodes | More layers | Larger weights | . | Restriction bias: what it is that you are able to represent, the restriction on possible hypotheses functions Started out with Perceptron, linear | Then more perceptrons for more complex functions | Then use sigmoids instead of 1-0 hard thresholding | So NN doesn’t have much restriction. It can represent any boolean function (more units), any continuous function (one hidden layer), and any arbitrary function with discontinuities (more hidden layers). | Danger of overfitting: use certain network structures, and cross validation. | . | Preference bias: given two algorithm representations, why we prefer one over the other Initialize weights at small random values | Always prefer simpler models when the error is similar: Occam’s Razor | . | Summary Perceptrons: thresholding unit | Networks can produce any Boolean function. | Perceptron rule: finite time for linearly separable data | General differentiable rule - back propagation &amp; gradient descent | . | .",
            "url": "http://blog.logancyang.com/note/2017/01/22/regression-and-nn.html",
            "relUrl": "/note/2017/01/22/regression-and-nn.html",
            "date": " • Jan 22, 2017"
        }
        
    
  
    
        ,"post29": {
            "title": "Graph Search: Subsets, Permutations",
            "content": "Traits for using BFS: . 1. Shortest path in a simple graph: given a initial state, a final state, a transition rule between states, ask how many (the least #) transitions from init to final. 2. Graph traversal. Only visit each node once. . Traits for using DFS: . (DFS method: build search tree, check conditions for recursing down a branch) . 1. Enumerate subsets, permutations. 2. Find all possible solutions. . Generally, when we do a search to enumerate cases using DFS recursion, there are 3 steps we should keep in mind, . 1. Define the recursion. 2. Think about what to do in the base case. When should we return directly. 3. In general cases (other than base case), how to make the problem smaller and recurse down. . Subsets I &amp; II (DFS template) . The thinking is to categorize cases by different head items. Enumerate the head item of a case (path) in a for loop in this way: . 1. Append the item into the path. 2. DFS recurse down onto the next case (generally a smaller case, with advanced index and/or updated reference parameter). 3. Pop the item from the path, to iterate to a different head item on the next iteration. . For this specific Subsets problem, the base case is just adding the current path. For Subsets II, the only difference is that the input can have duplicates and we don’t want the result subsets to be duplicate sets. Since the input is sorted (or we sort it by ourselves before DFS), when we encounter a number which is equal to the previous number in the for loop, we continue. Because the same number is taking the same place as the previous one, the resulting subsets with either of them are the same sets. . Permutations I &amp; II (DFS template) . It’s quite similar to the Subsets problems. The thinking is also to categorize cases by different head items, and enumerate the head item of a case (path) in a for loop. The difference is that now we don’t want to keep track of the index as a parameter passed into DFS. Our base case is that when the path has the same length as the original input sequence, the current path is added. . The for loop is now as such: . 1. Append the item into the path. 2. DFS recurse down after appending the new head item. Avoid the same number by checking if it&#39;s already in path, if yes, continue. 3. Pop the item from the path, iterate to a different head item on the next iteration. . For permutations II where we allow duplicates in the input list, we must sort it first and then do DFS. In the results, duplicate permutations must be avoided, but how? We introduce a new list, visited. We only add continuous same numbers to path, meaning if the previous same number is not visited, we continue. Check the code for details. . Summary for Subsets and Permutations . This kind of problems is not easy to understand. Recursion tree diagrams can help to clarify, but also keep in mind the code templates: inside the for loop, check condition to recurse down, then append in path, DFS down with path (with appropriate update in some parameter), pop from path. . . Subsets . . class Solution: &quot;&quot;&quot; @param S: The set of numbers. @return: A list of lists. See example. &quot;&quot;&quot; def subsets(self, S): if S is None or len(S) == 0: return [] S.sort() self.results = [] self.DFS([], 0, S) return self.results def DFS(self, path, ind, S): # base case, add each path of each recursion (sorted as required) # must make new list, list(path). If not, # res (path) points to the obj passed in, which is empty at the beginning res = list(path) self.results.append(res) # i is the first item&#39;s index in a path for i in xrange(ind, len(S)): path.append(S[i]) self.DFS(path, i+1, S) path.pop() . . Permutations: . . class Solution: &quot;&quot;&quot; @param nums: A list of Integers. @return: A list of permutations. &quot;&quot;&quot; def permute(self, nums): if nums is None or len(nums) == 0: return [] self.results = [] self.DFS([], nums) return self.results def DFS(self, path, nums): # base case if len(path) == len(nums): # must make new list, list(path). If not, # it points to the obj passed in, which is empty at # the beginning self.results.append(list(path)) return for i in xrange(len(nums)): # check if the ith number is already in path if nums[i] in path: continue path.append(nums[i]) self.DFS(path, nums) path.pop() . Combination Sum is the Sum version of Subsets, with duplicates allowed. . . Palindrome Partitioning . We deem the cuts to be the member of a subset, and this problem becomes finding all subsets of valid cuts. If there are N cuts, we can choose whether to include each cut, so there are 2^N ways to cut our string. For O(2^N) problems, it’s usually a Subsets problem. . The thinking is that, we have a substring from start to i, s[start:i], called prefix. This is the next head item of the new node (path) in the DFS tree, we later append it to path, DFS down, and pop. But before that we should check if it is a valid palindrome. . For a fixed start, we loop through all substrings starting there, check if it satisfies the condition (palindrome in this case), if yes DFS down starting at i (the next char after s[old_start:i]). Again the template of DFS: . for i in range(old_start_ind, data.length): get next_head item check if next_head satisfies our condition if not, continue path.append(next_head) DFS(path, i or i+1, data) # i or i+1 greater than old_start_ind path.pop() . For &quot;aab&quot;, we have start = 0: |a|ab, |aa|b, |aab|; start = 1: a|a|b, a|ab|; start = 2: aa|b|; start = 3 == len, aab|, add one full path and return. start progresses by new recursion, i scans inside each recursion from start+1 to len+1. . This is the method to enumerate all substrings that satisfies some condition. . . Factor Combinations . For example, 8 -&gt; [[2, 2, 2], [2, 4]]. Do not include 1 or n as a factor, and each sublist should be ascending. . This is a typical DFS problem: list all solutions (combinations) or a certain decomposition problem, in this case, factorization. . class Solution: def factors(self, n): self.result = [] if n &lt;= 1: return self.result path = [] self.dfs(n, path) return self.result def dfs(self, n, path): # base case # len(path) == 1 is the case [n], which is not included # in this question if n == 1 and len(path) &gt; 1: self.result.append(list(path)) return # recursion # factor must include n itself # or when it&#39;s down to the last factor, it&#39;s not added for factor in xrange(2, n+1): # check if it&#39;s a factor of not if n % factor != 0: continue # ensure ascending order if path == [] or path[-1] &lt;= factor: path.append(factor) self.dfs(n/factor, path) path.pop() return . 3 points that need special attention in this particular problem, . For n &lt;= 3, result = [] | For the loop from 2 to n, must include n: for factor in xrange(2, n+1): .... Because if we don’t include n, the factors are not added to path. For example, | . If we use xrange(2, n)... Input: 8 Ex1: 1st level DFS: n = 8, factor = 2, dfs(n/2 = 4, [2]) 2nd level DFS: n = 4, factor = 2, dfs(n/2 = 2, [2, 2]) 3rd level DFS: n = 2, factor is in xrange(2, 2) which is nothing, abort Failed to add path [2, 2, 2] Ex2: 1st level DFS: n = 8, factor = 2, dfs(n/2 = 4, [2]) 2nd level DFS: n = 4, factor is in xrange(2, 4), factor cannot reach 4 Failed to add path [2, 4] . To ensure ascending order in paths, check for path == [] or path[-1] &lt;= factor before append(factor). | . (Note that Python has short circuit evaluation in the conditionals. For or it means that if path == [], the latter part(s) won’t be checked. So there won’t be a case where path[-1] does not exist in this expression.) .",
            "url": "http://blog.logancyang.com/note/algo/2015/10/03/graphsearch.html",
            "relUrl": "/note/algo/2015/10/03/graphsearch.html",
            "date": " • Oct 3, 2015"
        }
        
    
  
    
        ,"post30": {
            "title": "Handy Python Notes",
            "content": "Key Concepts . Immutable = hashable = can be element of a set (or key of dict) | mutable = unhashable = cannot be element of a set (or key of dict) . | immutables in Python: int, float, long, complex | str | tuple | bytes | frozenset | . | mutables byte array | list | set | dict | . | Do not modify an container while looping over it. Create a copy and loop on that. . | DO NOT ASSIGN STRING CHAR AS IN LIST!!! list(string), modify, and ‘’.join(list) to get string back | To sort a string: | . | . &gt;&gt;&gt; s = &quot;dcba&quot; &gt;&gt;&gt; sorted_s_list = sorted(s) [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;] &gt;&gt;&gt; sorted_s = &#39;&#39;.join(sorted(s)) &quot;abcd&quot; . An example of function: | . S = [9, 9, 8, 7, 6] # cannot swap S items without passing S in, if a and b are indices def swapWrong(a, b): X tmp = a a = b b = tmp &gt;&gt;&gt; swapWrong(S[0], S[3]) &gt;&gt;&gt; S [9, 9, 8, 7, 6] def swap(S, a, b): V tmp = S[a] S[b] = S[a] S[a] = tmp &gt;&gt;&gt; swap(S, 0, 3) &gt;&gt;&gt; S [7, 9, 8, 9, 6] . . Swap 2 Elements in a List . Usually in all languages it looks like, | . tmp = A[i] A[i] = A[j] A[j] = tmp . In Python it can be as simple as A[i], A[j] = A[j], A[i]! | . . The List Count Method . The list data structure in Python has a method list.count(element) which takes O(n) time. | Can be high dimensional list, e.g. board[:m][:n].count(element). Side note: set(2DList) =&gt; 1DSet | . | If we want all counts, using a dict as a histogram gives all counts in one pass. | . . Use Dict as a Histogram . Don’t forget to check if key exists. | . for item in list: if item not in map: map[item] = 0 map[item] += 1 . A better way, use dict.get(key, default=None). default – This is the Value to be returned in case key does not exist. | . | . d = {} for color in colors: d[color] = d.get(color, 0) + 1 . . Zip 2 Lists into a Dictionary . zip two lists into a list of tuples | dict the list of tuples into a dictionary: note, the tuples must have length 2, error if 3 | an easy way of constructing a reversed dictionary | . &gt;&gt;&gt; a = [1, 2, 3] &gt;&gt;&gt; b = [&#39;x&#39;, &#39;y&#39;, &#39;z&#39;] &gt;&gt;&gt; ab = zip(a, b) &gt;&gt;&gt; ab [(1, &#39;x&#39;), (2, &#39;y&#39;), (3, &#39;z&#39;)] &gt;&gt;&gt; dict_ab = dict(ab) &gt;&gt;&gt; dict_ab {1: &#39;x&#39;, 2: &#39;y&#39;, 3: &#39;z&#39;} &gt;&gt;&gt; dict_ba = {value: key for key, value in dict_ab.iteritems()} &gt;&gt;&gt; dict_ba {&#39;y&#39;: 2, &#39;x&#39;: 1, &#39;z&#39;: 3} . . Check if a Float is a Whole Number . Two methods: (my_float).is_integer(), or my_float % 1 == 0 | The second one works because in Python, 5.5 % 1 = 0.5, 5.0 % 1 = 0.0, and 0.0 == 0 is True. | . . Get the Index of the Min/Max Value in a List S . If there are duplicates, these methods return the first occurrence | . # print max(enumerate(S)) # returns a tuple (ind, val), key is ind  print max(enumerate(S), key = lambda S: S[1]) # faster  print S.index(max(S)) # easier to read . . Infinity . float(&#39;inf&#39;) is positive inf, float(&#39;-inf&#39;) is negative | Be careful not to multiply inf by 0, it yields nan | . . Import .py File as Module . In Python, modeling a clustering as a set of sets is impossible since the elements of a set must be immutable. | Import self-defined module: put the file.py file into the same directory, and do | import module (as alias) | . . Closure . Closure: http://en.wikipedia.org/wiki/Closure_%28computer_programming%29 | Pass function as parameter into another function. | . def function_a(): def function_b(): . . Lambda . runtime function with no name | often used in conjunction with filter(), map(), reduce() | . def f (x): return x**2 ... &gt;&gt;&gt; print f(8) 64 &gt;&gt;&gt; &gt;&gt;&gt; g = lambda x: x**2 &gt;&gt;&gt; print g(8) 64 . Or pass a small function as argument | . &gt;&gt;&gt; pairs = [(1, &#39;one&#39;), (2, &#39;two&#39;), (3, &#39;three&#39;), (4, &#39;four&#39;)] &gt;&gt;&gt; pairs.sort(key=lambda pair: pair[1]) &gt;&gt;&gt; pairs [(4, &#39;four&#39;), (1, &#39;one&#39;), (3, &#39;three&#39;), (2, &#39;two&#39;)] . . Initialize a 2D Array (Nested List) . USE LIST COMPREHENSION! | DO NOT USE [[None] * ncol] * nrow]! [anything] * const has sublists pointing to the same object. | . S = [[0 for ncol in xrange(5)] for nrow in xrange(3)] &gt;&gt;&gt; S [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]] . Similarly, to initialize a triangle where ncol &lt;= nrow: don’t forget the nrow+1 | . | . S = [[0 for ncol in xrange(nrow+1)] for nrow in xrange(3)] &gt;&gt;&gt; S [[0], [0, 0], [0, 0, 0]] . To initialize a contant 2D(or even higher dimensional) list with the shape of a given list “tri” | . tri = [ [2], [3,4], [6,5,7], [4,1,8,3] ] &gt;&gt;&gt; f = [[0 for item in tri[nrow]] for nrow in xrange(len(tri))] [[0], [0, 0], [0, 0, 0], [0, 0, 0, 0]] . To initialize a 2D list f1 where the value of the first row is 0 to j and the first col is 0 to i. The commas and colons should be removed, just to make it more readable. | . &gt;&gt;&gt; f1 = [[j if i == 0, else: i if j == 0, else: 0 for j in xrange(4)] for i in xrange(4)] [[0, 1, 2, 3], [1, 0, 0, 0], [2, 0, 0, 0], [3, 0, 0, 0]] . . Flatten a 2D Array . sublist in matrix, then, item in sublist. | Nested loops in List Comprehension: outer loop first, then inner loop. | flatten = [item for sublist in matrix for item in sublist] | . # equivalent to flatten = [] for sublist in matrix: for item in sublist: flatten.append(item) . . Find Min/Max in 2D Nested List . &gt;&gt;&gt; S = [[random.randint(1, 10) for ncol in xrange(nrow+1)] for nrow in xrange(3)] &gt;&gt;&gt; S [[3], [1, 2], [5, 9, 2]] &gt;&gt;&gt; min(min(sublist) for sublist in S) 1 . . To Check a List of Booleans to Yield a Boolean: all(), any() . list S = [True, True, True] &gt;&gt;&gt; all(S) is True True &gt;&gt;&gt; any(S) is False False . Beware, DO NOT use it like all([2, 2, 2]) == 2, all() and any() are for booleans inside. | . . If-Else in List Comprehension . To initialize a list f from list S, if S[i] == 0, f[i] = 1, else f[i] = 0. Here is a 2D example: | . &gt;&gt;&gt; f = [[1 if S[i][j] == 0 else 0 for j in xrange(len(S[0]))] for i in xrange(len(S))] . [expr if S[] == val else expr for i in iterable] . | What about elif?? . | . l = [1, 2, 3, 4, 5] for values in l: if values==1: print &#39;yes&#39; elif values==2: print &#39;no&#39; else: print &#39;idle&#39; &gt;&gt;&gt; [&#39;yes&#39; if v == 1 else &#39;no&#39; if v == 2 else &#39;idle&#39; for v in l] [&#39;yes&#39;, &#39;no&#39;, &#39;idle&#39;, &#39;idle&#39;, &#39;idle&#39;] . To initialize a 2D (4*4) list f where f[i][0] = i, f[0][j] = j, else 0: | . &gt;&gt;&gt; [[j if i == 0 else i if j == 0 else 0 for j in xrange(4)] for i in xrange(4)] [[0, 1, 2, 3], [1, 0, 0, 0], [2, 0, 0, 0], [3, 0, 0, 0]] . [expr if i == val else expr if j == val else expr for i in iterable] | very powerful pythonic list comprehension!! | . . Create Dict with Keys from a Set or List . # create a dict with keys from a set (or list), value 0  T = set([1, 2, 4])  dictT = dict((element, 0) for element in T)  print dictT # output: {1: 0, 2: 0, 4: 0} . . Using Lists as Stacks . It’s very easy to use lists as stacks with append() and pop(). Last in first out. | . &gt;&gt;&gt; stack = [3, 4, 5] &gt;&gt;&gt; stack.append(6) &gt;&gt;&gt; stack.append(7) &gt;&gt;&gt; stack [3, 4, 5, 6, 7] &gt;&gt;&gt; stack.pop() 7 &gt;&gt;&gt; stack [3, 4, 5, 6] &gt;&gt;&gt; stack.pop() 6 &gt;&gt;&gt; stack.pop() 5 &gt;&gt;&gt; stack [3, 4] . . Using Lists as Queues . We can use pop(0) and append() to implement the queue behavior but it’s not efficient. Because popping from the first position is slow, all elements must shift. Instead it’s better to use collections.deque and its append() and popleft() methods. | . &gt;&gt;&gt; from collections import deque &gt;&gt;&gt; queue = deque([&quot;Eric&quot;, &quot;John&quot;, &quot;Michael&quot;]) &gt;&gt;&gt; queue.append(&quot;Terry&quot;) # Terry arrives &gt;&gt;&gt; queue.append(&quot;Graham&quot;) # Graham arrives &gt;&gt;&gt; queue.popleft() # The first to arrive now leaves &#39;Eric&#39; &gt;&gt;&gt; queue.popleft() # The second to arrive now leaves &#39;John&#39; &gt;&gt;&gt; queue # Remaining queue in order of arrival deque([&#39;Michael&#39;, &#39;Terry&#39;, &#39;Graham&#39;]) . . A Peculiar Usage of Augment Assignment in Python . The augmented addition operator += behaves unexpectedly in the following case, | . &gt;&gt;&gt; lst = [] &gt;&gt;&gt; print lst + &quot;123&quot; TypeError: can only concatenate list (not &quot;str&quot;) to list &gt;&gt;&gt; lst += &quot;123&quot; &gt;&gt;&gt; print lst [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;] . What happened is that += calls iadd() method and try to modify the list in-place, while adding all the elements of the iterable on the right to the list. | . . Class Method vs. Static Method . @staticmethod function is nothing more than a function defined inside a class. It is callable without instantiating the class first. It’s definition is immutable via inheritance. . | @classmethod function also callable without instantiating the class, but its definition follows Sub class, not Parent class, via inheritance. That’s because the first argument for @classmethod function must always be cls (class). . | . To be continued… . . Some good resources: . The tutorial from Python documentation | Data Structures | I/O | Errors and Exceptions | Classes, Iterators and Generators | Standard Library | .",
            "url": "http://blog.logancyang.com/note/python/2015/07/21/handy-python-notes.html",
            "relUrl": "/note/python/2015/07/21/handy-python-notes.html",
            "date": " • Jul 21, 2015"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, I’m Logan (Chao) Yang, a machine learning engineer and creative coder. . My main interests are machine learning, physics simulation, math animation and music production. This is my fastpages blog for notes, code, Jupyter notebooks, and essays. . I share my sketches, simulations and videos at my personal website. . Feel free to connect with me on LinkedIn. . I also publish on Medium, and occasionally share new things on Twitter. .",
          "url": "http://blog.logancyang.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "http://blog.logancyang.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}