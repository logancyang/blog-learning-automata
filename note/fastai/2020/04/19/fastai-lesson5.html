<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>FastAI Lesson 5: Collaborative Filtering cont.; Backpropagation; Accelerated SGD; NN from Scratch | Learning Automata</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="FastAI Lesson 5: Collaborative Filtering cont.; Backpropagation; Accelerated SGD; NN from Scratch" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="fast.ai note series" />
<meta property="og:description" content="fast.ai note series" />
<link rel="canonical" href="http://blog.logancyang.com/note/fastai/2020/04/19/fastai-lesson5.html" />
<meta property="og:url" content="http://blog.logancyang.com/note/fastai/2020/04/19/fastai-lesson5.html" />
<meta property="og:site_name" content="Learning Automata" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-04-19T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"fast.ai note series","@type":"BlogPosting","headline":"FastAI Lesson 5: Collaborative Filtering cont.; Backpropagation; Accelerated SGD; NN from Scratch","dateModified":"2020-04-19T00:00:00-05:00","datePublished":"2020-04-19T00:00:00-05:00","url":"http://blog.logancyang.com/note/fastai/2020/04/19/fastai-lesson5.html","mainEntityOfPage":{"@type":"WebPage","@id":"http://blog.logancyang.com/note/fastai/2020/04/19/fastai-lesson5.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://blog.logancyang.com/feed.xml" title="Learning Automata" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-164976898-2','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>FastAI Lesson 5: Collaborative Filtering cont.; Backpropagation; Accelerated SGD; NN from Scratch | Learning Automata</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="FastAI Lesson 5: Collaborative Filtering cont.; Backpropagation; Accelerated SGD; NN from Scratch" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="fast.ai note series" />
<meta property="og:description" content="fast.ai note series" />
<link rel="canonical" href="http://blog.logancyang.com/note/fastai/2020/04/19/fastai-lesson5.html" />
<meta property="og:url" content="http://blog.logancyang.com/note/fastai/2020/04/19/fastai-lesson5.html" />
<meta property="og:site_name" content="Learning Automata" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-04-19T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"fast.ai note series","@type":"BlogPosting","headline":"FastAI Lesson 5: Collaborative Filtering cont.; Backpropagation; Accelerated SGD; NN from Scratch","dateModified":"2020-04-19T00:00:00-05:00","datePublished":"2020-04-19T00:00:00-05:00","url":"http://blog.logancyang.com/note/fastai/2020/04/19/fastai-lesson5.html","mainEntityOfPage":{"@type":"WebPage","@id":"http://blog.logancyang.com/note/fastai/2020/04/19/fastai-lesson5.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="http://blog.logancyang.com/feed.xml" title="Learning Automata" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-164976898-2','auto');ga('require','displayfeatures');ga('send','pageview');</script>


    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Learning Automata</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About Me</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">FastAI Lesson 5: Collaborative Filtering cont.; Backpropagation; Accelerated SGD; NN from Scratch</h1><p class="page-description">fast.ai note series</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-04-19T00:00:00-05:00" itemprop="datePublished">
        Apr 19, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      10 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#note">note</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#fastai">fastai</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#discussion-on-basic-nn">Discussion on basic NN</a></li>
<li class="toc-entry toc-h2"><a href="#finetuning-what-happens-when-we-do-transfer-learning-on-resnet-34">Finetuning: what happens when we do transfer learning on resnet-34?</a></li>
<li class="toc-entry toc-h2"><a href="#collaborative-filtering-cont">Collaborative Filtering cont.</a></li>
<li class="toc-entry toc-h2"><a href="#weight-decay-wd-in-learner">Weight Decay wd in learner</a>
<ul>
<li class="toc-entry toc-h3"><a href="#reason-for-having-weight-decay">Reason for having weight decay</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#entity-embeddings-of-categorical-variables">Entity Embeddings of Categorical Variables</a></li>
<li class="toc-entry toc-h2"><a href="#sgd-with-mnist">SGD with MNIST</a>
<ul>
<li class="toc-entry toc-h3"><a href="#optimizer-sgd-with-momentum">Optimizer: SGD with Momentum</a></li>
<li class="toc-entry toc-h3"><a href="#optimizer-rmsprop">Optimizer: RMSprop</a></li>
<li class="toc-entry toc-h3"><a href="#optimizer-adam">Optimizer: Adam</a></li>
<li class="toc-entry toc-h3"><a href="#in-fastai-use-learner-and-it-sets-the-optimizer-adam-or-a-slight-variation-of-adam-by-default-and-you-dont-need-to-worry">In fastai, use learner, and it sets the optimizer (Adam or a slight variation of Adam by default) and you don’t need to worry.</a></li>
<li class="toc-entry toc-h3"><a href="#cross-entropy-loss">Cross-Entropy Loss</a></li>
</ul>
</li>
</ul><h2 id="discussion-on-basic-nn">
<a class="anchor" href="#discussion-on-basic-nn" aria-hidden="true"><span class="octicon octicon-link"></span></a>Discussion on basic NN</h2>

<p>PyTorch tip: anything with an underscore after, means in-place, for example <code class="highlighter-rouge">a.sub_(val)</code> means <code class="highlighter-rouge">a -= val</code>.</p>

<p>SGD:</p>

<ul>
  <li>in code: <code class="highlighter-rouge">a.sub_(lr * a.grad)</code>
</li>
  <li>in formula</li>
</ul>

<hr>
<p><img src="/images/fastai/sgd.png" alt="SGD formula" width="300" align="middle"></p>

<hr>
<p>There two types of tensors in NN</p>

<ul>
  <li>parameters</li>
  <li>activations</li>
</ul>

<p>Input data is one special case in activation.</p>

<p>Activation functions are element-wise functions. Most of the time just use ReLU.</p>

<p><strong>Universal Approximation Theorem: once you stack these tensors together, you can get arbitrarily close approximation of any function.</strong></p>

<p><img src="/images/fastai/params_activations.png" alt="alt text" title="params and activations"></p>

<h2 id="finetuning-what-happens-when-we-do-transfer-learning-on-resnet-34">
<a class="anchor" href="#finetuning-what-happens-when-we-do-transfer-learning-on-resnet-34" aria-hidden="true"><span class="octicon octicon-link"></span></a>Finetuning: what happens when we do transfer learning on resnet-34?</h2>

<p><code class="highlighter-rouge">resnet-34</code> is trained on imagenet where the task was to output probabilities for 1000 classes, so the last layer of weights has 1000 columns.</p>

<p>You probably don’t have 1000 classes, or don’t have the same 1000 classes. So that last layer is of no use to you.</p>

<hr>
<p>When you do <code class="highlighter-rouge">create_cnn</code> in fastai, it deletes the last layer and puts in two new weight matrices and a ReLU in between, the one to the right has <code class="highlighter-rouge"># columns = databunch.c</code>. All previous layers are frozen.
—</p>

<p>It’s faster, needs less memory when we freeze early layers.</p>

<hr>
<p>After <code class="highlighter-rouge">unfreeze</code>, we split our model into a few sections. We think the earlier layers are pretty good, so we give them a smaller learning rate. For example, the first section we give it <code class="highlighter-rouge">lr = 1e-5</code> and the second section we give it <code class="highlighter-rouge">lr = 1e-3</code>, etc. This is called <strong>Discriminative Learning Rate</strong>.
—</p>

<p>We don’t want to mess with already good layers with big learning rate.</p>

<p>When you call <code class="highlighter-rouge">fit</code>,</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># All layers use the same learning rate
</span><span class="n">fit</span><span class="p">(</span><span class="n">num_epoch</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
<span class="c1"># The last layers (added by create_cnn) use 1e-3, and ALL OTHER LAYERS USE THIS VALUE / 3.
# Will discuss the /3 in part II. It's a quirk of batch normalization
</span><span class="n">fit</span><span class="p">(</span><span class="n">num_epoch</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="nb">slice</span><span class="p">(</span><span class="mf">1e-3</span><span class="p">))</span>
<span class="c1"># The last layers added by create_cnn will use 1e-3, the first layers will use 1e-5,
# the middle layers will have equally spaced lr between the two
</span><span class="n">fit</span><span class="p">(</span><span class="n">num_epoch</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="nb">slice</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">))</span>
</code></pre></div></div>

<p>We use different lr for different <em>layer groups</em>.</p>

<p>For Collaborative Filtering, there is only one layer, so use just one lr is fine.</p>

<hr>
<div class="Toast">
   <span class="Toast-icon"><svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>
   <span class="Toast-content">
        Terminology: Matrix multiplication is one form of <b>Affine Functions</b>. Convolution is another form. When we talk about affine functions we mean linear function like matrix multiplication.
    </span>
</div>

<hr>

<h2 id="collaborative-filtering-cont">
<a class="anchor" href="#collaborative-filtering-cont" aria-hidden="true"><span class="octicon octicon-link"></span></a>Collaborative Filtering cont.</h2>

<p>Tip: Multiplying with a one-hot encoded matrix (identity matrix) is equivalent to a row lookup. Therefore, never actually do the matrix multiplication, do row lookup instead for better time and space performance.</p>

<p>Terminology: <strong>embedding</strong> means look something up in an array, which is equivalent to multiplying with an identity matrix, or one-hot encoded matrix.</p>

<p>Once we train a collaborative filtering model, the result user embeddings and movie embeddings contain interesting property of users and movies.</p>

<p>For example, the 1st number in the user embedding could mean whether that user likes a movie with Tom Hanks in it, and the 1st number in the movie embedding is an indicator whether the movie has Tom Hanks. Then, the dot product has a bigger value if both numbers are big, meaning the user will like that movie.</p>

<p>Each dimension in the embedding is a kind of <strong>latent factor</strong>.</p>

<p>Bias term: there are genuine bad movies, and there are users who generally rate movies low or high. That is when the <strong>bias term</strong> is useful. It is the value that’s not relevant to all the latent factors in the embedding.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>learn = collab_learner(data, n_factors=40, y_range=y_range, wd=1e-1)
</code></pre></div></div>

<div class="Toast">
   <span class="Toast-icon"><svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>
   <span class="Toast-content">
        Trick: to get more accuracy on the movie ratings data ranging from 0.5 to 5, set the y_range (which controls the sigmoid asymptotes) to [0, 5.5]
    </span>
</div>

<p><code class="highlighter-rouge">n_factors</code> for <code class="highlighter-rouge">collab_learner</code> is the # columns, or the # embedding dimensions. They are the “latent factors”. This learner is actually doing matrix factorization. <code class="highlighter-rouge">n_factors = 40</code> is an experiment result that works best.</p>

<p>One good trick to identify best and worst movies regardless of latent factors (they are good or bad on average and not affected by user attributes) is to look at bias. For example, these are some generally bad movies,</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[(tensor(-0.3807),
  'Children of the Corn: The Gathering (1996)',
  1.3157894736842106),
 (tensor(-0.2987), 'Mortal Kombat: Annihilation (1997)', 1.9534883720930232),
 (tensor(-0.2976), 'Striptease (1996)', 2.2388059701492535),
 (tensor(-0.2973),
  'Lawnmower Man 2: Beyond Cyberspace (1996)',
  1.7142857142857142),
 (tensor(-0.2699), 'Cable Guy, The (1996)', 2.339622641509434),
 (tensor(-0.2676), 'Free Willy 3: The Rescue (1997)', 1.7407407407407407),
 (tensor(-0.2578), 'Grease 2 (1982)', 2.0),
 (tensor(-0.2526), 'Barb Wire (1996)', 1.9333333333333333),
 (tensor(-0.2502), 'Bio-Dome (1996)', 1.903225806451613),
 (tensor(-0.2438), 'Island of Dr. Moreau, The (1996)', 2.1578947368421053),
 (tensor(-0.2407), 'Crow: City of Angels, The (1996)', 1.9487179487179487),
 (tensor(-0.2275), "Joe's Apartment (1996)", 2.2444444444444445),
 (tensor(-0.2187), 'Leave It to Beaver (1997)', 1.8409090909090908),
 (tensor(-0.2173), 'Lawnmower Man, The (1992)', 2.4461538461538463),
 (tensor(-0.2076), "Stephen King's The Langoliers (1995)", 2.413793103448276)]
</code></pre></div></div>

<p>And these are some generally good movies,</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(tensor(0.5738), "Schindler's List (1993)", 4.466442953020135),
 (tensor(0.5580), 'Titanic (1997)', 4.2457142857142856),
 (tensor(0.5491), 'Silence of the Lambs, The (1991)', 4.28974358974359),
 (tensor(0.5480), 'Shawshank Redemption, The (1994)', 4.445229681978798),
 (tensor(0.5439), 'Star Wars (1977)', 4.3584905660377355),
 (tensor(0.5278), 'L.A. Confidential (1997)', 4.161616161616162),
 (tensor(0.5112), 'As Good As It Gets (1997)', 4.196428571428571),
 (tensor(0.5078), 'Rear Window (1954)', 4.3875598086124405),
 (tensor(0.4927), 'Good Will Hunting (1997)', 4.262626262626263),
 (tensor(0.4855), 'Apt Pupil (1998)', 4.1),
 (tensor(0.4810), 'Casablanca (1942)', 4.45679012345679),
 (tensor(0.4728), 'Usual Suspects, The (1995)', 4.385767790262173),
 (tensor(0.4705), 'Close Shave, A (1995)', 4.491071428571429),
 (tensor(0.4539), 'Boot, Das (1981)', 4.203980099502488),
 (tensor(0.4514), 'Vertigo (1958)', 4.251396648044692)
</code></pre></div></div>

<p>It’s likely that recommending the ones with biggest positive biases is good for new users we don’t know much about.</p>

<p>To understand the latent factors, 40 is too much to look at. We can do PCA and reduce to 3 dimensions.</p>

<div class="Toast">
   <span class="Toast-icon"><svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>
   <span class="Toast-content">
        Note: checking activation layers in NN with PCA is often a good idea, because we have way too many activations to have intuition.
    </span>
</div>

<div class="Toast">
   <span class="Toast-icon"><svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>
   <span class="Toast-content">
        Trick: for image similarity, if we run similarity functions over the image activations directly it can be too large. A better idea is to run PCA on image activations and then use the reduced dimensions to run similarity!
    </span>
</div>

<h2 id="weight-decay-wd-in-learner">
<a class="anchor" href="#weight-decay-wd-in-learner" aria-hidden="true"><span class="octicon octicon-link"></span></a>Weight Decay <code class="highlighter-rouge">wd</code> in <code class="highlighter-rouge">learner</code>
</h2>

<p>Weight decay is a way of <strong>regularization</strong>, it controls the model complexity. <strong>It is the coefficient of the sum of squares of parameters</strong>, effectively reduce the complexity by making unnecessary parameters smaller. The value of weight decay <code class="highlighter-rouge">wd</code> is usually <code class="highlighter-rouge">1e-1</code> (0.1). This is the result of a lot of experiments on different dataset.</p>

<hr>
<p><img src="/images/fastai/wd-formula.png" alt="SGD formula" width="300"></p>

<hr>
<p>Weight decay <code class="highlighter-rouge">wd</code> is a parameter for <strong>all</strong> <code class="highlighter-rouge">learner</code>s in <code class="highlighter-rouge">fastai</code>, even if you don’t see it in the signature of a particular learner, it is there because it’s in the parent <code class="highlighter-rouge">learner</code> class. The default value is 0.01 and not 0.1 because in rare cases, too big a <code class="highlighter-rouge">wd</code> caps the model performance, but too small a <code class="highlighter-rouge">wd</code> just makes the model easy to overfit and doesn’t cap the performance, the solution is to stop early.</p>

<h3 id="reason-for-having-weight-decay">
<a class="anchor" href="#reason-for-having-weight-decay" aria-hidden="true"><span class="octicon octicon-link"></span></a>Reason for having weight decay</h3>

<p>We don’t want too much complexity, but we DO want many parameters to capture the potential curvy bits of reality. The answer is to have many parameters but penalize their values.</p>

<div class="Toast">
   <span class="Toast-icon"><svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>
   <span class="Toast-content">
        Note: complexity != # parameters. Because some parameters can be very small and close to 0, even though we have many of them, they are barely there which means not much complexity.
    </span>
</div>

<h2 id="entity-embeddings-of-categorical-variables">
<a class="anchor" href="#entity-embeddings-of-categorical-variables" aria-hidden="true"><span class="octicon octicon-link"></span></a>Entity Embeddings of Categorical Variables</h2>

<p><a href="https://arxiv.org/pdf/1604.06737.pdf">Paper</a>. Used NN and entity embedding layers for tabular data and got great results.</p>

<div class="Toast">
   <span class="Toast-icon"><svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>
   <span class="Toast-content">
        MAPE: mean average percentage error.
    </span>
</div>

<h2 id="sgd-with-mnist">
<a class="anchor" href="#sgd-with-mnist" aria-hidden="true"><span class="octicon octicon-link"></span></a>SGD with MNIST</h2>

<p>Subclassing is very very common in pytorch. Override the constructor.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Mnist_Logistic</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lin</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="c1"># xb means a minibatch of x
</span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xb</span><span class="p">):</span> <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">lin</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
</code></pre></div></div>

<p>Then create the logistic regression model,</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create the model and put it on GPU
</span><span class="n">model</span> <span class="o">=</span> <span class="n">Mnist_Logistic</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">model</span>
<span class="c1"># Mnist_Logistic(
#   (lin): Linear(in_features=784, out_features=10, bias=True)
# )
</span><span class="n">model</span><span class="o">.</span><span class="n">lin</span>
<span class="c1"># Linear(in_features=784, out_features=10, bias=True)
</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
<span class="c1"># torch.Size([64, 10])
</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">shape</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()]</span>
<span class="c1"># [torch.Size([10, 784]), torch.Size([10])]
# means it takes 784 dim input and output 10 dim,
# then there's the 10d bias
</span></code></pre></div></div>

<p>The SGD update with weight decay,</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lr</span><span class="o">=</span><span class="mf">2e-2</span>
<span class="n">loss_func</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">lr</span><span class="p">):</span>
    <span class="n">wd</span> <span class="o">=</span> <span class="mf">1e-5</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="c1"># weight decay
</span>    <span class="n">w2</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span> <span class="n">w2</span> <span class="o">+=</span> <span class="p">(</span><span class="n">p</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
    <span class="c1"># add to regular loss
</span>    <span class="c1"># THIS IS L2 REGULARIZATION!
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="n">w2</span><span class="o">*</span><span class="n">wd</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="n">p</span><span class="o">.</span><span class="n">sub_</span><span class="p">(</span><span class="n">lr</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
            <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
    <span class="c1"># Tensor.item() -&gt; a normal python number
</span>    <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

<span class="n">losses</span> <span class="o">=</span> <span class="p">[</span><span class="n">update</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">lr</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">train_dl</span><span class="p">]</span>
</code></pre></div></div>

<p>In this case, weight decay is equivalent to L2 regularization because the gradient of the regularization term <code class="highlighter-rouge">wd * w^2</code> gives <code class="highlighter-rouge">2 wd * w</code>.</p>

<div class="Toast">
   <span class="Toast-icon"><svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>
   <span class="Toast-content">
        Later there's a case where weight decay != L2 regularization!!
    </span>
</div>

<p>Weight decay helps prevent overfitting, so</p>
<ul>
  <li>we can have a giant model without overfitting</li>
  <li>or we can have a smaller dataset</li>
</ul>

<p>Too much weight decay can also hurt training.</p>

<p>The above is a logistic regression by pytorch from scratch. Here we make an NN with pytorch from scratch.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Mnist_NN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lin1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lin2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xb</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lin1</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">lin2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Mnist_NN</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">lr</span><span class="p">):</span>
    <span class="c1"># Instead of SGD we can use `optim.Adam`
</span>    <span class="n">opt</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="p">)</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

<span class="n">losses</span> <span class="o">=</span> <span class="p">[</span><span class="n">update</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="mf">1e-3</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">train_dl</span><span class="p">]</span>
</code></pre></div></div>

<h3 id="optimizer-sgd-with-momentum">
<a class="anchor" href="#optimizer-sgd-with-momentum" aria-hidden="true"><span class="octicon octicon-link"></span></a>Optimizer: SGD with Momentum</h3>

<p>Apply exponentially weighted moving average on the last several derivatives (steps). The more recent steps are exponentially higher weighted.</p>

<hr>
<p><img src="/images/fastai/momentum.png" alt="SGD with Momentum formula" width="300"></p>

<hr>
<p>Alpha is the momentum, S are the steps, g is the gradient.</p>

<p>To use it in pytorch, just use <code class="highlighter-rouge">optim.SGD</code> with a <code class="highlighter-rouge">momentum</code> param. Set it to 0.9.</p>

<h3 id="optimizer-rmsprop">
<a class="anchor" href="#optimizer-rmsprop" aria-hidden="true"><span class="octicon octicon-link"></span></a>Optimizer: RMSprop</h3>

<p>Geoff Hinton first mentioned RMSprop in his Coursera NN course! That is the correct way to cite it.</p>

<p>It’s very similar to SGD with momentum. It uses squared gradient in the denominator. If it’s small the step will be big.</p>

<p>If the gradient is consistently small, it will be a small number. If gradient is volatile or consistently big, it will be a big number. This is because <strong>if the gradient is always small we need to take bigger steps</strong>.</p>

<h3 id="optimizer-adam">
<a class="anchor" href="#optimizer-adam" aria-hidden="true"><span class="octicon octicon-link"></span></a>Optimizer: Adam</h3>

<p>Adam is Momentum + RMSprop. It keeps track of the exponentially weighted moving average and the squared gradient.</p>

<p>We still need to set the learning rate, and do learning rate annealing!</p>

<h3 id="in-fastai-use-learner-and-it-sets-the-optimizer-adam-or-a-slight-variation-of-adam-by-default-and-you-dont-need-to-worry">
<a class="anchor" href="#in-fastai-use-learner-and-it-sets-the-optimizer-adam-or-a-slight-variation-of-adam-by-default-and-you-dont-need-to-worry" aria-hidden="true"><span class="octicon octicon-link"></span></a>In fastai, use <code class="highlighter-rouge">learner</code>, and it sets the optimizer (Adam or a slight variation of Adam by default) and you don’t need to worry.</h3>

<p><code class="highlighter-rouge">fit_one_cycle</code> helps you get “super convergence”, i.e. train 10x faster than plain SGD. It has smaller lr and large momentum in the beginning, and then lr increases, momentum decreases. When it’s close to optimum, lr decreases again and momentum increases.</p>

<p><img src="/images/fastai/momentum_lr.png" alt="SGD with Momentum formula"></p>

<p>This is research result by Leslie Smith.</p>

<h3 id="cross-entropy-loss">
<a class="anchor" href="#cross-entropy-loss" aria-hidden="true"><span class="octicon octicon-link"></span></a>Cross-Entropy Loss</h3>

<p>If we have a classifier for Cats and Dogs</p>

<table>
  <thead>
    <tr>
      <th>Cat</th>
      <th>Dog</th>
      <th>Pred(Cat)</th>
      <th>Pred(Dog)</th>
      <th>X-entropy</th>
      <th>Comment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>0</td>
      <td>0.5</td>
      <td>0.5</td>
      <td>0.3</td>
      <td>unsure</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0</td>
      <td>0.98</td>
      <td>0.02</td>
      <td>0.01</td>
      <td>confident, right</td>
    </tr>
    <tr>
      <td>0</td>
      <td>1</td>
      <td>0.9</td>
      <td>0.1</td>
      <td>1</td>
      <td>confident, wrong</td>
    </tr>
    <tr>
      <td>0</td>
      <td>1</td>
      <td>0.5</td>
      <td>0.5</td>
      <td>0.3</td>
      <td>unsure</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0</td>
      <td>0.9</td>
      <td>0.1</td>
      <td>0.05</td>
      <td>confident, right</td>
    </tr>
  </tbody>
</table>

<p>The unsure cases have moderate loss, confident and right has the lowest loss, confident but wrong has the highest loss.</p>

<p>The cross-entropy formula is</p>

<p><code class="highlighter-rouge">-y log(y_hat) - (1-y) log(1-y_hat)</code></p>

<p>Keep in mind that now Cat is 1. This basically means,</p>

<p>If Cat (y=1), then look at log of <code class="highlighter-rouge">Pred(Cat)</code>;</p>

<p>If Dog (y=0), then look at log of <code class="highlighter-rouge">1-Pred(Cat)</code>, i.e. Pred(Dog);</p>

<p>MUST make sure the preds to plug into cross-entropy add to one. To make sure of that, use the <strong>softmax</strong> activation.</p>

<p>For multi-class classification, use <strong>cross-entropy</strong> as loss and <strong>softmax</strong> as activation.</p>

<div class="Toast">
   <span class="Toast-icon"><svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>
   <span class="Toast-content">
        Note: in PyTorch, calling <code>nn.CrossEntropyLoss()</code> actually calculates the softmax behind the scene so there is no need to add a softmax layer manually.
    </span>
</div>

<p>If you use a custom loss and want softmax output, make sure to add the softmax layer at the end.</p>

<div class="Toast">
   <span class="Toast-icon"><svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>
   <span class="Toast-content">
        Note: Regularization include <b>weight decay, batch norm, dropout</b>
    </span>
</div>


  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="logancyang/blog-learning-automata"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/note/fastai/2020/04/19/fastai-lesson5.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Notes, code and essays by Logan Yang.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/logancyang" title="logancyang"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/logancyang" title="logancyang"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
