<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>[ProdML] Amazon SageMaker Part II: Experiments, Deployment, and Model Monitoring | Learning Automata</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="[ProdML] Amazon SageMaker Part II: Experiments, Deployment, and Model Monitoring" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="ProdML SageMaker note series" />
<meta property="og:description" content="ProdML SageMaker note series" />
<link rel="canonical" href="http://blog.logancyang.com/note/sagemaker/prodml/mlops/2020/08/06/prod-ml-ii.html" />
<meta property="og:url" content="http://blog.logancyang.com/note/sagemaker/prodml/mlops/2020/08/06/prod-ml-ii.html" />
<meta property="og:site_name" content="Learning Automata" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-08-06T00:00:00-05:00" />
<script type="application/ld+json">
{"headline":"[ProdML] Amazon SageMaker Part II: Experiments, Deployment, and Model Monitoring","dateModified":"2020-08-06T00:00:00-05:00","description":"ProdML SageMaker note series","datePublished":"2020-08-06T00:00:00-05:00","@type":"BlogPosting","url":"http://blog.logancyang.com/note/sagemaker/prodml/mlops/2020/08/06/prod-ml-ii.html","mainEntityOfPage":{"@type":"WebPage","@id":"http://blog.logancyang.com/note/sagemaker/prodml/mlops/2020/08/06/prod-ml-ii.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://blog.logancyang.com/feed.xml" title="Learning Automata" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-164976898-2','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>[ProdML] Amazon SageMaker Part II: Experiments, Deployment, and Model Monitoring | Learning Automata</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="[ProdML] Amazon SageMaker Part II: Experiments, Deployment, and Model Monitoring" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="ProdML SageMaker note series" />
<meta property="og:description" content="ProdML SageMaker note series" />
<link rel="canonical" href="http://blog.logancyang.com/note/sagemaker/prodml/mlops/2020/08/06/prod-ml-ii.html" />
<meta property="og:url" content="http://blog.logancyang.com/note/sagemaker/prodml/mlops/2020/08/06/prod-ml-ii.html" />
<meta property="og:site_name" content="Learning Automata" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-08-06T00:00:00-05:00" />
<script type="application/ld+json">
{"headline":"[ProdML] Amazon SageMaker Part II: Experiments, Deployment, and Model Monitoring","dateModified":"2020-08-06T00:00:00-05:00","description":"ProdML SageMaker note series","datePublished":"2020-08-06T00:00:00-05:00","@type":"BlogPosting","url":"http://blog.logancyang.com/note/sagemaker/prodml/mlops/2020/08/06/prod-ml-ii.html","mainEntityOfPage":{"@type":"WebPage","@id":"http://blog.logancyang.com/note/sagemaker/prodml/mlops/2020/08/06/prod-ml-ii.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="http://blog.logancyang.com/feed.xml" title="Learning Automata" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-164976898-2','auto');ga('require','displayfeatures');ga('send','pageview');</script>


    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Learning Automata</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About Me</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">[ProdML] Amazon SageMaker Part II: Experiments, Deployment, and Model Monitoring</h1><p class="page-description">ProdML SageMaker note series</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-08-06T00:00:00-05:00" itemprop="datePublished">
        Aug 6, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      23 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#note">note</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#sagemaker">sagemaker</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#prodml">prodml</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#mlops">mlops</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#4-experiments">4. Experiments</a>
<ul>
<li class="toc-entry toc-h3"><a href="#41-an-experiment-example-using-tensorflow-2">4.1 An Experiment Example using TensorFlow 2</a></li>
<li class="toc-entry toc-h3"><a href="#42-an-experiment-example-using-pytorch">4.2 An Experiment Example using PyTorch</a></li>
<li class="toc-entry toc-h3"><a href="#43-tracking-lineage-finding-the-best-model">4.3 Tracking lineage: finding the best model</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#5-deployment">5. Deployment</a>
<ul>
<li class="toc-entry toc-h3"><a href="#51-batch-inference">5.1 Batch inference</a></li>
<li class="toc-entry toc-h3"><a href="#52-online-inference">5.2 Online inference</a></li>
<li class="toc-entry toc-h3"><a href="#53-autoscaling-according-to-demand">5.3 Autoscaling according to demand</a></li>
<li class="toc-entry toc-h3"><a href="#54-delete-the-endpoint">5.4 Delete the endpoint</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#6-model-monitoring">6. Model Monitoring</a>
<ul>
<li class="toc-entry toc-h3"><a href="#61-enable-live-data-capture-for-a-model-endpoint">6.1 Enable live data capture for a model endpoint</a></li>
<li class="toc-entry toc-h3"><a href="#62-generating-constraints-and-suggestions-from-a-baseline-dataset">6.2 Generating constraints and suggestions from a baseline dataset</a></li>
<li class="toc-entry toc-h3"><a href="#63-creating-a-monitoring-schedule">6.3 Creating a monitoring schedule</a></li>
<li class="toc-entry toc-h3"><a href="#64-visualizing-drift-by-comparing-data-distributions">6.4 Visualizing drift by comparing data distributions</a></li>
<li class="toc-entry toc-h3"><a href="#65-delete-resources">6.5 Delete resources</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#other-sagemaker-features">Other SageMaker features</a></li>
<li class="toc-entry toc-h2"><a href="#reference">Reference</a></li>
</ul><p>Previous post: <a href="https://blog.logancyang.com/note/sagemaker/prodml/mlops/2020/08/03/prod-ml.html">part I</a></p>

<h2 id="4-experiments">
<a class="anchor" href="#4-experiments" aria-hidden="true"><span class="octicon octicon-link"></span></a>4. Experiments</h2>

<p>In the previous section we went through the training process. In reality, the model needs a series of experiments to find the best set of hyperparameters. It is very important to properly keep track of the experiment results in a reproducible manner, and setup unified practice among team members. SageMaker Experiments solves this problem.</p>

<p>SageMaker Experiments has its own Python package you can import with:</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">smexperiments.experiment</span> <span class="kn">import</span> <span class="n">Experiment</span>
</code></pre></div></div>

<p>An <code class="highlighter-rouge">experiment</code> should be the outer most group for a collections of <code class="highlighter-rouge">trial</code>s. Each trial can map to a specific algorithm you use, such as a Random Forest, XGBoost, Logistic Regression, etc.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">smexperiments.trial</span> <span class="kn">import</span> <span class="n">Trial</span>
</code></pre></div></div>

<p>A trial is a series of steps, each step is called a <code class="highlighter-rouge">trial_component</code>.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">smexperiments.trial_component</span> <span class="kn">import</span> <span class="n">TrialComponent</span>
</code></pre></div></div>

<p>Each trial component can have a combination of inputs such as datasets, algorithm and parameters. You can produce desired outputs such as models, datasets, metrics and checkpoints.</p>

<p>Examples of trial components are</p>

<ul>
  <li>data preprocessing jobs</li>
  <li>training jobs</li>
  <li>batch transform jobs</li>
</ul>

<p>A trial component can be, e.g. the full preprocessing logic, or each step within it such as imputing missing values, one-hot encoding, etc. Each trial component (a trial step) can store information such as mean and standard deviation or the metric you care about during the experiments so you can compare them.</p>

<p>Next let’s look at examples for training MNIST classifiers using TensorFlow and PyTorch.</p>

<h3 id="41-an-experiment-example-using-tensorflow-2">
<a class="anchor" href="#41-an-experiment-example-using-tensorflow-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>4.1 An Experiment Example using TensorFlow 2</h3>

<p>We use a custom training script provided by TensorFlow for this use case. Check out the script <a href="https://github.com/lpatruno/sagemaker-course/blob/master/scripts/tf/tensorflow_mnist.py">here</a>.</p>

<p>Note that in <code class="highlighter-rouge">_parse_args()</code>, <code class="highlighter-rouge">model_dir</code> is passed from SageMaker and the others are from SageMaker environment variables.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">_parse_args</span><span class="p">():</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>

    <span class="c1"># Data, model, and output directories
</span>    <span class="c1"># model_dir is always passed in from SageMaker. By default this is a S3 path under the default bucket.
</span>    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">'--model_dir'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">'--sm-model-dir'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">'SM_MODEL_DIR'</span><span class="p">))</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">'--train'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">'SM_CHANNEL_TRAINING'</span><span class="p">))</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">'--hosts'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">list</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">'SM_HOSTS'</span><span class="p">)))</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">'--current-host'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">'SM_CURRENT_HOST'</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_known_args</span><span class="p">()</span>
</code></pre></div></div>

<p>Here is the main block which is very simple:</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="n">args</span><span class="p">,</span> <span class="n">unknown</span> <span class="o">=</span> <span class="n">_parse_args</span><span class="p">()</span>

    <span class="n">train_data</span><span class="p">,</span> <span class="n">train_labels</span> <span class="o">=</span> <span class="n">_load_training_data</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">train</span><span class="p">)</span>
    <span class="n">eval_data</span><span class="p">,</span> <span class="n">eval_labels</span> <span class="o">=</span> <span class="n">_load_testing_data</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">train</span><span class="p">)</span>

    <span class="n">mnist_classifier</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">eval_data</span><span class="p">,</span> <span class="n">eval_labels</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">current_host</span> <span class="o">==</span> <span class="n">args</span><span class="o">.</span><span class="n">hosts</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="c1"># save model to an S3 directory with version number '00000001'
</span>        <span class="n">mnist_classifier</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">sm_model_dir</span><span class="p">,</span> <span class="s">'000000001'</span><span class="p">),</span> <span class="s">'my_model.h5'</span><span class="p">)</span>
</code></pre></div></div>

<p>Note: the sample data from S3 is in <code class="highlighter-rouge">.npy</code> format which is much more performant than csv format.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create the Experiment
</span><span class="n">experiment_name</span> <span class="o">=</span> <span class="n">f</span><span class="s">"tf-mnist-{datetime.datetime.now().strftime('</span><span class="si">%</span><span class="s">Y</span><span class="si">%</span><span class="s">m</span><span class="si">%</span><span class="s">d</span><span class="si">%</span><span class="s">H</span><span class="si">%</span><span class="s">M')}"</span>
<span class="n">description</span> <span class="o">=</span> <span class="s">"Classification of mnist hand-written digits using tensorflow 2"</span>

<span class="n">tf_experiment</span> <span class="o">=</span> <span class="n">Experiment</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">experiment_name</span><span class="o">=</span><span class="n">experiment_name</span><span class="p">,</span>
                                  <span class="n">description</span><span class="o">=</span><span class="n">description</span><span class="p">,</span>
                                  <span class="n">sagemaker_boto_client</span><span class="o">=</span><span class="n">sagemaker_client</span><span class="p">)</span>

<span class="c1"># Create the Trial
</span><span class="n">trial_name</span> <span class="o">=</span> <span class="n">f</span><span class="s">"tf-trial-{datetime.datetime.now().strftime('</span><span class="si">%</span><span class="s">Y</span><span class="si">%</span><span class="s">m</span><span class="si">%</span><span class="s">d</span><span class="si">%</span><span class="s">H</span><span class="si">%</span><span class="s">M')}"</span>

<span class="n">tf_trial</span> <span class="o">=</span> <span class="n">Trial</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">trial_name</span><span class="o">=</span><span class="n">trial_name</span><span class="p">,</span>
                        <span class="n">experiment_name</span><span class="o">=</span><span class="n">tf_experiment</span><span class="o">.</span><span class="n">experiment_name</span><span class="p">,</span>
                        <span class="n">sagemaker_boto_client</span><span class="o">=</span><span class="n">sagemaker_client</span><span class="p">)</span>


<span class="kn">from</span> <span class="nn">sagemaker.tensorflow</span> <span class="kn">import</span> <span class="n">TensorFlow</span>

<span class="c1"># Create the training job. This takes 3-5 min
</span><span class="n">tf_estimator</span> <span class="o">=</span> <span class="n">TensorFlow</span><span class="p">(</span>
    <span class="n">entry_point</span><span class="o">=</span><span class="s">'../scripts/tf/tensorflow_mnist.py'</span><span class="p">,</span>
    <span class="n">role</span><span class="o">=</span><span class="n">role</span><span class="p">,</span>
    <span class="n">train_instance_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">train_instance_type</span><span class="o">=</span><span class="s">'ml.p2.xlarge'</span><span class="p">,</span>
    <span class="n">code_location</span><span class="o">=</span><span class="n">f</span><span class="s">"s3://{BUCKET}/{PREFIX}"</span><span class="p">,</span>
    <span class="n">output_path</span><span class="o">=</span><span class="n">f</span><span class="s">"s3://{BUCKET}/{PREFIX}"</span><span class="p">,</span>
    <span class="n">base_job_name</span><span class="o">=</span><span class="s">'tf-mnist'</span><span class="p">,</span>
    <span class="n">py_version</span><span class="o">=</span><span class="s">'py3'</span><span class="p">,</span>
    <span class="n">framework_version</span><span class="o">=</span><span class="s">'2.1.0'</span><span class="p">,</span>
    <span class="n">enable_sagemaker_metrics</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">experiment_config</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s">"TrialName"</span><span class="p">:</span> <span class="n">tf_trial</span><span class="o">.</span><span class="n">trial_name</span><span class="p">,</span>
        <span class="s">"TrialComponentDisplayName"</span><span class="p">:</span> <span class="s">"Training"</span><span class="p">,</span>
    <span class="p">}</span>

<span class="c1"># Now associate the estimator with the Experiment and Trial
</span><span class="n">tf_estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">{</span><span class="s">'training'</span><span class="p">:</span> <span class="n">training_data_uri</span><span class="p">},</span>
                 <span class="n">experiment_config</span><span class="o">=</span><span class="n">experiment_config</span><span class="p">)</span>
</code></pre></div></div>

<p>Notice that the <code class="highlighter-rouge">TensorFlow</code> constructor takes in some additional parameters compared to the previous training jobs: <code class="highlighter-rouge">py_version</code>, <code class="highlighter-rouge">framework_version</code>, and <code class="highlighter-rouge">enable_sagemaker_metrics</code>.</p>

<p>The SageMaker TensorFlow object has legacy mode and script mode, <code class="highlighter-rouge">py_version='py3'</code> indicates that we are using script mode. Legacy mode only supports Python 2. <code class="highlighter-rouge">framework_version</code> is the actual TensorFlow version we want to use.</p>

<p><code class="highlighter-rouge">enable_sagemaker_metrics=True</code> enables logging of metrics such as accuracy over time, so that we can see the visualization outside the notebook in SageMaker Studio.</p>

<p>After executing the code above, you can view your experiment metadata by clicking on the beaker on the sidebar! It shows a two-column table: experiment name and last modified time. First, click on the refresh button in this panel, then you’ll see the experiment you just ran. You can also search by experiment name in its search bar.</p>

<p>Double click on the experiment, it shows the trials under it. Double click on the trial, it then shows the trial components. It looks like this:</p>

<p><img src="/images/prodml/trial-component.png" alt="SageMaker trial component" align="middle"></p>

<p>The <strong>Metrics</strong> tab shows the metrics it automatically scraped from the log of our training job. It’s configured by the TensorFlow estimator object and the Docker image used. You can explore other tabs for more metadata.</p>

<h3 id="42-an-experiment-example-using-pytorch">
<a class="anchor" href="#42-an-experiment-example-using-pytorch" aria-hidden="true"><span class="octicon octicon-link"></span></a>4.2 An Experiment Example using PyTorch</h3>

<p>Now let’s switch to PyTorch and demonstrate how to run multiple trials with different hyperparameters.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>

<span class="o">%</span><span class="n">config</span> <span class="n">InlineBackend</span><span class="o">.</span><span class="n">figure_format</span> <span class="o">=</span> <span class="s">'retina'</span>

<span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,))])</span>

<span class="c1"># Download, load, and transform the data.
</span><span class="n">train_set</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">LOCAL_DATA_DIRECTORY</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">test_set</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">LOCAL_DATA_DIRECTORY</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">sagemaker_session</span><span class="o">.</span><span class="n">upload_data</span><span class="p">(</span>
    <span class="n">path</span><span class="o">=</span><span class="n">LOCAL_DATA_DIRECTORY</span><span class="p">,</span>
    <span class="n">bucket</span><span class="o">=</span><span class="n">BUCKET</span><span class="p">,</span>
    <span class="n">key_prefix</span><span class="o">=</span><span class="n">PREFIX</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">smexperiments.tracker</span> <span class="kn">import</span> <span class="n">Tracker</span>

<span class="k">with</span> <span class="n">Tracker</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">display_name</span><span class="o">=</span><span class="s">"Preprocessing"</span><span class="p">,</span> <span class="n">sagemaker_boto_client</span><span class="o">=</span><span class="n">sagemaker_client</span><span class="p">)</span> <span class="k">as</span> <span class="n">tracker</span><span class="p">:</span>
    <span class="n">tracker</span><span class="o">.</span><span class="n">log_parameters</span><span class="p">({</span>
        <span class="s">"normalization_mean"</span><span class="p">:</span> <span class="mf">0.1307</span><span class="p">,</span>
        <span class="s">"normalization_std"</span><span class="p">:</span> <span class="mf">0.3081</span><span class="p">,</span>
    <span class="p">})</span>

    <span class="n">tracker</span><span class="o">.</span><span class="n">log_input</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">"mnist-dataset"</span><span class="p">,</span> <span class="n">media_type</span><span class="o">=</span><span class="s">"s3/uri"</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">inputs</span><span class="p">)</span>


<span class="n">experiment_name</span> <span class="o">=</span> <span class="n">f</span><span class="s">"torch-mnist-{datetime.datetime.now().strftime('</span><span class="si">%</span><span class="s">Y</span><span class="si">%</span><span class="s">m</span><span class="si">%</span><span class="s">d</span><span class="si">%</span><span class="s">H</span><span class="si">%</span><span class="s">M')}"</span>
<span class="n">description</span> <span class="o">=</span> <span class="s">"Classification of mnist hand-written digits with pytorch."</span>

<span class="n">mnist_experiment</span> <span class="o">=</span> <span class="n">Experiment</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">experiment_name</span><span class="o">=</span><span class="n">experiment_name</span><span class="p">,</span>
                                     <span class="n">description</span><span class="o">=</span><span class="n">description</span><span class="p">,</span>
                                     <span class="n">sagemaker_boto_client</span><span class="o">=</span><span class="n">sagemaker_client</span><span class="p">)</span>


<span class="c1"># %%
</span><span class="kn">from</span> <span class="nn">sagemaker.pytorch</span> <span class="kn">import</span> <span class="n">PyTorch</span>

<span class="n">hidden_channel_trial_name_map</span> <span class="o">=</span> <span class="p">{}</span> <span class="c1"># Keep references to each Trial object
</span>
<span class="c1"># If you want to run the following training jobs asynchronously, you may need to increase
# your resource limit. Otherwise, you can run them sequentially.
</span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">num_hidden_channel</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">]):</span>

    <span class="c1"># create Trial object
</span>    <span class="n">trial_name</span> <span class="o">=</span> <span class="n">f</span><span class="s">"torch-{num_hidden_channel}-hidden-channels-{datetime.datetime.now().strftime('</span><span class="si">%</span><span class="s">Y</span><span class="si">%</span><span class="s">m</span><span class="si">%</span><span class="s">d</span><span class="si">%</span><span class="s">H</span><span class="si">%</span><span class="s">M')}"</span>
    <span class="n">cnn_trial</span> <span class="o">=</span> <span class="n">Trial</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
        <span class="n">trial_name</span><span class="o">=</span><span class="n">trial_name</span><span class="p">,</span>
        <span class="n">experiment_name</span><span class="o">=</span><span class="n">mnist_experiment</span><span class="o">.</span><span class="n">experiment_name</span><span class="p">,</span>
        <span class="n">sagemaker_boto_client</span><span class="o">=</span><span class="n">sagemaker_client</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="c1"># Have a dict for variable references to the trial names
</span>    <span class="n">hidden_channel_trial_name_map</span><span class="p">[</span><span class="n">num_hidden_channel</span><span class="p">]</span> <span class="o">=</span> <span class="n">trial_name</span>

    <span class="c1"># Associate the proprocessing trial component with the current trial
</span>    <span class="n">cnn_trial</span><span class="o">.</span><span class="n">add_trial_component</span><span class="p">(</span><span class="n">tracker</span><span class="o">.</span><span class="n">trial_component</span><span class="p">)</span>

    <span class="c1"># all input configurations, parameters, and metrics specified in estimator
</span>    <span class="c1"># definition are automatically tracked
</span>    <span class="n">estimator</span> <span class="o">=</span> <span class="n">PyTorch</span><span class="p">(</span>
        <span class="n">entry_point</span><span class="o">=</span><span class="s">'../scripts/torch/pytorch_mnist.py'</span><span class="p">,</span>
        <span class="n">role</span><span class="o">=</span><span class="n">role</span><span class="p">,</span>
        <span class="n">sagemaker_session</span><span class="o">=</span><span class="n">sagemaker_session</span><span class="p">,</span>
        <span class="n">framework_version</span><span class="o">=</span><span class="s">'1.1.0'</span><span class="p">,</span>
        <span class="n">train_instance_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">train_instance_type</span><span class="o">=</span><span class="s">'ml.c4.xlarge'</span><span class="p">,</span>
        <span class="n">code_location</span><span class="o">=</span><span class="n">f</span><span class="s">"s3://{BUCKET}/{PREFIX}"</span><span class="p">,</span>
        <span class="n">output_path</span><span class="o">=</span><span class="n">f</span><span class="s">"s3://{BUCKET}/{PREFIX}"</span><span class="p">,</span>
        <span class="n">base_job_name</span><span class="o">=</span><span class="s">'torch-mnist'</span><span class="p">,</span>
        <span class="n">hyperparameters</span><span class="o">=</span><span class="p">{</span>
            <span class="s">'epochs'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
            <span class="s">'backend'</span><span class="p">:</span> <span class="s">'gloo'</span><span class="p">,</span>
            <span class="s">'hidden_channels'</span><span class="p">:</span> <span class="n">num_hidden_channel</span><span class="p">,</span>
            <span class="s">'dropout'</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>
            <span class="s">'optimizer'</span><span class="p">:</span> <span class="s">'sgd'</span>
        <span class="p">},</span>
        <span class="n">metric_definitions</span><span class="o">=</span><span class="p">[</span>
            <span class="p">{</span><span class="s">'Name'</span><span class="p">:</span><span class="s">'train:loss'</span><span class="p">,</span> <span class="s">'Regex'</span><span class="p">:</span><span class="s">'Train Loss: (.*?);'</span><span class="p">},</span>
            <span class="p">{</span><span class="s">'Name'</span><span class="p">:</span><span class="s">'test:loss'</span><span class="p">,</span> <span class="s">'Regex'</span><span class="p">:</span><span class="s">'Test Average loss: (.*?),'</span><span class="p">},</span>
            <span class="p">{</span><span class="s">'Name'</span><span class="p">:</span><span class="s">'test:accuracy'</span><span class="p">,</span> <span class="s">'Regex'</span><span class="p">:</span><span class="s">'Test Accuracy: (.*?)</span><span class="si">%</span><span class="s">;'</span><span class="p">}</span>
        <span class="p">],</span>
        <span class="n">enable_sagemaker_metrics</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Now associate the estimator with the Experiment and Trial
</span>    <span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
        <span class="n">inputs</span><span class="o">=</span><span class="p">{</span><span class="s">'training'</span><span class="p">:</span> <span class="n">inputs</span><span class="p">},</span>
        <span class="n">experiment_config</span><span class="o">=</span><span class="p">{</span>
            <span class="s">"TrialName"</span><span class="p">:</span> <span class="n">cnn_trial</span><span class="o">.</span><span class="n">trial_name</span><span class="p">,</span>
            <span class="s">"TrialComponentDisplayName"</span><span class="p">:</span> <span class="s">"Training"</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="n">wait</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># give it a while before dispatching the next training job
</span>    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>

<p>The core of the code above is to loop through <code class="highlighter-rouge">num_hidden_channel</code> and create a trial for each value of it. We can specify other hyperparameters and a custom regex for metric definition we care about in the <code class="highlighter-rouge">PyTorch</code> object.</p>

<p>This can take 15 minutes to run. After completion, we can click on the beaker icon in the left bar. We can either inspect each individual trial and look at the charts for training over time, or we can look at all trial components in one chart by selecting them and right clicking to open then in a list, select add chart with Summary Statistics and Scatter Plot. It’s quite powerful to create such visualization in the browser without having to write any code.</p>

<p>Experiment is extremely important. For a team, it is good practice to use SageMaker Experiments for storing and grouping results for easy access.</p>

<h3 id="43-tracking-lineage-finding-the-best-model">
<a class="anchor" href="#43-tracking-lineage-finding-the-best-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>4.3 Tracking lineage: finding the best model</h3>

<p>We’ve run our experiments, now we can find the best model simply by running some code in the notebook.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sagemaker.analytics</span> <span class="kn">import</span> <span class="n">ExperimentAnalytics</span>

<span class="n">search_expression</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">"Filters"</span><span class="p">:[</span>
        <span class="p">{</span>
            <span class="s">"Name"</span><span class="p">:</span> <span class="s">"DisplayName"</span><span class="p">,</span>
            <span class="s">"Operator"</span><span class="p">:</span> <span class="s">"Equals"</span><span class="p">,</span>
            <span class="s">"Value"</span><span class="p">:</span> <span class="s">"Training"</span><span class="p">,</span>
        <span class="p">}</span>
    <span class="p">],</span>
<span class="p">}</span>

<span class="n">trial_component_analytics</span> <span class="o">=</span> <span class="n">ExperimentAnalytics</span><span class="p">(</span>
    <span class="n">sagemaker_session</span><span class="o">=</span><span class="n">sagemaker_session</span><span class="p">,</span>
    <span class="n">experiment_name</span><span class="o">=</span><span class="n">mnist_experiment</span><span class="o">.</span><span class="n">experiment_name</span><span class="p">,</span>
    <span class="n">search_expression</span><span class="o">=</span><span class="n">search_expression</span><span class="p">,</span>
    <span class="n">sort_by</span><span class="o">=</span><span class="s">"metrics.test:accuracy.max"</span><span class="p">,</span>
    <span class="n">sort_order</span><span class="o">=</span><span class="s">"Descending"</span><span class="p">,</span>
    <span class="n">metric_names</span><span class="o">=</span><span class="p">[</span><span class="s">'test:accuracy'</span><span class="p">],</span>
    <span class="n">parameter_names</span><span class="o">=</span><span class="p">[</span><span class="s">'hidden_channels'</span><span class="p">,</span> <span class="s">'epochs'</span><span class="p">,</span> <span class="s">'dropout'</span><span class="p">,</span> <span class="s">'optimizer'</span><span class="p">]</span>
<span class="p">)</span>

<span class="n">trial_component_analytics</span><span class="o">.</span><span class="n">dataframe</span><span class="p">()</span>
</code></pre></div></div>

<p>We will see the dataframe as the one below</p>

<p><img src="/images/prodml/trial-comps.png" alt="trial comps" align="middle"></p>

<p>Recall that we have a dictionary <code class="highlighter-rouge">hidden_channel_trial_name_map</code> for variable references to the trials, we can use it to look at the best trial,</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lineage_table</span> <span class="o">=</span> <span class="n">ExperimentAnalytics</span><span class="p">(</span>
    <span class="n">sagemaker_session</span><span class="o">=</span><span class="n">sagemaker_session</span><span class="p">,</span>
    <span class="n">search_expression</span><span class="o">=</span><span class="p">{</span>
        <span class="s">"Filters"</span><span class="p">:[{</span>
            <span class="s">"Name"</span><span class="p">:</span> <span class="s">"Parents.TrialName"</span><span class="p">,</span>
            <span class="s">"Operator"</span><span class="p">:</span> <span class="s">"Equals"</span><span class="p">,</span>
            <span class="s">"Value"</span><span class="p">:</span> <span class="n">hidden_channel_trial_name_map</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="p">}]</span>
    <span class="p">},</span>
    <span class="n">sort_by</span><span class="o">=</span><span class="s">"CreationTime"</span><span class="p">,</span>
    <span class="n">sort_order</span><span class="o">=</span><span class="s">"Ascending"</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">lineage_table</span><span class="o">.</span><span class="n">dataframe</span><span class="p">()</span>
</code></pre></div></div>

<p>Now we see a lot more information about it:</p>

<p><img src="/images/prodml/lineage.png" alt="lineage" align="middle"></p>

<p>SageMaker has more advanced hyperparameter tuning capabilities. You can dig in to learn more.</p>

<h2 id="5-deployment">
<a class="anchor" href="#5-deployment" aria-hidden="true"><span class="octicon octicon-link"></span></a>5. Deployment</h2>

<p>Deployment is an area that’s usually outside the expertise of most data scientists. SageMaker makes it easy to deploy both <strong>batch</strong> and <strong>online</strong> inference as APIs.</p>

<p>In order to deploy, we first need to instantiate a <code class="highlighter-rouge">sagemaker.model.Model</code> object. <code class="highlighter-rouge">model_data</code> is the serialized model.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># See the `sagemaker.model.Model`
# [API reference](https://sagemaker.readthedocs.io/en/stable/model.html) for more details.
</span><span class="kn">from</span> <span class="nn">sagemaker</span> <span class="kn">import</span> <span class="n">model</span>

<span class="n">model_data</span> <span class="o">=</span> <span class="s">'s3://.../model.tar.gz'</span>
<span class="n">image</span> <span class="o">=</span> <span class="s">'...dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:0.90-2-cpu-py3'</span>

<span class="n">churn_model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">model_data</span><span class="o">=</span><span class="n">model_data</span><span class="p">,</span>
                          <span class="n">image</span><span class="o">=</span><span class="n">image</span><span class="p">,</span>
                          <span class="n">role</span><span class="o">=</span><span class="n">role</span><span class="p">)</span>
</code></pre></div></div>

<p>To find the right <code class="highlighter-rouge">model_data</code> and <code class="highlighter-rouge">image</code> pair for the model you want to deploy, find that training job in SageMaker console -&gt; Training Jobs, once you find the right job by name, click into it, and you can find the image uri under the Algorithm panel. For the s3 location of that serialized model, scroll to the bottom of the page and find the Output panel, it’s under <code class="highlighter-rouge">S3 model artifact</code>.</p>

<h3 id="51-batch-inference">
<a class="anchor" href="#51-batch-inference" aria-hidden="true"><span class="octicon octicon-link"></span></a>5.1 Batch inference</h3>

<p>To make a batch inference endpoint, we need <code class="highlighter-rouge">sagemaker.transformer.Transformer</code> object.</p>

<p>See the <code class="highlighter-rouge">sagemaker.transformer.Transformer</code> <a href="https://sagemaker.readthedocs.io/en/stable/transformer.html">API reference</a> for more details.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">churn_transformer</span> <span class="o">=</span> <span class="n">churn_model</span><span class="o">.</span><span class="n">transformer</span><span class="p">(</span><span class="n">instance_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                            <span class="n">instance_type</span><span class="o">=</span><span class="s">'ml.m4.xlarge'</span><span class="p">,</span>
                                            <span class="n">strategy</span><span class="o">=</span><span class="s">'MultiRecord'</span><span class="p">,</span>
                                            <span class="n">assemble_with</span><span class="o">=</span><span class="s">'Line'</span><span class="p">,</span>
                                            <span class="n">output_path</span><span class="o">=</span><span class="n">f</span><span class="s">"s3://{BUCKET}/{PREFIX}/transform"</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="highlighter-rouge">strategy</code> can be <code class="highlighter-rouge">SingleRecord</code> or <code class="highlighter-rouge">MultiRecord</code>. <code class="highlighter-rouge">assemble_with</code> can be <code class="highlighter-rouge">None</code> or <code class="highlighter-rouge">Line</code>.</p>

<p>To run the transform batch job,</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Start a transform job and wait for it to finish
# Takes around 3-5 minutes
</span><span class="n">churn_transformer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">inference_data</span><span class="p">,</span>
                            <span class="n">content_type</span><span class="o">=</span><span class="s">'text/csv'</span><span class="p">,</span>
                            <span class="n">split_type</span><span class="o">=</span><span class="s">'Line'</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">'Waiting for transform job: '</span> <span class="o">+</span> <span class="n">churn_transformer</span><span class="o">.</span><span class="n">latest_transform_job</span><span class="o">.</span><span class="n">job_name</span><span class="p">)</span>
<span class="n">churn_transformer</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
</code></pre></div></div>

<p><code class="highlighter-rouge">content_type</code> is set to <code class="highlighter-rouge">'text/csv'</code> because we have csv data, <code class="highlighter-rouge">split_type</code> set to <code class="highlighter-rouge">'Line'</code> following the <code class="highlighter-rouge">transformer</code> object so it outputs one line at a time.</p>

<p><strong>This job is asynchronous by default</strong>. The <code class="highlighter-rouge">.wait()</code> method makes it blocking the notebook until it returns.</p>

<p>In the log outputs in the notebook, we can see the job actually starts a gunicorn web server.</p>

<p>To get the location of the output data, run</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">churn_transformer</span><span class="o">.</span><span class="n">output_path</span>
</code></pre></div></div>

<p>For each input file we get a corresponding <code class="highlighter-rouge">.out</code> file that contains the predicted labels.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># List files in S3 bucket
</span><span class="n">s3_client</span> <span class="o">=</span> <span class="n">boto_session</span><span class="o">.</span><span class="n">client</span><span class="p">(</span><span class="s">'s3'</span><span class="p">)</span>
<span class="n">s3_client</span><span class="o">.</span><span class="n">list_objects</span><span class="p">(</span><span class="n">Bucket</span> <span class="o">=</span> <span class="n">BUCKET</span><span class="p">,</span> <span class="n">Prefix</span> <span class="o">=</span> <span class="n">f</span><span class="s">'{PREFIX}/transform'</span><span class="p">)</span>
<span class="c1"># Download the output data from S3 to local filesystem
</span><span class="n">s3_client</span><span class="o">.</span><span class="n">download_file</span><span class="p">(</span>
    <span class="n">Bucket</span><span class="o">=</span><span class="n">BUCKET</span><span class="p">,</span>
    <span class="n">Key</span><span class="o">=</span><span class="n">f</span><span class="s">"{PREFIX}/transform/test-batch.csv.out"</span><span class="p">,</span>
    <span class="n">Filename</span><span class="o">=</span><span class="n">f</span><span class="s">'{LOCAL_DATA_DIRECTORY}/test-batch.csv.out'</span><span class="p">)</span>

<span class="c1"># Show a few lines of this file
</span><span class="err">!</span><span class="n">head</span> <span class="o">-</span><span class="mi">5</span> <span class="o">../</span><span class="n">data</span><span class="o">/</span><span class="n">churn</span><span class="o">/</span><span class="n">test</span><span class="o">-</span><span class="n">batch</span><span class="o">.</span><span class="n">csv</span><span class="o">.</span><span class="n">out</span>
<span class="s">"""
0.010853796266019344
0.005068291909992695
0.008791499771177769
0.16663919389247894
0.004287515766918659
"""</span>
</code></pre></div></div>

<p>The output file has one prediction per line corresponding to the input file.</p>

<p>Note that when we created the transformer object, we can find some new things in the SageMaker console. One thing is the model object. Find it in the console under Inference -&gt; Models. Inside, we can see the model name, the ARN role, creation time, container info, Docker image used, training job, and the location of the serialized model on S3.</p>

<p>In that GUI, we can add tag or delete the model under Actions, or Create batch tranform job, or Create endpoint, directly from the console.</p>

<p>We can also find the previous batch transform jobs under Inference. You can inspect the metadata and audit the entire process of these jobs if you click into them.</p>

<h3 id="52-online-inference">
<a class="anchor" href="#52-online-inference" aria-hidden="true"><span class="octicon octicon-link"></span></a>5.2 Online inference</h3>

<p>SageMaker abstracts away all the API code, network and routing logic and makes it extremely easy to deploy the model as an endpoint.</p>

<p>Here we demonstrate using a custom trained sklearn model. Instead of <code class="highlighter-rouge">sagemaker.model.Model</code>, we use its subclass <code class="highlighter-rouge">sagemaker.sklearn.SKLearnModel</code>. See the <code class="highlighter-rouge">sagemaker.sklearn.SKLearnModel</code> <a href="https://sagemaker.readthedocs.io/en/stable/sagemaker.sklearn.html#sagemaker.sklearn.model.SKLearnModel">API reference</a> for more details.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sagemaker</span> <span class="kn">import</span> <span class="n">sklearn</span>

<span class="n">model_data</span> <span class="o">=</span> <span class="s">'s3://.../output/model.tar.gz'</span>

<span class="n">sklearn_model</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">SKLearnModel</span><span class="p">(</span><span class="n">model_data</span><span class="o">=</span><span class="n">model_data</span><span class="p">,</span>
                                     <span class="n">role</span><span class="o">=</span><span class="n">role</span><span class="p">,</span>
                                     <span class="n">entry_point</span><span class="o">=</span><span class="s">'../scripts/sklearn/sklearn_rf.py'</span><span class="p">)</span>
</code></pre></div></div>

<p>Again, <code class="highlighter-rouge">model_data</code> is the serialized model in S3 which can be found in the Training job section from the SageMaker console. We also need the same custom script that we used to train the model as <code class="highlighter-rouge">entry_point</code>. It contains the code to deserialize the model.</p>

<p>See the <code class="highlighter-rouge">sagemaker.predictor.RealTimePredictor</code> <a href="https://sagemaker.readthedocs.io/en/stable/predictors.html">API reference</a> for more details.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sklearn_predictor</span> <span class="o">=</span> <span class="n">sklearn_model</span><span class="o">.</span><span class="n">deploy</span><span class="p">(</span><span class="n">initial_instance_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                         <span class="n">instance_type</span><span class="o">=</span><span class="s">"ml.m4.xlarge"</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>NOTE: This takes about 6-8 minutes to return. And an endpoint will be ready.</strong></p>

<p>Now we can click on the Endpoint List in the sidebar and refresh to see the new endpoint. Right click on it and click <code class="highlighter-rouge">Describe Endpoint</code>, a new tab is opened. There are three sub tabs:</p>

<ul>
  <li>Monitoring results</li>
  <li>Monitoring job history</li>
  <li>AWS settings</li>
</ul>

<p>Click on AWS settings, we get the following view.</p>

<p><img src="/images/prodml/endpoint-aws-settings.png" alt="SageMaker endpoint" align="middle"></p>

<p>Under <code class="highlighter-rouge">Production variants</code>, there is a table for different models deployed. We only have one model now – our sklearn random forest, so it gets all the traffic. <strong>If we have multiple models, this feature here is good for running A/B testing, or different deployment strategies such as canary deployment, or blue/green deployment.</strong></p>

<p>Now we can test the endpoint within the notebook as if we are just calling an sklearn object locally. The code below actually makes an HTTP request and gets the response.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">f</span><span class="s">'{LOCAL_DATA_DIRECTORY}/test-dataset.csv'</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>

<span class="c1"># Remove first column which contains labels
</span><span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>

<span class="n">preds</span> <span class="o">=</span> <span class="n">sklearn_predictor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>This endpoint can handle multiple records at the same time, no need to send a single row each time!</strong></p>

<p>Here are two interesting articles for further information on using the endpoint.</p>

<ul>
  <li><a href="https://aws.amazon.com/blogs/machine-learning/creating-a-machine-learning-powered-rest-api-with-amazon-api-gateway-mapping-templates-and-amazon-sagemaker/">How to use API Gateway to create a PUBLIC RESTful endpoint for an Amazon SageMaker inference</a></li>
  <li><a href="https://aws.amazon.com/blogs/machine-learning/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda/">Calling a SageMaker API Using Serverless Technology</a></li>
</ul>

<h3 id="53-autoscaling-according-to-demand">
<a class="anchor" href="#53-autoscaling-according-to-demand" aria-hidden="true"><span class="octicon octicon-link"></span></a>5.3 Autoscaling according to demand</h3>

<p>For the batch inference case, SageMaker spins up an instance, perform the computation and shuts it down. For online inference, it spins up an instance an keeps it alive. SageMaker supports autoscaling the number of instances according to your workload.</p>

<p><strong>To configure autoscaling for a model using the console</strong>:</p>

<ol>
  <li>Open the Amazon SageMaker console at https://console.aws.amazon.com/sagemaker/.</li>
  <li>In the navigation pane, choose <strong>Endpoints</strong>.</li>
  <li>Choose the endpoint that you want to configure.</li>
  <li>Under the <strong>Endpoint runtime settings</strong> heading, select the radio button corresponding to the model variant that you want to configure and click <strong>Configure autoscaling</strong>. The <strong>Configure variant automatic scaling</strong> page appears.</li>
  <li>The <strong>Variant automatic scaling</strong> section lets us configure the min/max number of instances.
    <ul>
      <li>For <strong>Minimum instance count</strong>, type the minimum number of instances that you want the scaling policy to maintain. At least 1 instance is required.</li>
      <li>For <strong>Maximum instance count</strong>, type the maximum number of instances that you want the scaling policy to maintain.</li>
    </ul>
  </li>
  <li>The <strong>Built-in scaling policy</strong> section lets us configure the conditions under which to scale the instances.
    <ul>
      <li>For the <strong>Target value</strong>, type the average number of invocations per instance per minute for the model. To determine this value, follow the guidelines in Load testing. Application Auto Scaling adds or removes instances to keep the metric close to the value that you specify.</li>
      <li>For <strong>Scale-in cool down</strong> and <strong>Scale-out cool down</strong> type the number seconds for each cool down period. Assuming that the order in the list is based on either most important to less important of first applied to last applied.</li>
      <li>Select <strong>Disable scale in</strong> to prevent the scaling policy from deleting variant instances if you want to ensure that your variant scales out to address increased traffic, but are not concerned with removing instances to reduce costs when traffic decreases. Scale-out activities are always enabled so that the scaling policy can create endpoint instances as needed.</li>
    </ul>
  </li>
  <li>Choose <strong>Save</strong>.</li>
</ol>

<p>More details, including how to define a custom scaling policy, can be found in the <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-add-console.html">Developer Guide</a>.</p>

<p><strong>There is no silver bullet for defining the auto scaling policy for your use case. Just come up with an estimate and iteratively find what works for you.</strong></p>

<h3 id="54-delete-the-endpoint">
<a class="anchor" href="#54-delete-the-endpoint" aria-hidden="true"><span class="octicon octicon-link"></span></a>5.4 Delete the endpoint</h3>

<p>Note that you will be charged by the resources you created in this section: models and the endpoint. They are under Inference in the SageMaker console. You can either delete them in the GUI, or run the code below in the notebook:</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">churn_model</span><span class="o">.</span><span class="n">delete_model</span><span class="p">()</span>

<span class="n">sklearn_predictor</span><span class="o">.</span><span class="n">delete_endpoint</span><span class="p">()</span>
<span class="n">sklearn_model</span><span class="o">.</span><span class="n">delete_model</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="6-model-monitoring">
<a class="anchor" href="#6-model-monitoring" aria-hidden="true"><span class="octicon octicon-link"></span></a>6. Model Monitoring</h2>

<p><strong>Note: monitoring is only available after the live endpoint is up for at least 1 hour.</strong></p>

<p>A machine learning model may perform well at the beginning but start to degrade over time. One big cause is data <strong>drift</strong>: the input data distribution changed over time. In SageMaker, we can setup monitoring for endpoints to log and store live data. With the capability, we can compare live data with historical data to identify drift or data quality issues.</p>

<h3 id="61-enable-live-data-capture-for-a-model-endpoint">
<a class="anchor" href="#61-enable-live-data-capture-for-a-model-endpoint" aria-hidden="true"><span class="octicon octicon-link"></span></a>6.1 Enable live data capture for a model endpoint</h3>

<p>First of all, get the prerequisites setup just like the previous section.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">cd</span> <span class="o">/</span><span class="n">root</span><span class="o">/</span><span class="n">sagemaker</span><span class="o">-</span><span class="n">course</span><span class="o">/</span><span class="n">notebooks</span><span class="o">/</span>

<span class="kn">import</span> <span class="nn">time</span>

<span class="kn">import</span> <span class="nn">boto3</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">sagemaker</span>
<span class="kn">from</span> <span class="nn">sagemaker</span> <span class="kn">import</span> <span class="n">get_execution_role</span><span class="p">,</span> <span class="n">model</span>

<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s">'display.max_colwidth'</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s">'display.max_columns'</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s">'display.max_rows'</span><span class="p">,</span> <span class="mi">70</span><span class="p">)</span>

<span class="c1"># S3 bucket information
</span><span class="n">BUCKET</span> <span class="o">=</span> <span class="s">'&lt;bucketname&gt;'</span>
<span class="n">PREFIX</span> <span class="o">=</span> <span class="s">'churn'</span>
<span class="n">LOCAL_DATA_DIRECTORY</span> <span class="o">=</span> <span class="n">f</span><span class="s">'../data/{PREFIX}'</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">"Artifacts will be written to s3://{BUCKET}/{PREFIX}"</span><span class="p">)</span>

<span class="c1"># Session variables we'll use throughout the notebook
</span><span class="n">sagemaker_session</span> <span class="o">=</span> <span class="n">sagemaker</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
<span class="n">boto_session</span> <span class="o">=</span> <span class="n">sagemaker_session</span><span class="o">.</span><span class="n">boto_session</span>
<span class="n">sagemaker_client</span> <span class="o">=</span> <span class="n">boto_session</span><span class="o">.</span><span class="n">client</span><span class="p">(</span><span class="s">'sagemaker'</span><span class="p">)</span>
<span class="n">role</span> <span class="o">=</span> <span class="n">get_execution_role</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'Role ARN: {role}'</span><span class="p">)</span>
</code></pre></div></div>

<p>Before we can setup the monitoring, we need a live endpoint. The previous section showed that, but let’s do it again for the XGBoost model.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_data</span> <span class="o">=</span> <span class="s">'s3://&lt;bucket&gt;/churn/.../output/model.tar.gz'</span>
<span class="n">image</span> <span class="o">=</span> <span class="s">'...'</span>

<span class="n">churn_model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">model_data</span><span class="o">=</span><span class="n">model_data</span><span class="p">,</span>
                          <span class="n">image</span><span class="o">=</span><span class="n">image</span><span class="p">,</span>
                          <span class="n">role</span><span class="o">=</span><span class="n">role</span><span class="p">,</span>
                          <span class="n">sagemaker_session</span><span class="o">=</span><span class="n">sagemaker_session</span><span class="p">)</span>
</code></pre></div></div>

<p>Again, <code class="highlighter-rouge">model_data</code> and <code class="highlighter-rouge">image</code> can be found in SageMaker console -&gt; Training jobs -&gt; the xgboost model we trained.</p>

<p>To setup the monitoring, we need to configure a <code class="highlighter-rouge">sagemaker.model_monitor.DataCaptureConfig</code> object. Set <code class="highlighter-rouge">enable_capture</code> to true, and the percentage of the data to be capured to 100%, then set the s3 path for the captured data where it should be stored.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sagemaker</span> <span class="kn">import</span> <span class="n">model_monitor</span><span class="p">,</span> <span class="n">predictor</span>

<span class="n">captured_data_s3_uri</span> <span class="o">=</span> <span class="n">f</span><span class="s">'s3://{BUCKET}/{PREFIX}/model-monitor/data-capture'</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'Captured data stored in bucket: {captured_data_s3_uri}'</span><span class="p">)</span>

<span class="n">data_capture_config</span> <span class="o">=</span> <span class="n">model_monitor</span><span class="o">.</span><span class="n">DataCaptureConfig</span><span class="p">(</span>
    <span class="n">enable_capture</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">sampling_percentage</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">destination_s3_uri</span><span class="o">=</span><span class="n">captured_data_s3_uri</span><span class="p">,</span>
    <span class="n">sagemaker_session</span><span class="o">=</span><span class="n">sagemaker_session</span><span class="p">)</span>

<span class="c1"># Deploy the endpoint if you don't have one yet. Takes a while
# Note that data_capture_config is passed in
</span><span class="n">churn_model</span><span class="o">.</span><span class="n">deploy</span><span class="p">(</span><span class="n">initial_instance_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                   <span class="n">instance_type</span><span class="o">=</span><span class="s">'ml.t2.medium'</span><span class="p">,</span>
                   <span class="n">data_capture_config</span><span class="o">=</span><span class="n">data_capture_config</span><span class="p">)</span>
</code></pre></div></div>

<p>Notice that <code class="highlighter-rouge">data_capture_config</code> is passed into the model deployment for monitoring to work.</p>

<p>To call this endpoint in code, we need <code class="highlighter-rouge">predictor.RealTimePredictor</code>. It encapsulates the HTTP request.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">churn_predictor</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">RealTimePredictor</span><span class="p">(</span>
    <span class="n">endpoint</span><span class="o">=</span><span class="n">churn_model</span><span class="o">.</span><span class="n">endpoint_name</span><span class="p">,</span>
    <span class="n">sagemaker_session</span><span class="o">=</span><span class="n">sagemaker_session</span><span class="p">,</span>
    <span class="n">serializer</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">csv_serializer</span><span class="p">,</span>
    <span class="n">content_type</span><span class="o">=</span><span class="s">'text/csv'</span><span class="p">)</span>
</code></pre></div></div>

<p>The part <code class="highlighter-rouge">serializer = predictor.csv_serializer</code> means that the predictor serializes the csv data into bytes BEFORE sending it over the wire to the endpoint via an HTTP request.</p>

<p>Here is the example to fake some live data using a csv file.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">"Sending test traffic to the endpoint {}. </span><span class="se">\n</span><span class="s">Please wait for a minute..."</span>
        <span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">churn_model</span><span class="o">.</span><span class="n">endpoint_name</span><span class="p">))</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">f</span><span class="s">'{LOCAL_DATA_DIRECTORY}/test_sample.csv'</span><span class="p">,</span> <span class="s">'r'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">payload</span> <span class="o">=</span> <span class="n">row</span><span class="o">.</span><span class="n">rstrip</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">churn_predictor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">payload</span><span class="p">)</span>
        <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
</code></pre></div></div>

<p>We can check if the data is capture in S3 by checking the output location. The format of the Amazon S3 output path is:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>s3://{destination-bucket-prefix}/{endpoint-name}/{variant-name}/yyyy/mm/dd/hh/filename.jsonl
</code></pre></div></div>

<p>We can inpsect this file by downloading it and print out its content:</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">json</span>
<span class="kn">from</span> <span class="nn">sagemaker.s3</span> <span class="kn">import</span> <span class="n">S3Uploader</span><span class="p">,</span> <span class="n">S3Downloader</span>

<span class="n">current_endpoint_capture_prefix</span> <span class="o">=</span> <span class="n">f</span><span class="s">'{PREFIX}/model-monitor/data-capture/{churn_model.endpoint_name}'</span>
<span class="n">capture_files</span> <span class="o">=</span> <span class="n">S3Downloader</span><span class="o">.</span><span class="nb">list</span><span class="p">(</span><span class="n">f</span><span class="s">"s3://{BUCKET}/{current_endpoint_capture_prefix}"</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Found Data Capture Files:"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">capture_files</span><span class="p">)</span>

<span class="n">capture_file</span> <span class="o">=</span> <span class="n">S3Downloader</span><span class="o">.</span><span class="n">read_file</span><span class="p">(</span><span class="n">capture_files</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="k">print</span><span class="p">(</span><span class="s">"=====Single Data Capture===="</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">capture_file</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">)[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)[:</span><span class="mi">2000</span><span class="p">])</span>

<span class="s">"""
=====Single Data Capture====
{
  "captureData": {
    "endpointInput": {
      "observedContentType": "text/csv",
      "mode": "INPUT",
      "data": "186,0.1,137.8,97,187.7,118,146.4,85,8.7,6,1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.10,0.11,0.12,0.13,0.14,0.15,0.16,0.17,1.1,0.18,0.19,0.20,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.30,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.40,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.50,0.51,0.52,0.53,1.2,1.3,0.54,1.4,0.55",
      "encoding": "CSV"
    },
    "endpointOutput": {
      "observedContentType": "text/csv; charset=utf-8",
      "mode": "OUTPUT",
      "data": "0.014719205908477306",
      "encoding": "CSV"
    }
  },
  "eventMetadata": {
    "eventId": "f38dfc73-c631-4396-bffc-3313a7157be7",
    "inferenceTime": "2020-05-20T14:08:54Z"
  },
  "eventVersion": "0"
}
"""</span>
</code></pre></div></div>

<p>In the printed file content we see that our input data is under <code class="highlighter-rouge">captureData:endpointInput:data</code>, and the endpoint gives back the prediction in <code class="highlighter-rouge">endpointOutput:data</code>.</p>

<p><strong>By simply passing in a <code class="highlighter-rouge">model_monitor.DataCaptureConfig</code>, we have enabled model montoring!</strong> This way we can have all the live data and their predictions nicely logged in S3!</p>

<h3 id="62-generating-constraints-and-suggestions-from-a-baseline-dataset">
<a class="anchor" href="#62-generating-constraints-and-suggestions-from-a-baseline-dataset" aria-hidden="true"><span class="octicon octicon-link"></span></a>6.2 Generating constraints and suggestions from a baseline dataset</h3>

<p>We use our training data as baseline for comparison. First, upload the training data to S3.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">baseline_prefix</span> <span class="o">=</span> <span class="n">f</span><span class="s">'{PREFIX}/baselining'</span>
<span class="n">baseline_data_prefix</span> <span class="o">=</span> <span class="n">f</span><span class="s">'{baseline_prefix}/data'</span>
<span class="n">baseline_results_prefix</span> <span class="o">=</span> <span class="n">f</span><span class="s">'{baseline_prefix}/results'</span>

<span class="c1"># Store the training data
</span><span class="n">baseline_data_uri</span> <span class="o">=</span> <span class="n">f</span><span class="s">'s3://{BUCKET}/{baseline_data_prefix}'</span>
<span class="c1"># Store the results of our baseline job
</span><span class="n">baseline_results_uri</span> <span class="o">=</span> <span class="n">f</span><span class="s">'s3://{BUCKET}/{baseline_results_prefix}'</span>

<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'Baseline data URI: {baseline_data_uri}'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'Baseline results URI: {baseline_results_uri}'</span><span class="p">)</span>

<span class="n">baseline_data_path</span> <span class="o">=</span> <span class="n">S3Uploader</span><span class="o">.</span><span class="n">upload</span><span class="p">(</span>
    <span class="n">f</span><span class="s">"{LOCAL_DATA_DIRECTORY}/training-dataset-with-header.csv"</span><span class="p">,</span>
    <span class="n">baseline_data_uri</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="highlighter-rouge">model_monitor.DefaultModelMonitor</code> is a plug-and-play tool that only needs the dataset and some pre and post processing scripts. There’s a more advanced and customized option that is <code class="highlighter-rouge">sagemaker.model_monitor.ModelMonitor</code> if you need more control.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">my_default_monitor</span> <span class="o">=</span> <span class="n">model_monitor</span><span class="o">.</span><span class="n">DefaultModelMonitor</span><span class="p">(</span>
    <span class="n">role</span><span class="o">=</span><span class="n">role</span><span class="p">,</span>
    <span class="n">instance_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">instance_type</span><span class="o">=</span><span class="s">'ml.m5.xlarge'</span><span class="p">,</span>
    <span class="n">volume_size_in_gb</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
    <span class="n">max_runtime_in_seconds</span><span class="o">=</span><span class="mi">3600</span><span class="p">)</span>

<span class="n">my_default_monitor</span><span class="o">.</span><span class="n">suggest_baseline</span><span class="p">(</span>
    <span class="n">baseline_dataset</span><span class="o">=</span><span class="n">baseline_data_path</span><span class="p">,</span>
    <span class="n">dataset_format</span><span class="o">=</span><span class="n">model_monitor</span><span class="o">.</span><span class="n">DatasetFormat</span><span class="o">.</span><span class="n">csv</span><span class="p">(</span><span class="n">header</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
    <span class="n">output_s3_uri</span><span class="o">=</span><span class="n">baseline_results_uri</span><span class="p">,</span>
    <span class="n">wait</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>Set <code class="highlighter-rouge">wait=True</code> to make it a blocking call. The baseline dataset must have the exact same schema as the live data, or the comparison will fail.</p>

<p>The log printed out after this cell is quite long because this is actually a Spark job SageMaker runs. Skip through all the Hadoop/Yarn stuff we focus on <code class="highlighter-rouge">Constraints</code> which is the thing we want from the monitor.</p>

<p><img src="/images/prodml/sm-constraints.png" alt="SageMaker monitor constraints" align="middle"></p>

<p>Then there is the <code class="highlighter-rouge">stats</code> part in the log. It has some summary statistics of each feature.</p>

<p>This particular processing job takes about 6-8 minutes to run.</p>

<p>The log is very long and hard to read. To better read it we can load the json files into pandas dataframes.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">S3Downloader</span><span class="o">.</span><span class="nb">list</span><span class="p">(</span><span class="n">f</span><span class="s">'s3://{BUCKET}/{baseline_results_prefix}'</span><span class="p">)</span>
<span class="s">"""
['s3://&lt;bucketname&gt;/churn/baselining/results/constraints.json',
 's3://&lt;bucketname&gt;/churn/baselining/results/statistics.json']
"""</span>

<span class="n">constraints_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">json_normalize</span><span class="p">(</span>
    <span class="n">my_default_monitor</span><span class="o">.</span><span class="n">suggested_constraints</span><span class="p">()</span><span class="o">.</span><span class="n">body_dict</span><span class="p">[</span><span class="s">"features"</span><span class="p">])</span>
<span class="n">constraints_df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="s">"""
name	inferred_type	completeness	num_constraints.is_non_negative
0	Churn	Integral	1.0	True
1	Account Length	Integral	1.0	True
2	VMail Message	Integral	1.0	True
3	Day Mins	Fractional	1.0	True
4	Day Calls	Integral	1.0	True
"""</span>
</code></pre></div></div>

<p>The <code class="highlighter-rouge">statistics.json</code> file contains statistical information about the data in the baseline. We can view these constraints by calling the <code class="highlighter-rouge">baseline_statistics</code> method.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">baseline_job</span> <span class="o">=</span> <span class="n">my_default_monitor</span><span class="o">.</span><span class="n">latest_baselining_job</span>
<span class="n">schema_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">json_normalize</span><span class="p">(</span><span class="n">my_default_monitor</span><span class="o">.</span><span class="n">baseline_statistics</span><span class="p">()</span><span class="o">.</span><span class="n">body_dict</span><span class="p">[</span><span class="s">"features"</span><span class="p">])</span>
<span class="n">schema_df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>

<span class="s">"""
name	inferred_type	numerical_statistics.common.num_present	numerical_statistics.common.num_missing	numerical_statistics.mean	numerical_statistics.sum	numerical_statistics.std_dev	numerical_statistics.min	numerical_statistics.max	numerical_statistics.distribution.kll.buckets	numerical_statistics.distribution.kll.sketch.parameters.c	numerical_statistics.distribution.kll.sketch.parameters.k	numerical_statistics.distribution.kll.sketch.data
0	Churn	Integral	2333	0	0.139306	325.0	0.346265	0.0	1.0	[{'lower_bound': 0.0, 'upper_bound': 0.1, 'cou...	0.64	2048.0	[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0,...
"""</span>
</code></pre></div></div>

<p>The above is just to show what columns (statistics) are available.</p>

<p>The most important columns that will be used to detect drift are <code class="highlighter-rouge">numerical_statistics.distribution.kll.&lt;something&gt;</code>. It is a compressed way to describe the distribution.</p>

<h3 id="63-creating-a-monitoring-schedule">
<a class="anchor" href="#63-creating-a-monitoring-schedule" aria-hidden="true"><span class="octicon octicon-link"></span></a>6.3 Creating a monitoring schedule</h3>

<p>Now we have the statistics to monitor drift, we can setup a periodic schedule to run the check.</p>

<p>Note that it takes at least 1 hour to run the code in this section. It’s because the most frequent rate for the SageMaker monitoring job is <strong>hourly</strong>.</p>

<p>To create the monitoring schedule, we use <code class="highlighter-rouge">create_monitoring_schedule</code> method in the default monitoring object. We specify the endpoint, the desired S3 location for the report which captures the violations in live data given the constraints, then we pass in statistics and constraints in the default monitor object, the desired schedule name, and a cron expression that’s generated by SageMaker SDK, and CloudWatch metrics for visualization later.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">gmtime</span><span class="p">,</span> <span class="n">strftime</span>

<span class="n">reports_prefix</span> <span class="o">=</span> <span class="n">f</span><span class="s">'{PREFIX}/reports'</span>
<span class="n">s3_report_path</span> <span class="o">=</span> <span class="n">f</span><span class="s">'s3://{BUCKET}/{reports_prefix}'</span>

<span class="n">mon_schedule_name</span> <span class="o">=</span> <span class="s">'xgb-churn-model-schedule-'</span> <span class="o">+</span> <span class="n">strftime</span><span class="p">(</span><span class="s">"</span><span class="si">%</span><span class="s">Y-</span><span class="si">%</span><span class="s">m-</span><span class="si">%</span><span class="s">d-</span><span class="si">%</span><span class="s">H-</span><span class="si">%</span><span class="s">M-</span><span class="si">%</span><span class="s">S"</span><span class="p">,</span> <span class="n">gmtime</span><span class="p">())</span>

<span class="n">my_default_monitor</span><span class="o">.</span><span class="n">create_monitoring_schedule</span><span class="p">(</span>
    <span class="n">endpoint_input</span><span class="o">=</span><span class="n">churn_predictor</span><span class="o">.</span><span class="n">endpoint</span><span class="p">,</span>
    <span class="n">output_s3_uri</span><span class="o">=</span><span class="n">s3_report_path</span><span class="p">,</span>
    <span class="n">statistics</span><span class="o">=</span><span class="n">my_default_monitor</span><span class="o">.</span><span class="n">baseline_statistics</span><span class="p">(),</span>
    <span class="n">constraints</span><span class="o">=</span><span class="n">my_default_monitor</span><span class="o">.</span><span class="n">suggested_constraints</span><span class="p">(),</span>
    <span class="n">monitor_schedule_name</span><span class="o">=</span><span class="n">mon_schedule_name</span><span class="p">,</span>
    <span class="n">schedule_cron_expression</span><span class="o">=</span><span class="n">model_monitor</span><span class="o">.</span><span class="n">CronExpressionGenerator</span><span class="o">.</span><span class="n">hourly</span><span class="p">(),</span>
    <span class="n">enable_cloudwatch_metrics</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>We can use a background thread to mimic live data.</p>

<p><strong>If there’s no live data coming into the endpoint, the monitoring job will fail later on.</strong></p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">threading</span> <span class="kn">import</span> <span class="n">Thread</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">sleep</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="n">runtime_client</span> <span class="o">=</span> <span class="n">boto3</span><span class="o">.</span><span class="n">client</span><span class="p">(</span><span class="s">'runtime.sagemaker'</span><span class="p">)</span>

<span class="c1"># (just repeating code from above for convenience/ able to run this section independently)
</span><span class="k">def</span> <span class="nf">invoke_endpoint</span><span class="p">(</span><span class="n">ep_name</span><span class="p">,</span> <span class="n">file_name</span><span class="p">,</span> <span class="n">runtime_client</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file_name</span><span class="p">,</span> <span class="s">'r'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">payload</span> <span class="o">=</span> <span class="n">row</span><span class="o">.</span><span class="n">rstrip</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">runtime_client</span><span class="o">.</span><span class="n">invoke_endpoint</span><span class="p">(</span><span class="n">EndpointName</span><span class="o">=</span><span class="n">ep_name</span><span class="p">,</span>
                                                      <span class="n">ContentType</span><span class="o">=</span><span class="s">'text/csv'</span><span class="p">,</span>
                                                      <span class="n">Body</span><span class="o">=</span><span class="n">payload</span><span class="p">)</span>
            <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">invoke_endpoint_forever</span><span class="p">():</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">invoke_endpoint</span><span class="p">(</span><span class="n">churn_model</span><span class="o">.</span><span class="n">endpoint_name</span><span class="p">,</span> <span class="n">f</span><span class="s">'{LOCAL_DATA_DIRECTORY}/test-dataset-input-cols.csv'</span><span class="p">,</span> <span class="n">runtime_client</span><span class="p">)</span>

<span class="n">thread</span> <span class="o">=</span> <span class="n">Thread</span><span class="p">(</span><span class="n">target</span> <span class="o">=</span> <span class="n">invoke_endpoint_forever</span><span class="p">)</span>
<span class="n">thread</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
</code></pre></div></div>

<p>Note that you need to stop the kernel to stop the endpoint invocations by the thread.</p>

<p>Now we can wait for the first monitoring output. The first one will come in on the hour.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mon_executions</span> <span class="o">=</span> <span class="n">my_default_monitor</span><span class="o">.</span><span class="n">list_executions</span><span class="p">()</span>
<span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">mon_executions</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"We created a hourly schedule above and it will kick off executions ON the hour.</span><span class="se">\n</span><span class="s">"</span>
        <span class="s">"We will have to wait till we hit the hour...</span><span class="se">\n\n</span><span class="s">"</span><span class="p">)</span>

<span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">mon_executions</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Waiting for the 1st execution to happen..."</span><span class="p">)</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">60</span><span class="p">)</span>
    <span class="n">mon_executions</span> <span class="o">=</span> <span class="n">my_default_monitor</span><span class="o">.</span><span class="n">list_executions</span><span class="p">()</span>
</code></pre></div></div>

<p>If we go to the GUI and click on Endpoints in the left sidebar, right click on the endpoint,</p>

<h3 id="64-visualizing-drift-by-comparing-data-distributions">
<a class="anchor" href="#64-visualizing-drift-by-comparing-data-distributions" aria-hidden="true"><span class="octicon octicon-link"></span></a>6.4 Visualizing drift by comparing data distributions</h3>

<p>After the monitoring job run, we see it produces 3 files:</p>

<ul>
  <li>constraint_violations.json</li>
  <li>constraints.json</li>
  <li>statistics.json</li>
</ul>

<p>We can download them from S3, or put them into a dataframe like this:</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">violations</span> <span class="o">=</span> <span class="n">my_default_monitor</span><span class="o">.</span><span class="n">latest_monitoring_constraint_violations</span><span class="p">()</span>
<span class="n">constraints_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">json_normalize</span><span class="p">(</span><span class="n">violations</span><span class="o">.</span><span class="n">body_dict</span><span class="p">[</span><span class="s">"violations"</span><span class="p">])</span>
<span class="n">constraints_df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>

<span class="s">"""
feature_name	constraint_check_type	description
0	State_OR	data_type_check	Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0</span><span class="si">%</span><span class="s">. Observed: Only 99.30555555555556</span><span class="si">% </span><span class="s">of data is Integral.
1	State_WV	data_type_check	Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0</span><span class="si">%</span><span class="s">. Observed: Only 99.30555555555556</span><span class="si">% </span><span class="s">of data is Integral.
2	State_UT	data_type_check	Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0</span><span class="si">%</span><span class="s">. Observed: Only 99.30555555555556</span><span class="si">% </span><span class="s">of data is Integral.
3	State_SC	data_type_check	Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0</span><span class="si">%</span><span class="s">. Observed: Only 99.30555555555556</span><span class="si">% </span><span class="s">of data is Integral.
...
"""</span>

<span class="n">latest_monitoring_violations</span> <span class="o">=</span> <span class="n">my_default_monitor</span><span class="o">.</span><span class="n">latest_monitoring_constraint_violations</span><span class="p">()</span>
<span class="n">latest_monitoring_statistics</span> <span class="o">=</span> <span class="n">my_default_monitor</span><span class="o">.</span><span class="n">latest_monitoring_statistics</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>Next, we get Execution and Baseline details from Processing Job Arn.</strong></p>

<p>Enter the ProcessingJob arn for an execution of a MonitoringSchedule below to get the result files associated with that execution</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">processing_job_arn</span> <span class="o">=</span> <span class="n">latest_execution</span><span class="o">.</span><span class="n">describe</span><span class="p">()[</span><span class="s">'ProcessingJobArn'</span><span class="p">]</span>

<span class="kn">import</span> <span class="nn">os</span>

<span class="kn">from</span> <span class="nn">sagemaker.model_monitor</span> <span class="kn">import</span> <span class="n">MonitoringExecution</span>

<span class="n">execution</span> <span class="o">=</span> <span class="n">MonitoringExecution</span><span class="o">.</span><span class="n">from_processing_arn</span><span class="p">(</span>
    <span class="n">sagemaker_session</span><span class="o">=</span><span class="n">sagemaker_session</span><span class="p">,</span>
    <span class="n">processing_job_arn</span><span class="o">=</span><span class="n">processing_job_arn</span><span class="p">)</span>

<span class="n">exec_inputs</span> <span class="o">=</span> <span class="p">{</span><span class="n">inp</span><span class="p">[</span><span class="s">'InputName'</span><span class="p">]:</span> <span class="n">inp</span> <span class="k">for</span> <span class="n">inp</span> <span class="ow">in</span> <span class="n">execution</span><span class="o">.</span><span class="n">describe</span><span class="p">()[</span><span class="s">'ProcessingInputs'</span><span class="p">]}</span>
<span class="n">exec_results</span> <span class="o">=</span> <span class="n">execution</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">destination</span>

<span class="n">baseline_statistics_filepath</span> <span class="o">=</span> <span class="n">exec_inputs</span><span class="p">[</span><span class="s">'baseline'</span><span class="p">][</span><span class="s">'S3Input'</span><span class="p">][</span><span class="s">'S3Uri'</span><span class="p">]</span> <span class="k">if</span> <span class="s">'baseline'</span> <span class="ow">in</span> <span class="n">exec_inputs</span> <span class="k">else</span> <span class="bp">None</span>
<span class="n">execution_statistics_filepath</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">exec_results</span><span class="p">,</span> <span class="s">'statistics.json'</span><span class="p">)</span>
<span class="n">violations_filepath</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">exec_results</span><span class="p">,</span> <span class="s">'constraint_violations.json'</span><span class="p">)</span>

<span class="n">baseline_statistics</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">S3Downloader</span><span class="o">.</span><span class="n">read_file</span><span class="p">(</span><span class="n">baseline_statistics_filepath</span><span class="p">))</span> <span class="k">if</span> <span class="n">baseline_statistics_filepath</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="bp">None</span>
<span class="n">execution_statistics</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">S3Downloader</span><span class="o">.</span><span class="n">read_file</span><span class="p">(</span><span class="n">execution_statistics_filepath</span><span class="p">))</span>
<span class="n">violations</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">S3Downloader</span><span class="o">.</span><span class="n">read_file</span><span class="p">(</span><span class="n">violations_filepath</span><span class="p">))[</span><span class="s">'violations'</span><span class="p">]</span>
</code></pre></div></div>

<p>We need to download a utility file from SageMaker to plot the distributions:</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">raw</span><span class="o">.</span><span class="n">githubusercontent</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">awslabs</span><span class="o">/</span><span class="n">amazon</span><span class="o">-</span><span class="n">sagemaker</span><span class="o">-</span><span class="n">examples</span><span class="o">/</span><span class="n">master</span><span class="o">/</span><span class="n">sagemaker_model_monitor</span><span class="o">/</span><span class="n">visualization</span><span class="o">/</span><span class="n">utils</span><span class="o">.</span><span class="n">py</span>

<span class="kn">import</span> <span class="nn">utils</span> <span class="k">as</span> <span class="n">mu</span>

<span class="n">mu</span><span class="o">.</span><span class="n">show_violation_df</span><span class="p">(</span><span class="n">baseline_statistics</span><span class="o">=</span><span class="n">baseline_statistics</span><span class="p">,</span>
                     <span class="n">latest_statistics</span><span class="o">=</span><span class="n">execution_statistics</span><span class="p">,</span>
                     <span class="n">violations</span><span class="o">=</span><span class="n">violations</span><span class="p">)</span>
</code></pre></div></div>

<p>SageMaker infers the data types from the baseline dataset we provided. It knows what features are integral and which are fractional.</p>

<p>We can also look at the summary statistics of each feature:</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">features</span> <span class="o">=</span> <span class="n">mu</span><span class="o">.</span><span class="n">get_features</span><span class="p">(</span><span class="n">execution_statistics</span><span class="p">)</span>
<span class="n">feature_baselines</span> <span class="o">=</span> <span class="n">mu</span><span class="o">.</span><span class="n">get_features</span><span class="p">(</span><span class="n">baseline_statistics</span><span class="p">)</span>
<span class="n">mu</span><span class="o">.</span><span class="n">show_distributions</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>

<span class="s">"""
	num_present	num_missing	mean	sum	std_dev	min	max
Churn	288	0	0.121015	34.85218	0.246669	0.002989	0.993593
Account Length	288	0	99.760417	28731.00000	40.589513	10.000000	232.000000
VMail Message	288	0	6.702083	1930.20000	12.276850	0.000000	43.000000
Day Mins	288	0	178.247222	51335.20000	54.363873	46.500000	328.100000
...

&lt;IT ALSO PLOTS THE DISTRIBUTION FOR EACH FEATURE&gt;
"""</span>
</code></pre></div></div>

<p>A very handy feature is to plot the live data distribution against baseline:</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mu</span><span class="o">.</span><span class="n">show_distributions</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">feature_baselines</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/images/prodml/live-vs-baseline.png" alt="Live vs Baseline" align="middle"></p>

<p>We can quickly see some of the baseline data are integers and some from the the live data are fractions. Overlaying them together gives us a great view to look for drift.</p>

<p>Note that there are other useful information and visualization available in</p>

<ul>
  <li>“Describe endpoint” in sidebar’s endpoint tab</li>
  <li>SageMaker console -&gt; Inference -&gt; Endpoints</li>
</ul>

<p>Check them out as well!</p>

<h3 id="65-delete-resources">
<a class="anchor" href="#65-delete-resources" aria-hidden="true"><span class="octicon octicon-link"></span></a>6.5 Delete resources</h3>

<p>If we don’t delete the resources created, we will get charged. We now have 3 kinds of resources:</p>

<ul>
  <li>endpoint</li>
  <li>model</li>
  <li>monitoring schedule job</li>
</ul>

<p>To delete all of them:</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sagemaker_session</span><span class="o">.</span><span class="n">delete_monitoring_schedule</span><span class="p">(</span><span class="n">mon_schedule_name</span><span class="p">)</span>
<span class="n">sagemaker_session</span><span class="o">.</span><span class="n">delete_endpoint</span><span class="p">(</span><span class="n">churn_model</span><span class="o">.</span><span class="n">endpoint_name</span><span class="p">)</span>
<span class="n">sagemaker_session</span><span class="o">.</span><span class="n">delete_model</span><span class="p">(</span><span class="n">churn_model</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="other-sagemaker-features">
<a class="anchor" href="#other-sagemaker-features" aria-hidden="true"><span class="octicon octicon-link"></span></a>Other SageMaker features</h2>

<ul>
  <li>SageMaker Ground Truth: build and manage training datasets, like Figure 8</li>
  <li>SageMaker Autopilot: build foolproof models for us</li>
  <li>SageMaker Neo: deploy models on different hardware</li>
  <li>SageMaker Marketplace: buy and sell model packages</li>
</ul>

<h2 id="reference">
<a class="anchor" href="#reference" aria-hidden="true"><span class="octicon octicon-link"></span></a>Reference</h2>

<ul>
  <li>Luigi’s SageMaker course</li>
</ul>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="logancyang/blog-learning-automata"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/note/sagemaker/prodml/mlops/2020/08/06/prod-ml-ii.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Notes, code and essays by Logan Yang.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/logancyang" title="logancyang"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/logancyang" title="logancyang"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
