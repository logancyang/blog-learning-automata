---
toc: true
layout: post
description: ProdML SageMaker note series
categories: [note, sagemaker, prodml, mlops]
title: "[ProdML] Amazon SageMaker Part II: Experiments, Deployment, and Monitoring"
comments: true
---

Previous post: [part I](https://blog.logancyang.com/note/sagemaker/prodml/mlops/2020/08/03/prod-ml.html)

## 4. Experiments

In the previous section we went through the training process. In reality, the model needs a series of experiments to find the best set of hyperparameters. It is very important to properly keep track of the experiment results in a reproducible manner, and setup unified practice among team members. SageMaker Experiments solves this problem.

SageMaker Experiments has its own Python package you can import with:

```py
from smexperiments.experiment import Experiment
```

An `experiment` should be the outer most group for a collections of `trial`s. Each trial can map to a specific algorithm you use, such as a Random Forest, XGBoost, Logistic Regression, etc.

```py
from smexperiments.trial import Trial
```

A trial is a series of steps, each step is called a `trial_component`.

```py
from smexperiments.trial_component import TrialComponent
```

Each trial component can have a combination of inputs such as datasets, algorithm and parameters. You can produce desired outputs such as models, datasets, metrics and checkpoints.

Examples of trial components are

- data preprocessing jobs
- training jobs
- batch transform jobs

A trial component can be, e.g. the full preprocessing logic, or each step within it such as imputing missing values, one-hot encoding, etc. Each trial component (a trial step) can store information such as mean and standard deviation or the metric you care about during the experiments so you can compare them.

Next let's look at examples for training MNIST classifiers using TensorFlow and PyTorch.

### 4.1 An Experiment Example using TensorFlow 2

We use a custom training script provided by TensorFlow for this use case. Check out the script [here](https://github.com/lpatruno/sagemaker-course/blob/master/scripts/tf/tensorflow_mnist.py).

Note that in `_parse_args()`, `model_dir` is passed from SageMaker and the others are from SageMaker environment variables.

```py
def _parse_args():
    parser = argparse.ArgumentParser()

    # Data, model, and output directories
    # model_dir is always passed in from SageMaker. By default this is a S3 path under the default bucket.
    parser.add_argument('--model_dir', type=str)
    parser.add_argument('--sm-model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))
    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAINING'))
    parser.add_argument('--hosts', type=list, default=json.loads(os.environ.get('SM_HOSTS')))
    parser.add_argument('--current-host', type=str, default=os.environ.get('SM_CURRENT_HOST'))

    return parser.parse_known_args()
```

Here is the main block which is very simple:

```py
if __name__ == "__main__":
    args, unknown = _parse_args()

    train_data, train_labels = _load_training_data(args.train)
    eval_data, eval_labels = _load_testing_data(args.train)

    mnist_classifier = model(train_data, train_labels, eval_data, eval_labels)

    if args.current_host == args.hosts[0]:
        # save model to an S3 directory with version number '00000001'
        mnist_classifier.save(os.path.join(args.sm_model_dir, '000000001'), 'my_model.h5')
```

Note: the sample data from S3 is in `.npy` format which is much more performant than csv format.

```py
# Create the Experiment
experiment_name = f"tf-mnist-{datetime.datetime.now().strftime('%Y%m%d%H%M')}"
description = "Classification of mnist hand-written digits using tensorflow 2"

tf_experiment = Experiment.create(experiment_name=experiment_name,
                                  description=description,
                                  sagemaker_boto_client=sagemaker_client)

# Create the Trial
trial_name = f"tf-trial-{datetime.datetime.now().strftime('%Y%m%d%H%M')}"

tf_trial = Trial.create(trial_name=trial_name,
                        experiment_name=tf_experiment.experiment_name,
                        sagemaker_boto_client=sagemaker_client)


from sagemaker.tensorflow import TensorFlow

# Create the training job. This takes 3-5 min
tf_estimator = TensorFlow(
    entry_point='../scripts/tf/tensorflow_mnist.py',
    role=role,
    train_instance_count=1,
    train_instance_type='ml.p2.xlarge',
    code_location=f"s3://{BUCKET}/{PREFIX}",
    output_path=f"s3://{BUCKET}/{PREFIX}",
    base_job_name='tf-mnist',
    py_version='py3',
    framework_version='2.1.0',
    enable_sagemaker_metrics=True)

experiment_config = {
        "TrialName": tf_trial.trial_name,
        "TrialComponentDisplayName": "Training",
    }

# Now associate the estimator with the Experiment and Trial
tf_estimator.fit(inputs={'training': training_data_uri},
                 experiment_config=experiment_config)
```

Notice that the `TensorFlow` constructor takes in some additional parameters compared to the previous training jobs: `py_version`, `framework_version`, and `enable_sagemaker_metrics`.

The SageMaker TensorFlow object has legacy mode and script mode, `py_version='py3'` indicates that we are using script mode. Legacy mode only supports Python 2. `framework_version` is the actual TensorFlow version we want to use.

`enable_sagemaker_metrics=True` enables logging of metrics such as accuracy over time, so that we can see the visualization outside the notebook in SageMaker Studio.

After executing the code above, you can view your experiment metadata by clicking on the beaker on the sidebar! It shows a two-column table: experiment name and last modified time. First, click on the refresh button in this panel, then you'll see the experiment you just ran. You can also search by experiment name in its search bar.

Double click on the experiment, it shows the trials under it. Double click on the trial, it then shows the trial components. It looks like this:

<img src="{{ site.baseurl }}/images/prodml/trial-component.png" alt="SageMaker trial component" align="middle"/>

The **Metrics** tab shows the metrics it automatically scraped from the log of our training job. It's configured by the TensorFlow estimator object and the Docker image used. You can explore other tabs for more metadata.

### 4.2 An Experiment Example using PyTorch

Now let's switch to PyTorch and demonstrate how to run multiple trials with different hyperparameters.

```py
from matplotlib import pyplot as plt
import pandas as pd
from torchvision import datasets, transforms

%config InlineBackend.figure_format = 'retina'

transform = transforms.Compose([transforms.ToTensor(),
                                transforms.Normalize((0.1307,), (0.3081,))])

# Download, load, and transform the data.
train_set = datasets.MNIST(LOCAL_DATA_DIRECTORY, train=True, transform=transform, download=True)
test_set = datasets.MNIST(LOCAL_DATA_DIRECTORY, train=False, transform=transform, download=True)

inputs = sagemaker_session.upload_data(
    path=LOCAL_DATA_DIRECTORY,
    bucket=BUCKET,
    key_prefix=PREFIX)

from smexperiments.tracker import Tracker

with Tracker.create(display_name="Preprocessing", sagemaker_boto_client=sagemaker_client) as tracker:
    tracker.log_parameters({
        "normalization_mean": 0.1307,
        "normalization_std": 0.3081,
    })

    tracker.log_input(name="mnist-dataset", media_type="s3/uri", value=inputs)


experiment_name = f"torch-mnist-{datetime.datetime.now().strftime('%Y%m%d%H%M')}"
description = "Classification of mnist hand-written digits with pytorch."

mnist_experiment = Experiment.create(experiment_name=experiment_name,
                                     description=description,
                                     sagemaker_boto_client=sagemaker_client)


# %%
from sagemaker.pytorch import PyTorch

hidden_channel_trial_name_map = {} # Keep references to each Trial object

# If you want to run the following training jobs asynchronously, you may need to increase
# your resource limit. Otherwise, you can run them sequentially.
for i, num_hidden_channel in enumerate([2, 5, 10]):

    # create Trial object
    trial_name = f"torch-{num_hidden_channel}-hidden-channels-{datetime.datetime.now().strftime('%Y%m%d%H%M')}"
    cnn_trial = Trial.create(
        trial_name=trial_name,
        experiment_name=mnist_experiment.experiment_name,
        sagemaker_boto_client=sagemaker_client,
    )
    # Have a dict for variable references to the trial names
    hidden_channel_trial_name_map[num_hidden_channel] = trial_name

    # Associate the proprocessing trial component with the current trial
    cnn_trial.add_trial_component(tracker.trial_component)

    # all input configurations, parameters, and metrics specified in estimator
    # definition are automatically tracked
    estimator = PyTorch(
        entry_point='../scripts/torch/pytorch_mnist.py',
        role=role,
        sagemaker_session=sagemaker_session,
        framework_version='1.1.0',
        train_instance_count=1,
        train_instance_type='ml.c4.xlarge',
        code_location=f"s3://{BUCKET}/{PREFIX}",
        output_path=f"s3://{BUCKET}/{PREFIX}",
        base_job_name='torch-mnist',
        hyperparameters={
            'epochs': 2,
            'backend': 'gloo',
            'hidden_channels': num_hidden_channel,
            'dropout': 0.2,
            'optimizer': 'sgd'
        },
        metric_definitions=[
            {'Name':'train:loss', 'Regex':'Train Loss: (.*?);'},
            {'Name':'test:loss', 'Regex':'Test Average loss: (.*?),'},
            {'Name':'test:accuracy', 'Regex':'Test Accuracy: (.*?)%;'}
        ],
        enable_sagemaker_metrics=True,
    )

    # Now associate the estimator with the Experiment and Trial
    estimator.fit(
        inputs={'training': inputs},
        experiment_config={
            "TrialName": cnn_trial.trial_name,
            "TrialComponentDisplayName": "Training",
        },
        wait=True,
    )

    # give it a while before dispatching the next training job
    time.sleep(2)
```

The core of the code above is to loop through `num_hidden_channel` and create a trial for each value of it. We can specify other hyperparameters and a custom regex for metric definition we care about in the `PyTorch` object.

This can take 15 minutes to run. After completion, we can click on the beaker icon in the left bar. We can either inspect each individual trial and look at the charts for training over time, or we can look at all trial components in one chart by selecting them and right clicking to open then in a list, select add chart with Summary Statistics and Scatter Plot. It's quite powerful to create such visualization in the browser without having to write any code.

Experiment is extremely important. For a team, it is good practice to use SageMaker Experiments for storing and grouping results for easy access.

### 4.3 Tracking lineage: finding the best model

We've run our experiments, now we can find the best model simply by running some code in the notebook.

```py
from sagemaker.analytics import ExperimentAnalytics

search_expression = {
    "Filters":[
        {
            "Name": "DisplayName",
            "Operator": "Equals",
            "Value": "Training",
        }
    ],
}

trial_component_analytics = ExperimentAnalytics(
    sagemaker_session=sagemaker_session,
    experiment_name=mnist_experiment.experiment_name,
    search_expression=search_expression,
    sort_by="metrics.test:accuracy.max",
    sort_order="Descending",
    metric_names=['test:accuracy'],
    parameter_names=['hidden_channels', 'epochs', 'dropout', 'optimizer']
)

trial_component_analytics.dataframe()
```

We will see the dataframe as the one below

<img src="{{ site.baseurl }}/images/prodml/trial-comps.png" alt="trial comps" align="middle"/>

Recall that we have a dictionary `hidden_channel_trial_name_map` for variable references to the trials, we can use it to look at the best trial,

```py
lineage_table = ExperimentAnalytics(
    sagemaker_session=sagemaker_session,
    search_expression={
        "Filters":[{
            "Name": "Parents.TrialName",
            "Operator": "Equals",
            "Value": hidden_channel_trial_name_map[2]
        }]
    },
    sort_by="CreationTime",
    sort_order="Ascending",
)

lineage_table.dataframe()
```

Now we see a lot more information about it:

<img src="{{ site.baseurl }}/images/prodml/lineage.png" alt="lineage" align="middle"/>

SageMaker has more advanced hyperparameter tuning capabilities. You can dig in to learn more.

## 5. Deployment

tbd

## Reference

- Luigi's SageMaker course