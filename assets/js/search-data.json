{
  
    
        "post0": {
            "title": "FastAI Lesson 8: Backprop from the Foundations",
            "content": "Jeremy’s starting comments . “cutting-edge deep learning” now is more and more about engineering and not papers. It’s about who can make things in code that work properly. | Part II of fastai is bottom-up learning with code. It helps you understand the connections between algorithms, and make your own algorithm for your own problem, and debug, profile, maintain it. | Swift and Julia are the promising languages for high performance computing. | . Swift for TensorFlow vs. PyTorch . . Swift is a thin layer on top of LLVM. LLVM compiles Swift code to super fast machine code. . Python is the opposite. We write Python as an interface but things usually run in C++. It prevents doing deep dives as we shall see in this course. . Opportunity: join the Swift for TF community to contribute and be a pioneer in this field! . Recreate fastai Library from Scratch . . Benefit of doing this . Really experiment | Understand it by creating it | Tweak everything | Contribute | Correlate papers with code | . Opportunities . Make homework at the cutting edge | There are few DL practitioners that know what you know now | Experiment lots, especially in your area of expertise | Much of what you find will have not be written about before | Don’t wait to be perfect before you start communicating. Write stuff down for the person you were 6 months ago. | . How to Train a Good Model . . 5 steps of reducing overfitting . more data | data augmentation | generalization architectures | regularization | reducing architecture complexity (this should be the last step) | . Start Reading Papers . Get pass the fear of Greek letters! It’s just code. . Opportunity: there are blog posts that describing a paper better than the paper does. Write these blog posts! . Read blog posts and also the paper itself. . Goal: Recreating a Modern CNN Model . . For development, Jeremy recommends nbdev for library development in Jupyter notebook. . Tip: Python’s fire library lets you convert a function into CLI. . notebook: 01_matmul . Important: get familiar with PyTorch Tensors. It can do everything like a numpy array and it can run on GPU. . tensor.view() is equivalent to nparray.reshape(). . Creating Matrix Multiplication . Pure Python with 3 nested loops (speed lvl0) . Implement matrix multiplaction with 3 loops. . def matmul(a,b): ar,ac = a.shape # n_rows * n_cols br,bc = b.shape assert ac==br c = torch.zeros(ar, bc) for i in range(ar): for j in range(bc): for k in range(ac): # or br c[i,j] += a[i,k] * b[k,j] return c . This is super slow because it’s in Python. A (5, 784) by (784, 10) matrix multiplication took ~1s. MNIST needs ~10K of them, so it will take 10K seconds, that’s unacceptable. . The way to speed this up is to use something other than Python – use PyTorch where it uses ATen (C++) under the hood. . Tip: to get LaTeX formula, go to Wikipedia and click edit. Or go to Arxiv and do Download other format on the top right, then download source. . Elementwise vector operations with 2 nested loops (speed lvl1) . def matmul(a,b): ar,ac = a.shape br,bc = b.shape assert ac==br c = torch.zeros(ar, bc) for i in range(ar): for j in range(bc): # Any trailing &quot;,:&quot; can be removed # a[i, :] means the whole ith row # b[:, j] means the whole jth col # This is not really Python, it tells Python to call C c[i,j] = (a[i,:] * b[:,j]).sum() return c . This is hundreds of times faster. Next, broadcasting makes it even faster. . Tip: to test equal for floats, set a tolerance and use something like torch.allclose(). Float’s implementation gives small numerical errors. . Broadcasting with 1 loop (speed lvl2) . Broadcasting is the most powerful tool to speed things up. It gets rid of for loops and does implicit broadcast loops. . Any time we use broadcasting, we are using C speed (on CPU) or CUDA speed (on GPU). . Easiest example is a + 1 where a is a tensor. 1 is automatically turned into a tensor that matches the shape of a. This is scalar to tensor. . We can also broadcast vector to higher order tensors. . c = tensor([10.,20,30]) m = tensor([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]]) c + m &quot;&quot;&quot; tensor([[11., 22., 33.], [14., 25., 36.], [17., 28., 39.]]) We don&#39;t really copy the rows, the rows are given a *stride* of 1. &quot;&quot;&quot; # To check what c looks like after broadcasting t = c.expand_as(m); t &quot;&quot;&quot; tensor([[10., 20., 30.], [10., 20., 30.], [10., 20., 30.]]) Use t.storage() we can check the memory usage! It shows we only have one row in memory, not really making a full matrix during broadcasting. Use t.stride() shows (0, 1) meaning stride is 0 for rows, 1 for columns. This idea is used in all linear algebra libraries. &quot;&quot;&quot; . To add a dimension, use unsqueeze(axis), or, use None at that axis when indexing. Example: . # These are not in-place, c is not updated c.unsqueeze(0) # or c[None, :] &quot;&quot;&quot; tensor([[10., 20., 30.]]) &quot;&quot;&quot; c.unsqueeze(1) # or c[:, None] &quot;&quot;&quot; tensor([[10.], [20.], [30.]]) &quot;&quot;&quot; . Tip: always use None over unsqueeze because it’s more convenient and we can add more than one axis. . Trick: . We can omit trailing ,: as in c[None, :] == c[None] | We can use ... as in c[:, None] == c[..., None]. This is helpful especially when we don’t know the rank of the tensor. | . def matmul(a,b): ar,ac = a.shape br,bc = b.shape assert ac==br c = torch.zeros(ar, bc) for i in range(ar): # c[i,j] = (a[i,:] * b[:,j]).sum() # previous # Notice we got rid of loop j, that&#39;s why it&#39;s even faster # than the elementwise ops previously c[i] = (a[i].unsqueeze(-1) * b).sum(dim=0) # Or c[i] = (a[i][:, None] * b).sum(dim=0) return c . Concrete example for the above code: . a = torch.tensor([ [1, 2, 3], [4, 5, 6] ]) b = torch.tensor([ [10, 10], [20, 20], [30, 30] ]) # For i = 0 a0 = a[0][:, None] &quot;&quot;&quot; a0: tensor([[1], [2], [3]]) b: tensor([[10, 10], [20, 20], [30, 30]]) a0 * b: we rotate a0, a row vector to be a col vector, and broadcast into shape of b, and does elementwise * tensor([[10, 10], [40, 40], [90, 90]]) Then sum over axis=0 (rows) tensor([140, 140]) This is the result of matmul for a row vector a[0] and matrix b. Do the same for the 2nd row of a, we have a @ b: tensor([[140, 140], [320, 320]]) &quot;&quot;&quot; . Now we only have one level of loop for the matrix multiplication, and it’s 1000x times faster than the raw Python version of 3 nested loops. . Broadcasting Rule . c = tensor([10., 20., 30.]) # Add leading axis 0, row c[None,:], c[None,:].shape &quot;&quot;&quot; tensor([[10., 20., 30.]]), torch.Size([1, 3]) &quot;&quot;&quot; # Add trailing axis, col c[:,None], c[:,None].shape &quot;&quot;&quot; tensor([[10.], [20.], [30.]]) torch.Size([3, 1]) &quot;&quot;&quot; # How does this do broadcasting? # Here is where the BROADCASTING RULE comes in # Where there&#39;s a missing dimension, np/pytorch fills in a dimension # with size 1. A dim of size 1 can be broadcast into any size. # E.g. (1, 1, 3) * (256, 256, 3) -&gt; (256, 256, 3) c[None,:] * c[:,None] &quot;&quot;&quot; tensor([[100., 200., 300.], [200., 400., 600.], [300., 600., 900.]]) &quot;&quot;&quot; # Similarly c[None] &gt; c[:,None] &quot;&quot;&quot; tensor([[0, 1, 1], [0, 0, 1], [0, 0, 0]], dtype=torch.uint8) &quot;&quot;&quot; . When operating on two arrays/tensors, Numpy/PyTorch compares their shapes element-wise. It starts with the trailing dimensions, and works its way forward. Two dimensions are compatible when . they are equal, or | one of them is 1, in which case that dimension is broadcasted to make it the same size | . Arrays do not need to have the same number of dimensions. For example, if you have a 256*256*3 array of RGB values, and you want to scale each color in the image by a different value, you can multiply the image by a one-dimensional array with 3 values. Lining up the sizes of the trailing axes of these arrays according to the broadcast rules, shows that they are compatible: . Image (3d array): 256 x 256 x 3 Scale (1d array): 3 Result (3d array): 256 x 256 x 3 . The numpy documentation includes several examples of what dimensions can and can not be broadcast together. . Einstein Summation with no loops (speed lvl3) . Einstein summation notation: ik,kj-&gt;ij . ik,kj: input . ij: output . Each letter is the size of a dimension. Repeated letters indicate dot product. . # c[i,j] += a[i,k] * b[k,j] # c[i,j] = (a[i,:] * b[:,j]).sum() def matmul(a,b): return torch.einsum(&#39;ik,kj-&gt;ij&#39;, a, b) . This is even faster. And we can create new operations easily such as batch matrix multiplication: bik,bkj-&gt;bij. . But having a string as a language inside of a language is not a good idea, e.g. regex. We should be able to write Swift and Julia operating at this speed in a few years. . PyTorch op with no loops (speed lvl4) . PyTorch’s matmul or @ operation is even faster than einsum, it’s ~50K times faster than raw Python. Because to do really fast matrix multiplication on big matrices, it can’t fit in CPU cache and needs to be chopped down into smaller matrices. BLAS libraries do that. Examples are NVidia’s cuBLAS, and Intel’s mkl. . Fully Connected Nets . Forward pass . First, load the data and apply normalization. . Note: use the training set’s mean and std to normalize the validation set! Always make sure the validation set and the training set are normalized in the same way. . # num hidden nh = 50 # simplified kaiming init / he init: divide by sqrt(n_inputs). m is # examples w1 = torch.randn(m,nh)/math.sqrt(m) b1 = torch.zeros(nh) w2 = torch.randn(nh,1)/math.sqrt(nh) b2 = torch.zeros(1) # This should be ~ (0,1) (mean,std)... x_valid.mean(),x_valid.std() def lin(x, w, b): &quot;&quot;&quot;Linear layer&quot;&quot;&quot; return x@w + b t = lin(x_valid, w1, b1) # The effect of Kaiming init: makes the linear output have # ~0 mean and 1 std t.mean(),t.std() . Kaiming init is a very important factor to train deep networks. Some researchers trained a 10K-layer network without normalization layers just with careful initialization. . def relu(x): return x.clamp_min(0.) . Tip: if there’s a function for some calculation in pytorch, such as clamp_min, it’s generally written in C and it’s faster than your implementation in Python. . Kaiming Init . From pytorch docs: a: the negative slope of the rectifier used after this layer (0 for ReLU by default) . std=2(1+a2)×fan_in text{std} = sqrt{ frac{2}{(1 + a^2) times text{fan _in}}}std=(1+a2)×fan_in2​ . ​ . This was introduced in the paper that described the Imagenet-winning approach from He et al: Delving Deep into Rectifiers, which was also the first paper that claimed “super-human performance” on Imagenet (and, most importantly, it introduced resnets!) . Simply put, for ReLU, if the inputs are mean 0 and std 1, the numbers below 0 are clipped so we lose half the variance. The way to fix it is to time it by 2, proposed in the paper. . # kaiming init / he init for relu w1 = torch.randn(m,nh)*math.sqrt(2/m) w1.mean(),w1.std() # (tensor(0.0001), tensor(0.0508)) t = relu(lin(x_valid, w1, b1)) t.mean(),t.std() # (tensor(0.5678), tensor(0.8491)) . Conv layers can also be looked at as linear layers with a special weight matrix where there are a lot of 0s for the pixels outside the filter, so this initialization does the same thing for them. . In pytorch, . init.kaiming_normal_(w1, mode=&#39;fan_out&#39;) # check doc by `init.kaiming_normal_??` . Then we can write the model and the loss function. We use MSE for now for simplicity. . def model(xb): l1 = lin(xb, w1, b1) l2 = relu(l1) l3 = lin(l2, w2, b2) return l3 def mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean() y_train,y_valid = y_train.float(),y_valid.float() preds = model(x_train) mse(preds, y_train) . Backward pass . All you need to know about matrix calculus from scratch: https://explained.ai/matrix-calculus/index.html . def mse_grad(inp, targ): # grad of loss with respect to output of previous layer inp.grad = 2. * (inp.squeeze() - targ).unsqueeze(-1) / inp.shape[0] def relu_grad(inp, out): # grad of relu with respect to input activations inp.grad = (inp&gt;0).float() * out.grad def lin_grad(inp, out, w, b): # grad of matmul with respect to input inp.grad = out.grad @ w.t() w.grad = (inp.unsqueeze(-1) * out.grad.unsqueeze(1)).sum(0) b.grad = out.grad.sum(0) def forward_and_backward(inp, targ): # forward pass: l1 = inp @ w1 + b1 l2 = relu(l1) out = l2 @ w2 + b2 # we don&#39;t actually need the loss in backward! # this is just here if we want to print it out! loss = mse(out, targ) # backward pass: mse_grad(out, targ) lin_grad(l2, out, w2, b2) relu_grad(l1, l2) lin_grad(inp, l1, w1, b1) . Layers as Classes . Observe the above code, we see each function for grad can take in inputs, weight and bias. We can make layer classes to have inputs, weight and bias, and define a forward() to calculate outputs, and backward() to calculate the gradients. Refactor the previous code, . class Relu(): # Notice this Relu() class does not have __init__ # Instantiating an instance is just `Relu()` # dunder call means we can use the class name as a function! def __call__(self, inp): self.inp = inp self.out = inp.clamp_min(0.)-0.5 return self.out def backward(self): self.inp.g = (self.inp&gt;0).float() * self.out.g class Lin(): def __init__(self, w, b): self.w, self.b = w, b def __call__(self, inp): self.inp = inp self.out = inp@self.w + self.b return self.out def backward(self): self.inp.g = self.out.g @ self.w.t() # Creating a giant outer product, just to sum it, is inefficient! self.w.g = (self.inp.unsqueeze(-1) * self.out.g.unsqueeze(1)).sum(0) self.b.g = self.out.g.sum(0) class Mse(): def __call__(self, inp, targ): self.inp = inp self.targ = targ self.out = (inp.squeeze() - targ).pow(2).mean() return self.out def backward(self): self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0] class Model(): def __init__(self, w1, b1, w2, b2): self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)] self.loss = Mse() def __call__(self, x, targ): for l in self.layers: x = l(x) return self.loss(x, targ) def backward(self): self.loss.backward() for l in reversed(self.layers): l.backward() . Layers as Modules . We see that all layers have outputs, forward and backward passes. Further refactoring, we introduce the Module class (similar to pytorch nn.Module). . class Module(): def __call__(self, *args): self.args = args self.out = self.forward(*args) return self.out def forward(self): raise Exception(&#39;not implemented&#39;) def backward(self): self.bwd(self.out, *self.args) class Relu(Module): def forward(self, inp): return inp.clamp_min(0.) - 0.5 def bwd(self, out, inp): inp.g = (inp&gt;0).float() * out.g class Lin(Module): def __init__(self, w, b): self.w,self.b = w,b def forward(self, inp): return inp@self.w + self.b def bwd(self, out, inp): inp.g = out.g @ self.w.t() self.w.g = inp.t() @ out.g self.b.g = out.g.sum(0) class Mse(Module): def forward (self, inp, targ): return (inp.squeeze() - targ).pow(2).mean() def bwd(self, out, inp, targ): inp.g = 2 * (inp.squeeze()-targ).unsqueeze(-1) / targ.shape[0] class Model(): def __init__(self): self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)] self.loss = Mse() def __call__(self, x, targ): for l in self.layers: x = l(x) return self.loss(x, targ) def backward(self): self.loss.backward() for l in reversed(self.layers): l.backward() . Equivalent Code in PyTorch . from torch import nn class Model(nn.Module): def __init__(self, n_in, nh, n_out): super().__init__() self.layers = [ nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out) ] self.loss = mse def __call__(self, x, targ): for l in self.layers: x = l(x) return self.loss(x.squeeze(), targ) model = Model(m, nh, 1) %time loss = model(x_train, y_train) %time loss.backward() . In the next lesson, we will get the training loop, the optimizer, and other loss functions. . Homework . Read Kaiming’s paper: Delving Deep into Rectifiers. Focus on section 2.2. | Xavier init paper is also really readable, we will implement a lot from it. | . My Random Thoughts . The career path after fast.ai should be something that mixes engineering and research: . Applied scientist or research engineer. It’s different from the usual “data scientist”, which focuses on analytics, business metrics and non-DL work (lacks in engineering and research in DL); it’s also different from ML engineer, which focuses on productionizing and maintaining models (lacks in research). | Open source contribution to key DL projects. Swift for Tensorflow is one advocated by Jeremy. | . Some one who can implement DL frameworks from scratch and grasp key DL research shouldn’t be a “data scientist” or “ML engineer” in a non-research organization. There are tons of data scientists and ML engineers out there, but those who can reach high level of fast.ai competence are rare. . Demonstrate the knowledge by blogging and making a great project. .",
            "url": "http://blog.logancyang.com/note/fastai/2020/05/22/fastai-lesson8.html",
            "relUrl": "/note/fastai/2020/05/22/fastai-lesson8.html",
            "date": " • May 22, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "FastAI Lesson 7: Resnets from scratch; U-net; Generative adversarial networks; RNNs",
            "content": "Notebook: lesson7-resnet-mnist . PyTorch puts channel at the 1st dimension by default. An image in MNIST is a (1,28, 28) rank 3 tensor. . In fastai, there is a difference between validation and test set. Test set doesn’t have label. Use validation set for model development. If you want to do inference on many things at a time and not one at a time, set the data as test instead of valid. . Initial steps: . Create ItemList from image folders | Split into train and valid. | . Tip: for data augmentation, MNIST can&#39;t have much transformation, you can&#39;t flip it because it changes the meaning of the number, you can&#39;t zoom because it&#39;s low res. The only transform is to add random padding. Do this transform on training set but not validation set. If not using a pretrained model, don’t pass in stat in normalize() for databunch, it grabs a subset of the data at random and figures out how to normalize. . plot_multi(_plot, nrow, ncol, figsize=()) is a fastai function that plots multiple examples in a grid. Define _plot first for what to show. . # Showing a batch of data using the DataBlock API xb,yb = data.one_batch() xb.shape,yb.shape # DataBunch has .show_batch() data.show_batch(rows=3, figsize=(5,5)) . Then we define the convolution function with fixed kernel size, stride and padding. . # ni is # input channels, nf is # output filters (kernels) def conv(ni,nf): return nn.Conv2d(ni, nf, kernel_size=3, stride=2, padding=1) model = nn.Sequential( # 1 channel in, 8 channels out (picked by us), output size 8*14*14 conv(1, 8), nn.BatchNorm2d(8), nn.ReLU(), # 8 channel in, 16 channels out (picked by us), output size 16*7*7 conv(8, 16), # 7 nn.BatchNorm2d(16), nn.ReLU(), # 16 channel in, 32 channels out (picked by us), output size 32*4*4 conv(16, 32), # 4 nn.BatchNorm2d(32), nn.ReLU(), # 32 channel in, 16 channels out (picked by us), output size 16*2*2 conv(32, 16), # 2 nn.BatchNorm2d(16), nn.ReLU(), # 16 channel in, 10 channels out (picked by us), output size 10*1*1 conv(16, 10), # 1 nn.BatchNorm2d(10), Flatten() # remove (1,1) grid ) . . Trick: Flatten() gets rid of all the unit axes (the axes with 1s)! (10, 1, 1) becomes just (10,), a flat vector of dim 10! # create learner learn = Learner(data, model, loss_func = nn.CrossEntropyLoss(), metrics=accuracy) # print learner summary print(learn.summary()) # pop data onto GPU xb = xb.cuda() # check model output shape model(xb).shape . This is a model we built from scratch with a simple CNN architecture, it takes 12s to train on my GPU and got 98.8% accuracy! Already super good. . Refactor . fastai has conv_layer so we can skip writing all the batch norm and relu’s. . def conv2(ni,nf): return conv_layer(ni,nf,stride=2) model = nn.Sequential( conv2(1, 8), # 14 conv2(8, 16), # 7 conv2(16, 32), # 4 conv2(32, 16), # 2 conv2(16, 10), # 1 Flatten() # remove (1,1) grid ) learn = Learner(data, model, loss_func = nn.CrossEntropyLoss(), metrics=accuracy) learn.fit_one_cycle(10, max_lr=0.1) . It’s the same as previous code, just looks better. Train 10 epochs, we can get to 99%+ accuracy. . Introduce the Residual Block . The residual block is a revolutionary technique in computer vision. . Kaiming He et. al. at Microsoft Research initially found that a 56-layer CNN was performing worse than a 20-layer CNN which made no sense. He created an architecture where a 56-layer CNN contains the 20-layer CNN, by adding some skip connections that skipped some conv layers. That way, it must be as least as good as the 20-layer CNN because the deeper CNN could just set the skipped conv layers to 0 and only keep the identity links. . . Instead of having . output = conv( conv (x) ) . The residual block is . output = conv( conv (x) ) + x . The result was that he won ImageNet that year (2015). . . Trick: if an NN or GAN doesn&#39;t work so well, try replacing the conv layers with residual blocks! Check out the fantastic paper Visualizing the Loss Landscape. This is 3 years later since ResNet, and people started to realize why it worked. With the skip connections, the loss landscape is much smoother. . The batch norm had the same story. This reminds us innovation usually comes from intuition. Intuition comes first, people realize what’s going on and why it works much later. . . fastai has res_block. We add a res_block after every conv2 layer from previous code, we get . model = nn.Sequential( conv2(1, 8), res_block(8), conv2(8, 16), res_block(16), conv2(16, 32), res_block(32), conv2(32, 16), res_block(16), conv2(16, 10), Flatten() ) . Further refactoring it, . def conv_and_res(ni,nf): return nn.Sequential(conv2(ni, nf), res_block(nf)) model = nn.Sequential( conv_and_res(1, 8), conv_and_res(8, 16), conv_and_res(16, 32), conv_and_res(32, 16), conv2(16, 10), Flatten() ) learn = Learner(data, model, loss_func = nn.CrossEntropyLoss(), metrics=accuracy) learn.lr_find(end_lr=100) learn.recorder.plot() learn.fit_one_cycle(12, max_lr=0.05) print(learn.summary()) . . Tip: when you try out new architectures, keep refactor the code and reuse more to avoid mistakes. Resnet is quite good and can reach SOTA accuracy for a lot of tasks. More modern techniques such as group convolutions don’t train as fast. . DenseNet is another architecture, its only difference from Resnet is that instead of a x + conv(conv(x)), it does concat(x, conv(conv(x))) (the channel gets a little bigger). It is called a DenseBlock instead of ResBlock. The paper of DenseNet seems complicated but it’s really very similar to Resnet. . DenseNet is very memory intensive because it maintains all previous features, BUT it has much fewer parameters. It works really well for small datasets. . U-Net . Use resnet34 and half-stride. What half-stride is really doing is nearest-neighbor interpolation or a bilinear interpolation with stride 1, it up samples the patch and increases the size, as shown below. . . Fantastic paper for convolution: A Guide to Convolution Arithmetic for Deep Learning . Nowadays we use a pretrained resnet34 as the encoder in U-net. . . Trick: if you see two convs in a row, probably should use a resnet block instead. A skip connection with &quot;+&quot; or &quot;concat&quot; usually works great. U-net came before resnet and densenet but it had a lot of the similar ideas and worked great for segmentation tasks. . . Tip: don&#39;t use U-net for classification, because you only need the down-sampling part, not the up-sampling part. Use U-net for generative purposes such as image segmentation because the output resolution is the same as input resolution. Image Restoration with U-Net and GAN . Notebook: superres-gan . We use the U-net architecture to train a super-resolution model. This is a model which can increase the resolution of a low-quality image. Our model won’t only increase resolution—it will also remove jpeg artifacts, and remove unwanted text watermarks. . In order to make our model produce high quality results, we need to create a custom loss function which incorporates feature loss (also known as perceptual loss), along with gram loss. These techniques can be used for many other types of image generation task, such as image colorization. . Traditionally, the GAN is hard to train because the initial generator and critic are bad. Fastai uses pretrained generator and critic, so they are already pretty good. After that the training of GAN is much easier. . . To train a fastai version GAN, we need two folders, one with high-res original images, one with generated images. . . Trick: to free GPU memory without restarting notebook, run the code below. To free GPU memory without restarting notebook, run . my_learner = None gc.collect() . Running nvidia-smi won’t show it freed because pytorch has pre-allocated cache, but it’s available. . # need to wrap the loss with AdaptiveLoss for GAN to work # Will revisit in Part II loss_critic = AdaptiveLoss(nn.BCEWithLogitsLoss()) # Use gan_critic() and not resnet here def create_critic_learner(data, metrics): return Learner(data, gan_critic(), metrics=metrics, loss_func=loss_critic, wd=wd) # GAN version of accuracy: accuracy_thresh_expand learn_critic = create_critic_learner(data_crit, accuracy_thresh_expand) learn_critic.fit_one_cycle(6, 1e-3) learn_critic.save(&#39;critic-pre2&#39;) . . Tip: for GAN, fastai&#39;s GANLearner figures out the back and forth training of the generator and the critic for us. Use the hyperparameters like this below. switcher = partial(AdaptiveGANSwitcher, critic_thresh=0.65) learn = GANLearner.from_learners( learn_gen, learn_crit, weights_gen=(1.,50.), show_img=False, switcher=switcher, opt_func=partial(optim.Adam, betas=(0.,0.99)), wd=wd ) learn.callback_fns.append(partial(GANDiscriminativeLR, mult_lr=5.)) lr = 1e-4 learn.fit(40,lr) # NOTE: the train_loss and gen_loss should stay around the same values # because when the generator and critic both get better, the loss is relative. # The only way to tell how it&#39;s doing is by looking at the image results # Use show_img=True to check . WGAN . Notebook wgan is briefly mentioned for the task of generating image from pure noise without pretraining. Jeremy mentioned it’s a relatively old approach and the task isn’t particularly useful, but it’s good research exercise. . Perceptual Loss (Feature Loss) . Notebook: lesson7-superres . Paper: Perceptual Losses for Real-Time Style Transfer and Super-Resolution . Jeremy didn’t like the name “perceptual loss” and he named it “feature loss” in fastai library. . Convention: in U-net shaped architecture, the down-sampling part is called encoder and the up-sampling part is called decoder. . The paper’s idea is to compare the generated image with the target image using a new loss function, that is, the activation from a middle layer in an ImageNet pretrained VGG network. Use the two images and pass them through this network up to that layer and check the difference. The intuition for this is that, each pixel in that activation should be capturing some feature of ImageNet images, such as furriness, round shaped, has eyeballs, etc. If the two images agree on these features they should have small loss with this loss function. . . With 1 GPU and 1-2hr time, we can generate medium res images from low res images, or high res from medium res using this approach. . A fastai student Jason in 2018 cohort created the famous deOldify project. He crappified color images to black and white, and trained a GAN with feature loss to color 19th century images! . Recap . . Watch the videos again and go through notebooks in detail to understand better. . Recurrent Neural Network . Notebook: lesson7-human-numbers . Toy example dataset with numbers in English, the task is to predict the next word – language model. . xxbos: beginning of stream, meaning start of document. . data.bptt: bptt is backprop thru time . Basic NN with 1 hidden layer: . . One step toward RNN . . Basic RNN: . . Refactor, make it a loop –&gt; RNN . . There is nothing new for an RNN, it is just a fully connected NN with maintained states. . What GRU or LSTM is basically doing is to determine how much of the green arrow to keep and how much of the brown arrow to keep. Will see more in Course Part II. . . Homework . Read papers and write blog posts in plain language. | Build visualizations and apps. Just finish something. | . Answers and Comments from Jeremy . Reinforcement learning is far from being useful for average real world problems. Transfer learning is the most promising. | If you can build the notebooks using fastai from blank, it’s already really rare and very competent in DL. | .",
            "url": "http://blog.logancyang.com/note/fastai/2020/04/30/fastai-lesson7.html",
            "relUrl": "/note/fastai/2020/04/30/fastai-lesson7.html",
            "date": " • Apr 30, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "FastAI Lesson 6: Regression on Tabular Time Series; Regularization; Convolution; Data Ethics",
            "content": "Tip: Use platform.ai to find clusters of unlabeled data to label manually. It uses pretrained models so you can pick a middle layer and pick some specific projections from it. It’s a way of clustering similar images for labeling and it helps you build your better model on top of it. There is a thread for more info. . Regression on Tabular Time Series; Regularization cont. . Look at the rossmann dataset from Kaggle. It uses historical sales data to predict a small period in its future. . Most of times, the approach of embeddings and tabular data is more effective than RNNs for time series forecasting because we have useful metadata like day of week, day of month, locations, etc. RNN is better for pure sequences. So, in most business settings, use tabular approach for time series! . Tip: grab a small part of data, say 2000 rows to explore first. Split to training and test. . The two notebooks for rossmann data are great examples of pandas data preprocessing. There are preprocessors Categortify (-1 for NaN), Fillmissing (replace NaN with median), etc. You can apply them by simply assigning them to a list and pass into databunch creation. . Next, identify categorical and continuous variables. Think carefully. For example, Day may be a number but it’s categorical because in some cases the number of sales for a day is independent of surrounding days. . Don’t forget to make the validation set with the same time range as the test set! . procs=[FillMissing, Categorify, Normalize] cat_vars = [&#39;Store&#39;, &#39;DayOfWeek&#39;, &#39;Year&#39;, &#39;Month&#39;, &#39;Day&#39;, &#39;StateHoliday&#39;, &#39;CompetitionMonthsOpen&#39;, &#39;Promo2Weeks&#39;, &#39;StoreType&#39;, &#39;Assortment&#39;, &#39;PromoInterval&#39;, &#39;CompetitionOpenSinceYear&#39;, &#39;Promo2SinceYear&#39;, &#39;State&#39;, &#39;Week&#39;, &#39;Events&#39;, &#39;Promo_fw&#39;, &#39;Promo_bw&#39;, &#39;StateHoliday_fw&#39;, &#39;StateHoliday_bw&#39;, &#39;SchoolHoliday_fw&#39;, &#39;SchoolHoliday_bw&#39;] cont_vars = [&#39;CompetitionDistance&#39;, &#39;Max_TemperatureC&#39;, &#39;Mean_TemperatureC&#39;, &#39;Min_TemperatureC&#39;, &#39;Max_Humidity&#39;, &#39;Mean_Humidity&#39;, &#39;Min_Humidity&#39;, &#39;Max_Wind_SpeedKm_h&#39;, &#39;Mean_Wind_SpeedKm_h&#39;, &#39;CloudCover&#39;, &#39;trend&#39;, &#39;trend_DE&#39;, &#39;AfterStateHoliday&#39;, &#39;BeforeStateHoliday&#39;, &#39;Promo&#39;, &#39;SchoolHoliday&#39;] dep_var = &#39;Sales&#39; df = train_df[cat_vars + cont_vars + [dep_var,&#39;Date&#39;]].copy() data = (TabularList.from_df(df, path=path, cat_names=cat_vars, cont_names=cont_vars, procs=procs,) .split_by_idx(valid_idx) # NOTE: fastai assumes classification if label_cls is not float, # here we need regression, so make it a FloatList! # log=True takes log of y, use this for percent error because # it turns ratio into difference, RMSPE -&gt; RMSE .label_from_df(cols=dep_var, label_cls=FloatList, log=True) .add_test( TabularList.from_df( test_df, path=path, cat_names=cat_vars, cont_names=cont_vars)) .databunch()) . Tip: for target variable as population, sales etc. where we care more about change rather than absolute differences, we use Root Mean Squared Percent Error (RMSPE) rather than RMSE. Take the log of y with log=True above to make it RMSE. . . Trick: set y_range a bit wider than actual to get better result. In this case 1.2 * ymax. For this problem, the architecture is a fully connected NN. This Kaggle competition is 3 years old but there is no significant better model than it. . We use a weight matrix of 1000 by 500, which is 500K parameters on a few 100K dataset. It is going to overfit. Use regularization to counter overfitting, NOT reducing the parameters manually. . ps=[0.001, 0.01] is DROPOUT. The dropout paper. . Dropout is not dropping weights, but dropping activations! . . Each minibatch we throw away a difference subset of the activations with probability p. A common value is 0.5. . Hinton mentioned where this idea came from. He’s a neural scientist by training so he used dropout to imitate the effect of spiking neurons given that we don’t exact know how neurons spike. . “If you have noisy activations, you can afford to use a much bigger model” – Hinton. . These math ideas almost never come from math but from physical intuitions. . Jeremy advice for research: the original dropout paper is one of the most influential papers in the last decade but was rejected by NIPS. The research community is poor at recognizing important work. Find what interests you and don’t just follow what most people say. . Dropout works really really well! . learn = tabular_learner(data, layers=[1000,500], ps=[0.001,0.01], emb_drop=0.04, y_range=y_range, metrics=exp_rmspe) . ps is a list of dropout probabilities. All fastai learners have this parameter. . In PyTorch, dropout is applied at training time and not test time. You don’t need to do anything at test time. . emb_drop is to randomly drop activations in the embedding. . Reminder: what is embeddings in this case? It’s the vector for each feature, e.g. stores, DayOfWeek, Year, etc. The data matrix started with them as columns and each entry in time as row. Do a matrix factorization and we get these embeddings for the features. . . BatchNorm1d is for continuous variables. . Batch normalization paper. Interestingly, the attempt to explain why it helps NN training (by reducing covariate shifts) was wrong in this paper. Usually researchers have intuitions and did the experiment, it worked, and tried to find math explanations post-hoc. Another paper found it is not why it works. . . With batch norm, the loss landscape is not as bumpy as without batch norm! You can increase your learning rate! . Explanation: the reason it works is activation range shift. people used to think the normalization by mean and std is the important reason why it worked. But it’s actually the two extra parameters. We use activations * g + b to shift the activations from one range to another, just like when we have output predictions in 0-1 but we need to shift to 1-5 for movie ratings. g and b are the biases to directionly shift the range rather than relying on the weights. The weights have nonlinear relationships because of the nonlinear activations in between so they are hard to tune for range shifting (that’s why the loss landscape is bumpy). With 2 more direct controlling parameters, the range shift is much easier. . A much newer method is called Weight Norm and is used by fastai. . . Note: for batch norm, we don&#39;t actually use the exact mean and std for each minibatch, that will be too bumpy. We use the exponentially weighted moving average of mean and std. That&#39;s why there is a momentum=0.1 in batch norm layer, it is not the momentum as in optimization, it is for moving average. . A smaller momentum means less variations from minibatch to minibatch ==&gt; less regularization . . A larger momentum means more variations from minibatch to minibatch ==&gt; more regularization . . In the regularization techniques, we . always want batch norm | weight decay is more preferable than L2 regularization (are they not the same thing? yes and no? ) | weight decay vs. dropout? No definitive answer. Use a bit of both. Try it out. | . The next regularization technique is data augmentation. Jeremy is most excited about this approach. . CNN, Data Augmentation; Regularization cont. . Now use the pets-more notebook. . # check this list of transformations doc(get_transforms) . Tip: for border transform, “reflection” works the best! Better than black borders. And warping works great. . Apply the kind of transform that will match test data. . Research: how to do data augmentation outside CV, e.g. in NLP. . Jeremy goes on explaining convolution and image kernel with visual explanation here, developed by ex-Uber colleague Victor Powell. . . Since 3-by-3 kernels can only reach the second to last rim of the image, we need padding. 0-padding is fine but reflective padding is better. . How CNN works: . We have RGB color image of size w * h * 3, apply k 3 * 3 * 3 kernels to it (without hard definitions such as left right sobel, just randomly init them). Each kernel outputs 1 number, so we end up with a w * h * k output of the image. . | We can have stride 2 by 2 so the w * h gets shrinked by a factor of 2. We also apply 2x more kernels, so the output gets squashed into a longer stick shape with smaller cross section.. . | We repeatedly stack a lot of these conv layers. . | . . . One interesting trick is that we use a 7 by 7 kernel (and more padding) for the first layer to handle the input image, then use 3 by 3 ones in hidden layers. Jeremy will talk about why in Course Part II. . Tip: learn.summary() and learn.model prints out the info and architecture of the NN. . Heatmap for CNN . Next, find the heatmap for CNN’s focus in images. . # Make the kernel for 3 channels k = tensor([ [0. ,-5/3,1], [-5/3,-5/3,1], [1. ,1 ,1], ]).expand(1,3,3,3)/6 . In PyTorch, the shape of the tensor in the case is (# kernels, # channels, height, width). . . Trick: indexing None into a tensor in both pytorch and numpy gives to a new unit axis!! (shown below) t.shape = (3, 352, 352) # Very handy trick t[None].shape # (1, 3, 352, 352) . How the last layers work . Say we have a (w=11, h=11, ch=512) shape layer, that is the last layer of the conv part of the network. The output expects a vector of shape (class=37, 1), for that we take the mean of every slice in the 512 slices, each (w, h) slice only outputs 1 number. This is called AVERAGE POOLING. Now, we have a (512, 1) vector. We then need a weight matrix of (37, 512). This is a linear layer that has input size 512 and output size 37. . W1 -- W2 -- W3 * (x1 | x2 | x3 | ... | x512)_T -- ... -- W37 Each weight vector W has length 512, it dots the (512, 1) vector, produces a weighted sum, activation A1 for class 1. Same for other Ws. We then have an output of (A1 ... A37). The different setting of each W, a shape (1, 512) row vector is the deciding factor that maps the 512 features from average pooling into the final 37 classes. . This is how the classification is done. Each averaged number from a slice indicates a “feature”, it could be “how fluffy it is”, “does it have pointy ears”, etc. The average is equivalent to a sum of the activations in each slice indicating how activated collectively they are in one slice. . Interestingly, if you squash the 512 channels to 1 and average over all the (w, h) slices, the output is one (w, h) matrix. This is the average of all the pixels in the same position in each channel. It then indicates the relevance of “position”, not individual features. With this, we can create a heatmap that shows where it is most relevant to decide whether the cat is a maine coone. . Tip: fastai has a great advanced feature called a “hook”. It allows you to tap into the pytorch code, e.g. a forward pass and manipulate the values in the middle layers. . Tip: if you use hook, don’t forget to remove it after, because it will store the intermediate values everytime you call the model and it is memory intensive. . # To get the conv part of the network, do this m = learn.model.eval() m[0] # Then it shows the conv part of the network # Create a minibatch with just one image in it xb,_ = data.one_item(x) xb_im = Image(data.denorm(xb)[0]) # Pop it into a GPU xb = xb.cuda() # To hook into the output of m[0] from fastai.callbacks.hooks import * def hooked_backward(cat=y): # `with` in Python is called context manager, # at the end of `with` it will remove the hook with hook_output(m[0]) as hook_a: with hook_output(m[0], grad=True) as hook_g: # pytorch allows us to use the model as a function m() preds = m(xb) preds[0,int(cat)].backward() # we don&#39;t care about pred, we care about the hook return hook_a,hook_g hook_a, hook_g = hooked_backward() # fastai hook has .stored for the things you want in the hook acts = hook_a.stored[0].cpu() acts.shape def show_heatmap(hm): _,ax = plt.subplots() xb_im.show(ax) ax.imshow(hm, alpha=0.6, extent=(0,352,352,0), interpolation=&#39;bilinear&#39;, cmap=&#39;magma&#39;); show_heatmap(avg_acts) . Tip: it’s very important to frequently print out the shapes of the tensors, and think about why, think about the pictures in CNN architecture, the learn.summary() and learn.model. . Data Ethics . Check out Rachel’s TED talk here. . . One potential solution: put human in the loop! . Don’t be slaves of algorithm, avoid run-away feedback loops (bad recommendations feed bad behaviors). .",
            "url": "http://blog.logancyang.com/note/fastai/2020/04/22/fastai-lesson6.html",
            "relUrl": "/note/fastai/2020/04/22/fastai-lesson6.html",
            "date": " • Apr 22, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "FastAI Lesson 5: Collaborative Filtering cont.; Backpropagation; Accelerated SGD; NN from Scratch",
            "content": "Discussion on basic NN . PyTorch tip: anything with an underscore after, means in-place, for example a.sub_(val) means a -= val. . SGD: . in code: a.sub_(lr * a.grad) | in formula | . . . . There two types of tensors in NN . parameters | activations | . Input data is one special case in activation. . Activation functions are element-wise functions. Most of the time just use ReLU. . Universal Approximation Theorem: once you stack these tensors together, you can get arbitrarily close approximation of any function. . . Finetuning: what happens when we do transfer learning on resnet-34? . resnet-34 is trained on imagenet where the task was to output probabilities for 1000 classes, so the last layer of weights has 1000 columns. . You probably don’t have 1000 classes, or don’t have the same 1000 classes. So that last layer is of no use to you. . . When you do create_cnn in fastai, it deletes the last layer and puts in two new weight matrices and a ReLU in between, the one to the right has # columns = databunch.c. All previous layers are frozen. — . It’s faster, needs less memory when we freeze early layers. . . After unfreeze, we split our model into a few sections. We think the earlier layers are pretty good, so we give them a smaller learning rate. For example, the first section we give it lr = 1e-5 and the second section we give it lr = 1e-3, etc. This is called Discriminative Learning Rate. — . We don’t want to mess with already good layers with big learning rate. . When you call fit, . # All layers use the same learning rate fit(num_epoch, lr=1e-3) # The last layers (added by create_cnn) use 1e-3, and ALL OTHER LAYERS USE THIS VALUE / 3. # Will discuss the /3 in part II. It&#39;s a quirk of batch normalization fit(num_epoch, lr=slice(1e-3)) # The last layers added by create_cnn will use 1e-3, the first layers will use 1e-5, # the middle layers will have equally spaced lr between the two fit(num_epoch, lr=slice(1e-5, 1e-3)) . We use different lr for different layer groups. . For Collaborative Filtering, there is only one layer, so use just one lr is fine. . . . Terminology: Matrix multiplication is one form of Affine Functions. Convolution is another form. When we talk about affine functions we mean linear function like matrix multiplication. . Collaborative Filtering cont. . Tip: Multiplying with a one-hot encoded matrix (identity matrix) is equivalent to a row lookup. Therefore, never actually do the matrix multiplication, do row lookup instead for better time and space performance. . Terminology: embedding means look something up in an array, which is equivalent to multiplying with an identity matrix, or one-hot encoded matrix. . Once we train a collaborative filtering model, the result user embeddings and movie embeddings contain interesting property of users and movies. . For example, the 1st number in the user embedding could mean whether that user likes a movie with Tom Hanks in it, and the 1st number in the movie embedding is an indicator whether the movie has Tom Hanks. Then, the dot product has a bigger value if both numbers are big, meaning the user will like that movie. . Each dimension in the embedding is a kind of latent factor. . Bias term: there are genuine bad movies, and there are users who generally rate movies low or high. That is when the bias term is useful. It is the value that’s not relevant to all the latent factors in the embedding. . learn = collab_learner(data, n_factors=40, y_range=y_range, wd=1e-1) . . Trick: to get more accuracy on the movie ratings data ranging from 0.5 to 5, set the y_range (which controls the sigmoid asymptotes) to [0, 5.5] n_factors for collab_learner is the # columns, or the # embedding dimensions. They are the “latent factors”. This learner is actually doing matrix factorization. n_factors = 40 is an experiment result that works best. . One good trick to identify best and worst movies regardless of latent factors (they are good or bad on average and not affected by user attributes) is to look at bias. For example, these are some generally bad movies, . [(tensor(-0.3807), &#39;Children of the Corn: The Gathering (1996)&#39;, 1.3157894736842106), (tensor(-0.2987), &#39;Mortal Kombat: Annihilation (1997)&#39;, 1.9534883720930232), (tensor(-0.2976), &#39;Striptease (1996)&#39;, 2.2388059701492535), (tensor(-0.2973), &#39;Lawnmower Man 2: Beyond Cyberspace (1996)&#39;, 1.7142857142857142), (tensor(-0.2699), &#39;Cable Guy, The (1996)&#39;, 2.339622641509434), (tensor(-0.2676), &#39;Free Willy 3: The Rescue (1997)&#39;, 1.7407407407407407), (tensor(-0.2578), &#39;Grease 2 (1982)&#39;, 2.0), (tensor(-0.2526), &#39;Barb Wire (1996)&#39;, 1.9333333333333333), (tensor(-0.2502), &#39;Bio-Dome (1996)&#39;, 1.903225806451613), (tensor(-0.2438), &#39;Island of Dr. Moreau, The (1996)&#39;, 2.1578947368421053), (tensor(-0.2407), &#39;Crow: City of Angels, The (1996)&#39;, 1.9487179487179487), (tensor(-0.2275), &quot;Joe&#39;s Apartment (1996)&quot;, 2.2444444444444445), (tensor(-0.2187), &#39;Leave It to Beaver (1997)&#39;, 1.8409090909090908), (tensor(-0.2173), &#39;Lawnmower Man, The (1992)&#39;, 2.4461538461538463), (tensor(-0.2076), &quot;Stephen King&#39;s The Langoliers (1995)&quot;, 2.413793103448276)] . And these are some generally good movies, . (tensor(0.5738), &quot;Schindler&#39;s List (1993)&quot;, 4.466442953020135), (tensor(0.5580), &#39;Titanic (1997)&#39;, 4.2457142857142856), (tensor(0.5491), &#39;Silence of the Lambs, The (1991)&#39;, 4.28974358974359), (tensor(0.5480), &#39;Shawshank Redemption, The (1994)&#39;, 4.445229681978798), (tensor(0.5439), &#39;Star Wars (1977)&#39;, 4.3584905660377355), (tensor(0.5278), &#39;L.A. Confidential (1997)&#39;, 4.161616161616162), (tensor(0.5112), &#39;As Good As It Gets (1997)&#39;, 4.196428571428571), (tensor(0.5078), &#39;Rear Window (1954)&#39;, 4.3875598086124405), (tensor(0.4927), &#39;Good Will Hunting (1997)&#39;, 4.262626262626263), (tensor(0.4855), &#39;Apt Pupil (1998)&#39;, 4.1), (tensor(0.4810), &#39;Casablanca (1942)&#39;, 4.45679012345679), (tensor(0.4728), &#39;Usual Suspects, The (1995)&#39;, 4.385767790262173), (tensor(0.4705), &#39;Close Shave, A (1995)&#39;, 4.491071428571429), (tensor(0.4539), &#39;Boot, Das (1981)&#39;, 4.203980099502488), (tensor(0.4514), &#39;Vertigo (1958)&#39;, 4.251396648044692) . It’s likely that recommending the ones with biggest positive biases is good for new users we don’t know much about. . To understand the latent factors, 40 is too much to look at. We can do PCA and reduce to 3 dimensions. . . Note: checking activation layers in NN with PCA is often a good idea, because we have way too many activations to have intuition. . Trick: for image similarity, if we run similarity functions over the image activations directly it can be too large. A better idea is to run PCA on image activations and then use the reduced dimensions to run similarity! Weight Decay wd in learner . Weight decay is a way of regularization, it controls the model complexity. It is the coefficient of the sum of squares of parameters, effectively reduce the complexity by making unnecessary parameters smaller. The value of weight decay wd is usually 1e-1 (0.1). This is the result of a lot of experiments on different dataset. . . . . Weight decay wd is a parameter for all learners in fastai, even if you don’t see it in the signature of a particular learner, it is there because it’s in the parent learner class. The default value is 0.01 and not 0.1 because in rare cases, too big a wd caps the model performance, but too small a wd just makes the model easy to overfit and doesn’t cap the performance, the solution is to stop early. . Reason for having weight decay . We don’t want too much complexity, but we DO want many parameters to capture the potential curvy bits of reality. The answer is to have many parameters but penalize their values. . . Note: complexity != # parameters. Because some parameters can be very small and close to 0, even though we have many of them, they are barely there which means not much complexity. Entity Embeddings of Categorical Variables . Paper. Used NN and entity embedding layers for tabular data and got great results. . . MAPE: mean average percentage error. SGD with MNIST . Subclassing is very very common in pytorch. Override the constructor. . class Mnist_Logistic(nn.Module): def __init__(self): super().__init__() self.lin = nn.Linear(784, 10, bias=True) # xb means a minibatch of x def forward(self, xb): return self.lin(xb) . Then create the logistic regression model, . # Create the model and put it on GPU model = Mnist_Logistic().cuda() model # Mnist_Logistic( # (lin): Linear(in_features=784, out_features=10, bias=True) # ) model.lin # Linear(in_features=784, out_features=10, bias=True) model(x).shape # torch.Size([64, 10]) [p.shape for p in model.parameters()] # [torch.Size([10, 784]), torch.Size([10])] # means it takes 784 dim input and output 10 dim, # then there&#39;s the 10d bias . The SGD update with weight decay, . lr=2e-2 loss_func = nn.CrossEntropyLoss() def update(x,y,lr): wd = 1e-5 y_hat = model(x) # weight decay w2 = 0. for p in model.parameters(): w2 += (p**2).sum() # add to regular loss # THIS IS L2 REGULARIZATION! loss = loss_func(y_hat, y) + w2*wd loss.backward() with torch.no_grad(): for p in model.parameters(): p.sub_(lr * p.grad) p.grad.zero_() # Tensor.item() -&gt; a normal python number return loss.item() losses = [update(x,y,lr) for x,y in data.train_dl] . In this case, weight decay is equivalent to L2 regularization because the gradient of the regularization term wd * w^2 gives 2 wd * w. . . Later there&#39;s a case where weight decay != L2 regularization!! Weight decay helps prevent overfitting, so . we can have a giant model without overfitting | or we can have a smaller dataset | . Too much weight decay can also hurt training. . The above is a logistic regression by pytorch from scratch. Here we make an NN with pytorch from scratch. . class Mnist_NN(nn.Module): def __init__(self): super().__init__() self.lin1 = nn.Linear(784, 50, bias=True) self.lin2 = nn.Linear(50, 10, bias=True) def forward(self, xb): x = self.lin1(xb) x = F.relu(x) return self.lin2(x) model = Mnist_NN().cuda() def update(x,y,lr): # Instead of SGD we can use `optim.Adam` opt = optim.Adam(model.parameters(), lr) y_hat = model(x) loss = loss_func(y_hat, y) loss.backward() opt.step() opt.zero_grad() return loss.item() losses = [update(x,y,1e-3) for x,y in data.train_dl] . Optimizer: SGD with Momentum . Apply exponentially weighted moving average on the last several derivatives (steps). The more recent steps are exponentially higher weighted. . . . . Alpha is the momentum, S are the steps, g is the gradient. . To use it in pytorch, just use optim.SGD with a momentum param. Set it to 0.9. . Optimizer: RMSprop . Geoff Hinton first mentioned RMSprop in his Coursera NN course! That is the correct way to cite it. . It’s very similar to SGD with momentum. It uses squared gradient in the denominator. If it’s small the step will be big. . If the gradient is consistently small, it will be a small number. If gradient is volatile or consistently big, it will be a big number. This is because if the gradient is always small we need to take bigger steps. . Optimizer: Adam . Adam is Momentum + RMSprop. It keeps track of the exponentially weighted moving average and the squared gradient. . We still need to set the learning rate, and do learning rate annealing! . In fastai, use learner, and it sets the optimizer (Adam or a slight variation of Adam by default) and you don’t need to worry. . fit_one_cycle helps you get “super convergence”, i.e. train 10x faster than plain SGD. It has smaller lr and large momentum in the beginning, and then lr increases, momentum decreases. When it’s close to optimum, lr decreases again and momentum increases. . . This is research result by Leslie Smith. . Cross-Entropy Loss . If we have a classifier for Cats and Dogs . Cat Dog Pred(Cat) Pred(Dog) X-entropy Comment . 1 | 0 | 0.5 | 0.5 | 0.3 | unsure | . 1 | 0 | 0.98 | 0.02 | 0.01 | confident, right | . 0 | 1 | 0.9 | 0.1 | 1 | confident, wrong | . 0 | 1 | 0.5 | 0.5 | 0.3 | unsure | . 1 | 0 | 0.9 | 0.1 | 0.05 | confident, right | . The unsure cases have moderate loss, confident and right has the lowest loss, confident but wrong has the highest loss. . The cross-entropy formula is . -y log(y_hat) - (1-y) log(1-y_hat) . Keep in mind that now Cat is 1. This basically means, . If Cat (y=1), then look at log of Pred(Cat); . If Dog (y=0), then look at log of 1-Pred(Cat), i.e. Pred(Dog); . MUST make sure the preds to plug into cross-entropy add to one. To make sure of that, use the softmax activation. . For multi-class classification, use cross-entropy as loss and softmax as activation. . . Note: in PyTorch, calling nn.CrossEntropyLoss() actually calculates the softmax behind the scene so there is no need to add a softmax layer manually. If you use a custom loss and want softmax output, make sure to add the softmax layer at the end. . . Note: Regularization include weight decay, batch norm, dropout",
            "url": "http://blog.logancyang.com/note/fastai/2020/04/19/fastai-lesson5.html",
            "relUrl": "/note/fastai/2020/04/19/fastai-lesson5.html",
            "date": " • Apr 19, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "FastAI Lesson 4: NLP; Tabular Data; Collaborative Filtering and Embeddings",
            "content": "NLP Continued from lesson 3 . 2018 was the year that transfer learning started working well with NLP. You train a language model on a very large dataset to “learn to speak English”, and then do transfer learning for other purposes such as text classification, NER, etc. . Language model on all of Wikipedia (Wikitext 103, ~1B tokens) ==&gt; Fine-tune this language model using your target corpus (in this case, IMDb movie reviews) ==&gt; Extract the encoder from this fine tuned language model, and pair it with a classifier. Then fine-tune this model for the final classification task (in this case, sentiment analysis) . We would think that an LM trained on Wikipedia wouldn’t work well for slangs and informal language, but actually when it gets finetuned with your target corpus, it works. . For the imdb notebook, the sample data has columns label, text, is_valid (in validation set). . For tokenization, most often a token is a word, sometimes it can be &#39;s or a punctuation or symbol. . The full unique set of tokens is vocabulary. Here a limit of 60K tokens and a frequency threshold of 2 are applied to the vocab. . xxunk means unknown token, it means that word was not common enough to be in the vocab. . # Using the data block API is the better way to create the DataBunch data = (TextList.from_csv(path, &#39;texts.csv&#39;, cols=&#39;text&#39;) # how to split out validation set based on bool column .split_from_df(col=2) # which column is the label .label_from_df(cols=0) .databunch()) . The reviews are in a training and test set following an imagenet structure. The only difference is that there is an unsup folder on top of train and test that contains the unlabelled data. . Training an LM on Wiki data takes 2-3 days on a decent GPU, no need to do that, just download the pretrained model. . Even if we have a large target corpus, we still prefer to start from the pretrained model on wikitext-103, there is no reason to start from random. . We are going to use that ‘knowledge’ of the English language to build our classifier, but first, like for computer vision, we need to fine-tune the pretrained model to our particular dataset. Because the English of the reviews left by people on IMDB isn’t the same as the English of wikipedia, we’ll need to adjust the parameters of our model by a little bit. Plus there might be some words that would be extremely common in the reviews dataset but would be barely present in wikipedia, and therefore might not be part of the vocabulary the model was trained on. . This is where the unlabelled data is going to be useful to us, as we can use it to fine-tune our model. Let’s create our data object with the data block API (next line takes a few minutes). . . Trick: make sure to train the LM on all of the data including the test set, because it doesn&#39;t matter, there is no label for LM. #Inputs: all the text files in path data_lm = (TextList.from_folder(path) #We may have other temp folders that contain text files so # we only keep what&#39;s in train and test .filter_by_folder(include=[&#39;train&#39;, &#39;test&#39;, &#39;unsup&#39;]) #We randomly split and keep 10% (10,000 reviews) for validation .split_by_rand_pct(0.1) #We want to do a language model so we label accordingly .label_for_lm() .databunch(bs=bs)) data_lm.save(&#39;data_lm.pkl&#39;) data_lm = load_data(path, &#39;data_lm.pkl&#39;, bs=bs) data_lm.show_batch() # This is an RNN learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3) #Downloading https://s3.amazonaws.com/fast-ai-modelzoo/wt103-fwd . It takes very long to even train one epoch on an average GPU. Mine took ~20min for 1 epoch before unfreeze. It can easily take overnight to train a good model. . For LM on Wikipedia, ~30% accuracy is quite good. For more specific documents like medical or legal, it can be higher. . After training the LM on the target corpus, we save the encoder . learn.save_encoder(&#39;fine_tuned_enc&#39;) . Next, we can build the classifier. . #grab all the text files in path, MUST use the same vocab and order data_clas = (TextList.from_folder(path, vocab=data_lm.vocab) #split by train and valid folder (that only keeps &#39;train&#39; and #&#39;test&#39; so no need to filter) .split_by_folder(valid=&#39;test&#39;) #label them all with their folders .label_from_folder(classes=[&#39;neg&#39;, &#39;pos&#39;]) .databunch(bs=bs)) data_clas.save(&#39;data_clas.pkl&#39;) learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5) learn.load_encoder(&#39;fine_tuned_enc&#39;) learn.lr_find() learn.recorder.plot() # moms is the momentum for the optimizer learn.fit_one_cycle(1, 2e-2, moms=(0.8,0.7)) learn.save(&#39;first&#39;) learn.load(&#39;first&#39;) # NOTE: This only unfreezes the last 2 layers learn.freeze_to(-2) # NOTE: The 2.6**4 stuff is called discriminative learning rate # the range controls the lr for different layers since they learn # best at different rate learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2), moms=(0.8,0.7)) learn.save(&#39;second&#39;) learn.load(&#39;second&#39;) # Unfreeze last 3 layers learn.freeze_to(-3) learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3), moms=(0.8,0.7)) learn.save(&#39;third&#39;) learn.load(&#39;third&#39;) # Unfreeze whole model to finetune learn.unfreeze() learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7)) learn.predict(&quot;I really loved that movie, it was awesome!&quot;) . NOTE: When creating the classifier learner, we MUST have the same vocab as the pretrained language model. The line . data_clas = (TextList.from_folder(path, vocab=data_lm.vocab) ... . is very important. Have the right data_lm in memory and create data_clas with its vocab, or there will be an error when loading the pretrained model via learn.load_encoder(&#39;fine_tuned_enc&#39;) and it will say . Error(s) in loading state_dict for AWD_LSTM: size mismatch... . . Trick: for text classification, unfreezing one layer at a time and train some more is an effective strategy. . Trick: Jeremy created a random forest to find best hyperparameter setting. The best number for discriminative learning rate is 2.6^4. This is similar to AutoML for hyperparam search. For training Chinese language models, search the forum for more info. . Tabular Data using Deep Learning, Embeddings for Categorical Variables . . People were skeptical about using neural nets on tabular data, they often use logistic regression, random forest, gradient boosting machines to do it. In fact, NN is extremely useful for tabular data. . With NN, you don’t need to hand engineer features as much as before. It’s more accurate and requires less maintenance. Jemery used to use Random Forest 99% of the time for tabular data, now he uses NN 90% of the time. . Nobody else created a library for NN on tabular data, fastai has fastai.tabular. In the notebook lesson4-tabular there is a detailed example. . It assumes the data is in a pandas dataframe. Pandas can read from csv, relational db, Spark and Hadoop. . The independent variables (features) can be continuous or categorical. With NN, we use embeddings for categorical variables. . Instead of having “transform”s as in CV such as brightening, flipping, normalization etc., we have “processor”s for tabular data. The difference is that transforms are for data augmentation and are different each time, but processors are run once ahead of time. . dep_var = &#39;salary&#39; cat_names = [ &#39;workclass&#39;, &#39;education&#39;, &#39;marital-status&#39;, &#39;occupation&#39;, &#39;relationship&#39;, &#39;race&#39; ] cont_names = [&#39;age&#39;, &#39;fnlwgt&#39;, &#39;education-num&#39;] # 1. Deal with missing values in some way # 2. Use pandas categorical variables # 3. Normalize continuous variables by mean 0 and std 1 procs = [FillMissing, Categorify, Normalize] data = (TabularList.from_df( df, path=path, cat_names=cat_names, cont_names=cont_names, procs=procs) # when split validation set, must have contiguous indices # think time periods, video frames, or other structure in data .split_by_idx(list(range(800, 1000))) .label_from_df(cols=dep_var) .add_test(test) .databunch() ) data.show_batch(rows=10) learn = tabular_learner(data, layers=[200, 100], metrics=accuracy) learn.fit(1, 1e-2) # Inference row = df.iloc[0] learn.predict(row) . For time series tabular data, you generally don’t use RNN for them. Instead, you can time box them into day_of_week, time_of_day, etc. and it will give you state of the art result. . Collaborative Filtering and Embeddings . When you have data about who-bought-what, who-liked-what, you can have two columns like [userId, productId] in the most basic form. Other metadata can be added, like timestamp, review, etc. . This matrix is very sparse because most users didn’t buy most products / watched most movies. . In this example our data has userId, movieId, rating, timestamp. . ratings = pd.read_csv(path/&#39;ratings.csv&#39;) data = CollabDataBunch.from_df(ratings, seed=42) y_range = [0,5.5] learn = collab_learner(data, n_factors=50, y_range=y_range) . For recommender systems, a big challenge is the Cold Start problem. It means that we particularly care about recommending new movies or recommend relevant movies to new users which we don’t have any data for. The solution is to have a second model on user or movie metadata to quatify the similarities. . Netflix fixed the cold start problem by UX. It asks a new user whether they like the movies they show as a survey. For new movies, they just need to let some hundreds of people watch it and rate them. It wasn’t quite a cold start problem for Netflix. . But for selling products, you might not want people to look at your range of products. You could for example find the metadata of the users such as what geography they are from, their age, gender, and other features to predict whether they would like something. . Collaborative filtering is specifically for when you already have some data about the preferences of the users. . A user has an embedding vector. A movie has an embedding vector. A bias term needs to be added in the user embedding and can be interpretted as the user’s tendency to like movies in general regardless of what movie. Similarly, a bias term in the movie embedding is like the likeability of a movie regardless of users. . The target value is the rating in the range 0 to 5. We dot the user embedding and the movie (item) embedding along with the weights, and pass it through a sigmoid (and times 5) to get a numbder between 0 - 5. Notice that this is actually a “logistic regression” (linear layer on inputs and a sigmoid) but with MSE loss and target variables as numbers between 0 - 5. . . Question: Sigmoid with MSE is interesting, is there a maximum likelihood explanation for this? Note that this mapping from the product of embeddings to the range [0, 5] is still regression and not classification, so the loss used is MSE and not cross entropy. . Why pass through the sigmoid? It makes the model learn easier and let the weights converge to relevant results. It is very common to use sigmoid or softmax as the last layer to produce the output. . . Question: it seems this is a sigmoid(linear model) with MSE, the optimization is nonconvex. How is it done?",
            "url": "http://blog.logancyang.com/note/fastai/2020/04/13/fastai-lesson4.html",
            "relUrl": "/note/fastai/2020/04/13/fastai-lesson4.html",
            "date": " • Apr 13, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "FastAI Lesson 3: Data blocks; Multi-label classification; Segmentation",
            "content": "Must know: Dataset class in PyTorch . class Dataset(object): &quot;&quot;&quot;An abstract class representing a dataset All other datasets should subclass it and override these methods &quot;&quot;&quot; def __getitem__(self, index): &quot;&quot;&quot;Allow [] indexing&quot;&quot;&quot; raise NotImplementedError def __len__(self): raise NotImplementedError . . Note: Pythonistas call special magic methods __xxx__(): &quot;dunder&quot; xxx. PyTorch has another class called the DataLoader for making minibatches. Then, fastai’s DataBunch uses DataLoaders to create a training DataLoader and a validation DataLoader. fastai has the data block API to customize the creation of DataBunch by isolating the underlying parts of that process in separate blocks, mainly: . Where are the inputs and how to create them? | How to split the data into a training and validation sets? | How to label the inputs? | What transforms to apply? | How to add a test set? | How to wrap in dataloaders and create the DataBunch? | Check the fastai docs for function signatures and types. . To find the corresponding notebooks for the docs, go to the fastai repo . https://github.com/fastai/fastai/tree/master/docs_src/ . For multi-label image classification such as this one, to put this in a DataBunch while using the data block API, we need to use ImageList (and not ImageDataBunch). This will make sure the model created has the proper loss function to deal with the multiple classes. . # This does image data augmentation by flipping them horizontally by default. # Here we enable vertical flipping as well so it rotates every 90 degrees left and right, so 8 possible settings. # warp: fastai has fast perspective warping. For satellite image we don&#39;t need warping tfms = get_transforms(flip_vert=True, max_lighting=0.1, max_zoom=1.05, max_warp=0.) . We often want to call the same function but with different values of a parameter. For example, . # We want to call with different thresh def acc_02(inp, targ): return accuracy_thresh(inp, targ, thresh=0.2) # Equivalent: a CS concept called &quot;partial&quot; or partial function application, # pass in the original function and the param, returns a new wrapper function (py3) acc_02 = partial(accuracy_thresh, thresh=0.2) . This is really common thing to do! . Question: How to use online feedback to retrain model? Answer: Add the labeled new data into the training set, load the old model, unfreeze, use a slightly larger learning rate and more epochs, train (fine-tune) the model some more. . Before unfreeze, we train the model’s last layer. The learning rate should look like this. . . Note that do not set the learning rate at the bottom, set it at the steepest place. . After unfreeze, we finetune the full model. The learning rate should look like this. . . For this shape, we find where it starts to go up, and set it to be 10x smaller than that point as the left of the lr range, and the old learning rate for the frozen model divided by 5 or 10. Will talk about this (called discriminative learning rate) in the future. . In the notebook example for the planet data challenge, Jeremy first trained a model on size 128 by 128 image. This is for faster experimentation and can be used as a pretrained model for the actual 256 by 256 image next. . The process is to create the DataBunch with new size . learn.load(&#39;stage-2-rn50&#39;) # Note, set bs=32 and restart kernel after loading the saved model, # or GPU can run out of memory data = (src.transform(tfms, size=256) .databunch(bs=32).normalize(imagenet_stats)) learn.data = data data.train_ds[0][0].shape # Output: torch.Size([3, 256, 256]) # Freeze means we go back to train the last few layers for transfer learning learn.freeze() learn.lr_find() learn.recorder.plot() # Check the image below for output lr=1e-2/2 learn.fit_one_cycle(5, slice(lr)) learn.save(&#39;stage-1-256-rn50&#39;) learn.unfreeze() learn.fit_one_cycle(5, slice(1e-5, lr/5)) learn.recorder.plot_losses() # Note: save() is used for stages. Data used is also saved learn.save(&#39;stage-2-256-rn50&#39;) # export() returns a pickle file for inference. It saves all # transforms, weights but not data. # Check https://docs.fast.ai/tutorial.inference.html learn.export() . . New Task: Segmentation . Example: . . In segmentation, every pixel needs to be classified. . The training data needs to have images with all pixels labeled. It’s hard to create such datasets, so usually we download them. . Every time we use the datasets, we should find the citation and credit the creators appropriately. . Question: what to do if training loss &gt; validation loss Answer: This means underfitting. Try 1. Train more epochs 2. Smaller learning rate 3. Decrease regularization: weight decay, dropout, data augmentation . The model for segmentation used is U-Net . . Note: what does fit_one_cycle() do? fit_one_cycle helps you get &quot;super convergence&quot;, i.e. train 10x faster than plain SGD. It has smaller lr and large momentum in the beginning, and then lr increases, momentum decreases. When it&#39;s close to optimum, lr decreases again and momentum increases. Paper by Leslie Smith. Towards Data Science article here. You pass in the max learning rate, and it uses a range of learning rates as the picture shows below, it goes up first and down after. The downward part is called annealing which is well known, but the upward part is quite new. The motivation is to avoid the optimization being stuck in a local minimum. The loss surface is usually quite bumpy at some areas and flat in other areas. . . The approach was proposed by Leslie Smith. Read about it more here. . The fastai version of unet is better than the state-of-the-art result published which is a model called hundred-layer-tiramisu! . . Trick: if GPU memory runs out very frequently, use half-precision (16-bit) rather than single-precision (32-bit) float in training. Just add .to_fp16() to any learner. learn = unet_learner( data, models.resnet34, metrics=metrics).to_fp16() . Make sure the GPU driver is update-to-date to use this feature. . Head Pose Estimation: A Regression Task . This is a regression task and the output is a set of (x, y) coordinates. We train a CNN. Instead of using a cross-entropy loss, use MSE. . Preview next lesson: IMDB Review Sentiment, an NLP Task . For texts, we create TextDataBunch from csv. Texts need to be tokenized and numericalized. . When we do text classifications, we actually create 2 models: one is a language model (pretrain, for transfer learning later), the other is a classification model. . The SOTA accuracy for this dataset is ~95% and this notebook achieves that level. . . Note: in deep learning, we don&#39;t care about n-grams, that&#39;s for old time NLP&#39;s feature engineering. Extra: Jeremy mentioned activations and pointed to one great resource . A Visual Proof that NN can approximate any shape, or, universal approximation theorem: . http://neuralnetworksanddeeplearning.com/chap4.html | . (This is an online book, need to check it out!) . What really is deep learning from a math perspective: It&#39;s a series of matrix multiplications with max(x, 0) (ReLU) in between, and we use gradient descent to adjust the weights in these matrices to reduce the final error. The forward pass is something like E = loss_func(W3.dot( max(W2.dot( max( W1.dot(X), 0)), 0), 0)) The only thing is the matrices are large. That&#39;s it. . Usually, the hardest part is to create the DataBunch, the rest is straightforward in fastai. .",
            "url": "http://blog.logancyang.com/note/fastai/2020/04/11/fastai-lesson3.html",
            "relUrl": "/note/fastai/2020/04/11/fastai-lesson3.html",
            "date": " • Apr 11, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "FastAI Lesson 2: Data Cleaning and Production. SGD from Scratch",
            "content": "Lesson 2 Data Cleaning and Production. SGD from Scratch . The notebook “Lesson 2 Download” has code for downloading images from Google images search result in parallel. It is a great way to create my own image dataset. . . Note: ImageDataBunch can create validation set with valid_pct. ALWAYS SET RANDOM SEED BEFORE CREATING THE VALIDATION SET. We need the same validation set for each run. Other times, randomness is good because we need to know the model runs stably with randomness. . DL models are pretty good at randomly noisy data. But “biased noisy” data is not good. . # Fastai way of opening an image for inspection img = open_image(path/&#39;black&#39;/&#39;00000021.jpg&#39;) . Facilitate experiments with notebook GUI widgets . There is an in-notebook GUI widget called fastai.widgets.ImageCleaner for cleaning the dataset. It has buttons to remove problematic examples and creates a new cleaned.csv from where you can create a new ImageDataBunch with the corrected labels to continue training your model. . These things are called ipywidgets. . Put models in production . Inference on CPU is good enough for the vast majority of use cases. Inference on GPU is a big hassle, you have to deal with batching and queueing, etc. Unless the website has extremely high traffic, inferece on CPU is a much better choice, and it can be horizontally scaled easily. . defaults.device = torch.device(&#39;cpu&#39;) # Open image for inspection img = open_image(path/&#39;black&#39;/&#39;00000021.jpg&#39;) img . The code below loads the trained model and is run at web app starting time once, should run fairly quickly. . # We create our Learner in production enviromnent like this, # just make sure that path contains the file &#39;export.pkl&#39; from before learn = load_learner(path) . Then we can do inference in the web application, . # inference pred_class, pred_idx, outputs = learn.predict(img) pred_class . The endpoint should look like (this example is for Starlette app) . @app.route(&quot;/classify-url&quot;, methods=[&quot;GET&quot;]) async def classify_url(request): bytes = await get_bytes(request.query_params[&quot;url&quot;]) img = open_image(BytesIO(bytes)) _,_,losses = learner.predict(img) return JSONResponse({ &quot;predictions&quot;: sorted( zip(cat_learner.data.classes, map(float, losses)), key=lambda p: p[1], reverse=True ) }) . . Note: **Starlette** is similar to Flask but supports modern async and await for Python 3. **FastAPI** is another option. Common Training Problems . Learning rate | Number of epochs | . If validation loss explodes, learning rate is too high. Decrease the learning rate. . If training loss is higher than validation loss, it means underfitting, either learning rate is too low or number of epochs too low. Try higher learning rate or more epochs. . If the error rate goes down and up again, it’s probably overfitting. But it’s pretty hard to overfit with the settings in fastai here. In this case, if the learning rate is already good and the model is trained a long time but we are still not satisfied with the error rate (note fastai error rate is always validation error), it can mean that we need more data. There is no shortcut to know how much data we need in advance. But sometimes we don’t need that much data we thought we needed. . . Note: Some say if training loss is lower than validation loss, it&#39;s overfitting. IT IS NOT TRUE!! Any model that&#39;s trained correctly always have training loss lower than validation loss. That is right.. . Note: unbalanced classes is often NOT a problem, it just works. Jeremy spent years experimenting and thinks it&#39;s not a problem, no need to do oversampling for less common classes, it just works. Rule of thumb for training flow and setting learning rate . default_lr = 3e-3 learn.fit_one_cycle(4, default_lr) learn.unfreeze() learn.fit_one_cycle(4, slice(steepest_pt_in_plot, default_lr/10)) . In Lesson 2 SGD notebook, we generate some data points for a linear regression. . n=100 # tensor n by 2, all ones x = torch.ones(n,2) # In PyTorch, any function that ends with _ means no return and modify in-place! # With ., int becomes float x[:,0].uniform_(-1.,1) x[:5] a = tensor(3.,2) # Note in Python &#39;@&#39; is matmult. It&#39;s more general tensor product in pytorch y = x@a + torch.rand(n) . The above is 95% we need to know about PyTorch . create a tensor | update a tensor | tensor product | . . Note: The `rank` of a tensor is the number of dimensions, or axes. An RGB image is a rank 3 tensor. A vector is a rank 1 tensor. Tensor is just high dimensional arrays. . Trick: for matplotlib animations, set rc(&#39;animation&#39;, html=&#39;jshtml&#39;) instead of %matplotlib notebook. Vocab . learning rate: step size to multiply gradient by | epoch: one complete run on all data. A batch size of 100 for 1000 datapoints means 1 epoch = 10 iterations. In genral we don’t want to do too many epochs, it lets the model see the same data too many times and overfit | minibatch: random bunch of data points to do weight update | SGD: gradient descent using minibatches | Model architecture: the math function to fit the data with | Parameters: or weights or coefficients. The knobs to tune for a model | Loss function: how far away to the target | . Homework . Make logistic regression optimization animation and blog post | Make a minimal web app for a model using FastAPI | .",
            "url": "http://blog.logancyang.com/note/fastai/2020/04/08/fastai-lesson2.html",
            "relUrl": "/note/fastai/2020/04/08/fastai-lesson2.html",
            "date": " • Apr 8, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "FastAI Lesson 1: Pets",
            "content": "Lesson 1 . # Shows the module imported from, function signature with types # such as url:str, fname:Union[pathlib.Path, str]=None # Union[...] means it can be any of the type in this list help(func) # python3 path object can be used like this path_img = path/&#39;image&#39; # ImageDataBunch in fastai has a factory method from_name_re() to use regex to get the data and labels # Normalize to square image, size 224 is very commonly used data = ImageDataBunch.from_name_re( path_img, fnames, pat, ds_tfms=get_transforms(), size=224, bs=bs ).normalize(imagenet_stats) . A DataBunch object in fastai contains everything about data. It has training, validation, test data, all the labels. . # normalize the data in the DataBunch object to 0 mean 1 std # in this case, RGB channels with be 0 mean and 1 std data.normalize(imagenet_stats) # Always take a look at the data data.show_batch(rows=3, figsize=(7,6)) print(data.classes) # data.c is the number of classes len(data.classes), data.c . Learner in fastai is a model object, it takes in the DataBunch object and model architecture. It also let you specify the metric to look at for training. . # Note that models.resnet34 is pretrained on imagenet # error_rate is for validation set, because creating a DataBunch object automatically creates the validation set learn = cnn_learner(data, models.resnet34, metrics=error_rate) learn.model # fit_one_cycle() is much better than fit() (paper in 2018), always use it. # 4 is the # epoch to run, it&#39;s a good place to start learn.fit_one_cycle(4) # This can saves the model trained on different DataBunch to different places learn.save(&#39;stage-1&#39;) . To interpret the output of a learner, . # ClassificationInterpretation has a factory method from_learner, # it takes in a learner object and returns a ClassificationInterpretation object interp = ClassificationInterpretation.from_learner(learn) losses,idxs = interp.top_losses() len(data.valid_ds)==len(losses)==len(idxs) # This is the most useful tool, it plots the examples the model got wrong # with biggest losses, and show titles as # `predicted / actual / score for predicted class / score for actual class` interp.plot_top_losses(9, figsize=(15,11)) # Use doc() to check how to use, click `show in doc` to documentation website for more details and source code doc(interp.plot_top_losses) # Another useful tool, confusion matrix interp.plot_confusion_matrix(figsize=(12,12), dpi=60) # Yet another, the most useful, prints the most wrong ones (predicted, actual, # times) interp.most_confused(min_val=2) . Previously we called fit_one_cycle(4), it only trains the last layer!! The whole model is frozen by default! . . Note: transfer learning is always a two-step process, train the last layer for some epochs first, and then unfreeze the whole model for some more training/fine-tuning # After training the last layer, we unfreeze the whole model learn.unfreeze() . When we unfreeze the whole model and retrain, the error rate is actually worse than before. . Now, load the stage 1 model back on again, we need lr_find to find a good learning rate. The default learning rate is something like 0.003. We need to change it. . The learning rate is the most important hyperparameter. . Use learn.recorder.plot() to plot learning rate vs loss graph and set the learning rate range to the steepest downward slope. . Next, unfreeze the model and use that learning rate . learn.unfreeze() # Pass a range of learning rate from 1e-6 to 1e-4 # means first layers use 1e-6 and last layers use 1e-4 # Good rule of thumb, set the right side of slice to be 10x smaller # than default lr, and left to be at least 10x smaller than the lowest # point in the lr vs loss graph learn.fit_one_cycle(2, max_lr=slice(1e-6,1e-4)) . Now it gets better than stage 1! . With these two stages, the model can be already really good. It is at least average Kaggle practitioner level. . Next, we can use a bigger model - resnet50. . If GPU runs out of memory, use smaller batch size. It can be set in the DataBunch creation. . Again, the two-stage workflow . data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(), size=299, bs=bs//2).normalize(imagenet_stats) learn = cnn_learner(data, models.resnet50, metrics=error_rate) # Stage 1 learn.fit_one_cycle(8) learn.save(&#39;stage-1-50&#39;) # Stage 2 learn.unfreeze() learn.lr_find() learn.recorder.plot() # Pick learning rate range around the steepest downward slope learn.fit_one_cycle(3, max_lr=slice(1e-6,1e-4)) # Evaluation interp = ClassificationInterpretation.from_learner(learn) interp.most_confused(min_val=2) . . For an imagenet-style dataset, it has data in folders named with the labels. MNIST for example, has images for 3 in a folder named 3. Use ImageDataBunch.from_folder(...) | For a csv file with columns [filepath, label], use ImageDataBunch.from_csv(...) | For data where labels are in the filename, use regex and ImageDataBunch.from_name_re(...) | For more complex cases, construct any function and pass into ImageDataBunch.from_name_func(...) | What’s really cool is that the documentation of fastai is actually a collection of Jupyter Notebooks with working example code! . Success story: a fastai alumnus created a security anomaly detection software by using the exact code for this lesson on mouse trajectories. . There are examples where problems can be converted into an image problem and apply CNN. Such as audio to spectral form. . Homework 1 . Use my own dataset to train an image classifier. .",
            "url": "http://blog.logancyang.com/note/fastai/2020/04/07/fastai-lesson1.html",
            "relUrl": "/note/fastai/2020/04/07/fastai-lesson1.html",
            "date": " • Apr 7, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Nature of Code Notes Part I: Creating Physics Engine",
            "content": "Topics in Nature of Code . Part I: Creating Physics Engine . Vectors | Forces | Oscillations | Particle Systems Inheritance | Polymorphism | . | Box2D | Steering Forces | . Part II: Complexity . Flocking | Cellular Automata | Fractals | . Part III: Intelligence . Evolution | Neural Networks | . 1. Introduction: Random Walker, Gaussian, Custom Distributions, Perlin Noise . For a function that yields a standard Gaussian distribution (mean = 0, std = 1), we just add our mean and multiply by our stddev to everything the standard Gaussian yields. . random() gives us uniform distribution. . To get a custom distribution, there are 2 main approaches. . The “bucket” approach | . Draw from [0, 0, 0, 0, 1], we have 80% chance picking 0. . 2-number approach (Rejection Sampling: a Monte Carlo Method) | . let vals, norms; let width, height; let drawLoop = 0; function monteCarlo() { let foundOne = false; let iter = 0; let r1, r2; while (!foundOne &amp;&amp; iter &lt; 10000) { r1 = random(1); r2 = random(1); // target function: y = x^2 target_y = r1 * r1; if (r2 &lt; target_y) { foundOne = true; return r1; } iter++; } // If there&#39;s a problem, not found return 0; } function setup() { width = 600; height = 600; canvas = createCanvas(width, height); canvas.position(10, 10); canvas.style(&quot;outline&quot;, &quot;black 3px solid&quot;); vals = Array(width).fill(0); norms = Array(width).fill(0); } function draw() { background(255); stroke(148,0,211); strokeWeight(4); // Draw a sample between (0, 1) sampleNumber = monteCarlo(); bin = int(sampleNumber * width); vals[bin] += 1 * 10; let normalization = false; maxBinCount = 0; for (let x = 0; x &lt; vals.length; x++) { line(x, height, x, height-norms[x]); if (vals[x] &gt; height) { normalization = true; } if (vals[x] &gt; maxBinCount) maxBinCount = vals[x]; } for (let x = 0; x &lt; vals.length; x++) { if (normalization) norms[x] = vals[x] / maxBinCount * height; else norms[x] = vals[x]; } textSize(24); noStroke(); text(`Monte Carlo Iteration: ${drawLoop}`, 50, 50); drawLoop++; if (drawLoop &gt; 5000) { text(&quot;Done!&quot;, 50, 100); noLoop(); } } . Dan Shiffman called this approach the “2-number approach”. I found on Wikipedia that it is actually one method under the umbrella of Monte Carlo simulations called Rejection Sampling. In that article, there is a visual description that is helpful to understand why this works: . To visualize the motivation behind rejection sampling, imagine graphing the density function of a random variable onto a large rectangular board and throwing darts at it. Assume that the darts are uniformly distributed around the board. Now remove all of the darts that are outside the area under the curve. The remaining darts will be distributed uniformly within the area under the curve, and the x-positions of these darts will be distributed according to the random variable&#39;s density. This is because there is the most room for the darts to land where the curve is highest and thus the probability density is greatest. . This is a method for simulating a custom continuous distribution. . TODO: Make this visualization along side the bar chart distribution in a synchronous way, put it on my website learning-automata.co. . Additional note: in pseudo-random number sampling, there is a method to generate discrete random variables. The method is to use CDF. For example, r.v. X = 0, 1, 2. P(X) = 0.2, 0.7, 0.1 accordingly. Then divide [0, 1) into . Uniformly draw from [0, 1), return 0, 1, 2 depending on which interval it falls into. 0.2 0.9 1 |========|============================|====| return: 0 1 2 . Perlin Noise . Perlin Noise is developed by Prof. Perlin at NYU in the 80s. On a high level, it is a technique to make smoothness and achieve natural looking motions or textures. Will use it more in the future. . 2. Vectors and Forces . In Processing, there is a class PVector. In P5.js, the class is p5.Vector and can be created using . let someVector = createVector(some_x, some_y); . The class has vector math methods such as add(), dot(), cross(), mag(), magSq(), dist(), rotate(), angleBetween(), etc. . The constructor takes 2 or 3 arguments, depending on 2D or 3D. . Static method for PVector in Processing (Java) . PVector f = new PVector(0, 1); float mass = 2; // If we want to calculate acceleration, we need A = f/mass // But we can&#39;t do it directly passing in the f object, because // it will be updated in-place. We need static function in the // PVector class to make a copy of f PVector a = PVector.div(f, mass); . Applying force with draw() . Since it’s sometimes unnecessary to keep track time or # draw loops in a sketch, we can re-apply force every frame. In this case, DO NOT forget to set acceleration to 0 (mult 0) after every frame udpate! . // Inside the Mover class void update() { vel.add(acc); location.add(vel); // Note, reset acc acc.mult(0); } . If we do choose to apply the force with a parameter time, then we don’t have to do it every frame. But that can be a rare use case. . Takeaway, location and velocity are cumulative between frames, but always calculate force fresh every frame! . Simulate friction . friction = - mu * || N || * vel_hat . where ||N|| is the magnitude of the normal force from the surface, and vel_hat the unit velocity vector. . Don’t forget when calculating the friction, copy the velocity vector and do not change it in-place. . 3. Oscillations . Rotation in Processing, . // PI = 180 degrees, this is 45 degrees float angle = PI/4; float aVelocity = 0; float aAcceleration = 0.0001; void setup() { size(800, 200); smooth(); } void draw() { background(255); fill(127); stroke(0); // Note that `rotate` has center at origin (0, 0), // aka top left corner of the canvas, we need translate to // make it centered translate(width/2, height/2); rectMode(CENTER); rotate(); rect(0, 0, 64, 36); angle += aVelocity; aVelocity += aAcceleration; } . Polar coordinate . x = r * cos(a); y = r * sin(a); . Simple harmonic motion . // Say period = 200, it means 200 frames are one period float x = amplitude * sin((frameCount / period) * TWO_PI); // But really we can just use one &quot;angle&quot; variable in sin() float x = amplitude * sin(angle); . Springs . // Bob: Mover class object class Mover { PVector position; PVector velocity; PVector acceleration; void applyForce() {}; } class Spring { float k; float restLength; // If we need moving anchor, we can have it as a Mover object PVector anchor; // This is powerful, Spring directly modifies Mover object void connect(Mover m) { // Calculates displacement, force float force = ...; // and then apply the force to m m.applyForce(force); }; } . It’s good to use physics engine libraries to simulate complex spring systems. . 4. Particle Systems . Java ArrayList: add(), get(), remove(), size() . ArrayList&lt;Particle&gt; particles = new ArrayList&lt;Particle&gt;(); particles.add(new Particle()); particles.get(&lt;index&gt;); particles.remove(&lt;index&gt;); // Enhanced Java loop // Con: can&#39;t modify the arraylist while in the loop for (Particle p: particles) { p.update(); p.display(); } // PROBLEM MODIFYING ARRAYLIST WHILE LOOPING: // If we want to remove from arraylist in the middle // the indices change, e.g. removing c from a, b, c, d, e // we do remove(2), and it // gives a, b, d, e, with d occupying index 2 now. // The loop goes on to i=3 and d is skipped // SOLUTION: // To avoid this when removing in the loop, LOOP BACKWARD for (int i = particles.size(); i &gt;= 0; i--) { if (particle.isDead()) { particles.remove(i); } particles[i].update(); particles[i].display(); } . JavaScript Array: .push(), [i], .splice(i, numToBeRemoved), .length . Note: .splice() can also add items. ref . Organize Particles into one ParticleSystem class . class ParticleSystem { ArrayList&lt;Particle&gt; particles; ParticleSystem() { particles = new ArrayList&lt;Particle&gt;(); } void addParticle(Particle p) { particles.add(p); } void run() { ... } } . A ParticleSystem class is essentially an ArrayList or Particles. Its constructor should be initializing that ArrayList. It can also have a centerPosition where all its particles initialize at. It should be able to addParticle(), display all particles via a loop in run(), and more depending on the use cases. . We can also have a system of ParticleSystems where it is an ArrayList of ParticleSystems. We can assign properties to the system of systems and separate the functionality for each level of system. . Inheritance . Inherit everything from super class | Add data or funtionality | Override functions | super ! Call its parent’s function | // extends is the keyword for inheritance class Kitten extends Mammal { int numWhiskers; void sleep() { println(&quot;purrrr&quot;); super.sleep(); } void meow() {} } . Another example . // extends is the keyword for inheritance class SquareParticle extends Particle { SquareParticle(PVector l) { super(l); } void display() { fill(127); stroke(0); rectMode(CENTER); rect(location.x, location.y, 16, 16); } } . Polymorphism . Polymorphism allows us to use child classes as type parent class, e.g. put child classes into one array of type parent class. . Suppose we have a class Animal, and Dog, Cat extends it. . Animal[] kingdom = new Animal[100]; Animal spot = new Dog(); kingdom[0] = new Dog(); kingdom[1] = new Cat(); ... for (Animal a: kingdom) { // This is powerful! These call their own subclass functions! a.sleep(); a.eat(); } . JavaScript ES6 has the same syntax extends and super as Java! . Note that JS has differences between ES5 class syntax and ES6 ones. . ES5 has . function MyClass(params) { this.params = params; } . while ES6 has . class MyClass { constructor(params) { this.params = params; } } . Inheritance with overriding constructor (must call super in a child constructor before using this, or it won’t work) . class ParentClass { constructor(params) { this.params = params; } } class ChildClass extends ParentClass { constructor(childParams) { super(parentParams); this.params = otherChildParams; } } . When overriding another method: We can use super.method() in a Child method to call Parent method. . Here is a good reference. . To use code for base classes in different folders, put . &lt;script src=&quot;/absolute_path_to_js_file&quot;&gt;&lt;/script&gt; . in the header of html files, NOT import because it is not a module. . TODO: Need to take note here for modules in the future. . Use Image Textures with Particles . Processing uses PImage for images. Preload an image and don’t load it in the constructor. Processing has P2D mode to render things much faster. . There is this blendMode concept, with blendMode(ADD) all the RGB values are added on top of each other which creates a brighter effect. . Ref video .",
            "url": "http://blog.logancyang.com/note/natureofcode/simulation/processing/p5js/2020/03/13/nature-of-code-1.html",
            "relUrl": "/note/natureofcode/simulation/processing/p5js/2020/03/13/nature-of-code-1.html",
            "date": " • Mar 13, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Flask Full Stack Part 1: Frontend Quick Walkthrough",
            "content": "This is the note for the Flask course by Jose Portilla on Udemy. . 1. Overview . Flask is a super simple Python web framework and is also scalable with a lot of 3rd party libraries. Choosing Flask rather than NodeJS because Python has nice ecosystem in machine learning. What Flask does in general: . Connect with UI | Connect to database and handle CRUD (create, read, update, delete) | . For example, it can handle HTML forms via library WTForms. SQLite is sufficient as a database for a small website. . Flask is a middle man between the frontend UI and the database backend. . We will use Jinja templates to grab information from Python and Flask to send as HTML. . . 2. HTML quick reference . &lt;!DOCTYPE html&gt; tells it’s an html file. &lt;head&gt; contains metadata, title on the tab and links to javascript, &lt;body&gt; contains content such as forms, styles, headers, etc. . 2.1. Basic tags, list, div, span, attribute . &lt;h1&gt;: heading one . &lt;h6&gt;: heading six . &lt;p&gt;: paragraph . &lt;strong&gt;: bold . &lt;em&gt;: italics . &lt;br&gt;: line break . For full reference, go to Mozilla HTML elements reference. . CodePen and JSFiddle are good online test grounds. . &lt;ol&gt;: ordered list . &lt;li&gt;: list item . &lt;ul&gt;: unordered list . Lists can be nested. . &lt;div&gt; stands for division. . &lt;div&gt; and &lt;span&gt; can separate the HTML page into sections. . &lt;div&gt; is for larger division/block of elements, &lt;span&gt; is for substring such as . &lt;p&gt;Here is &lt;span class=&#39;myclass&#39;&gt;some text&lt;/span&gt;. woohoo! &lt;/p&gt; . for doing styling on myclass. . 2.1.1. HTML Attributes . &lt;img src=&quot;&lt;link to image&gt;&quot; alt=&quot;Uh oh! No image&quot;&gt; . &lt;link to image&gt; can be an url online or a path to local file. Next sections will show how to organize static files in Flask. . &lt;a href=&quot;&lt;some url&gt;&quot;&gt;My link here&lt;/a&gt; . Again, &lt;some url&gt; can be an URL online or a path to another html file locally. . Note that &lt;img&gt; is a self closing tag but &lt;a&gt; is not. . 2.2. HTML Forms . Consist of &lt;form&gt; and &lt;input&gt; tags. . Example 1 (from code example Forms Basics) . &lt;form&gt; &lt;h1&gt;Log In&lt;/h1&gt; &lt;h2&gt;Please Input your Password and Email&lt;/h2&gt; &lt;input type=&quot;email&quot; name=&quot;useremail&quot; value=&quot;Email Here&quot;&gt; &lt;input type=&quot;password&quot; name=&quot;password&quot; value=&quot;Password&quot;&gt; &lt;input type=&quot;submit&quot; name=&quot;&quot; value=&quot;Enter&quot;&gt; &lt;h1&gt;Choose a Color!&lt;/h1&gt; &lt;h2&gt;Click on the Button when ready&lt;/h2&gt; &lt;input type=&quot;color&quot; &gt; &lt;h2&gt;Enter some Text!&lt;/h2&gt; &lt;input type=&quot;text&quot; name=&quot;&quot; value=&quot;Text goes here&quot;&gt; &lt;/form&gt; . . The email input type will let the browser check if it’s a valid email with @. value is prefilled. . The password type hides the input in the box. value is what’s prefilled and hidden. . The submit type is a button where value has the text shown on the button. . The color type is interesting but not commonly used, it lets you select from a color palette. . GET will send back the info to our action URL. . POST submits data to be processed. . Forms must set label for each text box in order to let the user see which field is which in the UI. The for in &lt;label&gt; must match the id in &lt;input&gt; to label the input properly. . Example: (from example Form Labels) . &lt;!-- Upon submitting the form will perform the action (a redirect) --&gt; &lt;form action=&quot;http://www.google.com&quot; method=&quot;get&quot;&gt; &lt;label for=&quot;email&quot;&gt;EMAIL:&lt;/label&gt; &lt;input type=&quot;email&quot; id=&quot;email&quot; name=&quot;useremail&quot; value=&quot;Email Here&quot;&gt; &lt;label for=&quot;pass&quot;&gt;PASSWORD:&lt;/label&gt; &lt;input type=&quot;password&quot; id=&quot;pass&quot; name=&quot;password&quot; placeholder=&quot;Password&quot;&gt; &lt;!-- Validation --&gt; &lt;!-- Usually do a lot more validation with Backend--&gt; &lt;!-- Use the attribute: required to require input--&gt; &lt;label for=&quot;userinput&quot;&gt;TEXT:&lt;/label&gt; &lt;input type=&quot;text&quot; id=&quot;userinput&quot; name=&quot;input&quot; placeholder=&quot;Enter Text Here&quot; required&gt; &lt;input type=&quot;submit&quot; &gt; &lt;/form&gt; . . action is the action that gets triggered upon form submission. Making it an URL is a redirect. . value in the text input type tag is a pre-populated string that is shown in the text input box before typing. It is also the value that actually gets submitted for the field. . placeholder is a hint to the user when the field is empty and it is greyed out. . For type “password”, value is prefilled and hidden, placeholder is a hint without actual filled value and is not hidden. . 2.2.1. Form Selections . When two input radio buttons share the same name, only one can be selected. . Example: (from example form seletions) . &lt;form method=&quot;get&quot;&gt; &lt;h3&gt;Do you already own a dog?&lt;/h3&gt; &lt;label for=&quot;yes&quot;&gt;Yes&lt;/label&gt; &lt;input type=&quot;radio&quot; id=&quot;yes&quot; name=&quot;dog_choice&quot; value=&quot;yes&quot;&gt; &lt;label for=&quot;no&quot;&gt;No:&lt;/label&gt; &lt;input type=&quot;radio&quot; id=&quot;no&quot; name= &quot;dog_choice&quot; value=&quot;no&quot;&gt; &lt;p&gt;How clean is your house (Rated 1-3 with 3 being cleanest))&lt;/p&gt; &lt;select name=&quot;stars&quot;&gt; &lt;option value=&quot;Great&quot;&gt;3&lt;/option&gt; &lt;option value=&quot;Okay&quot;&gt;2&lt;/option&gt; &lt;option value=&quot;Bad&quot;&gt;1&lt;/option&gt; &lt;/select&gt; &lt;p&gt;Any other comments?&lt;/p&gt; &lt;textarea name=&quot;name&quot; rows=&quot;8&quot; cols=&quot;80&quot;&gt;&lt;/textarea&gt; &lt;input type=&quot;submit&quot; name=&quot;&quot; value=&quot;Submit Feedback&quot;&gt; &lt;/form&gt; . . &lt;select&gt; gives a dropdown selection of &lt;option&gt;s. Each option has a value. The value of the option selected will be assigned to name (variable name) of the &lt;select&gt; and the backend can see name = value for this dropdown. . &lt;textarea&gt; is a big text box which can be set with # rows and columns. . Note that &lt;submit&gt;’s value is just the string shown on the submit button. . Once hit submit, the URL will be updated and a part in the format ?name=value&amp;name=value&amp;name=value will be appended. . 3. CSS Crash Course . CSS = Cascading Style Sheet . CSS controls the color, background, borders and much more. . Create a .css file | Use CSS syntax to link element tags | Add style name-value pairs | Connect CSS to HTML | 3.1. Colors . Example: (from Part1_master/css) . /*Colors can be names, or codes*/ h1{ color: blue; } li { color: rgb(0,200,0); } /*Search Google for hex color, it has a hex color picker*/ p{ color: #000000; } /*a is alpha, controls transparency, 1 is fully opaque, 0 fully transparent*/ h4{ color: rgba(13,90,140,0.5) } . The general format is shown below, don’t forget the ; . Selected Tag { property: value; } . To link the CSS file to HTML, add the following in the &lt;head&gt; section to the HTML. . &lt;link rel=&quot;stylesheet&quot; href=&quot;Part1_master.css&quot;&gt; . rel is the relationship attribute of the link, it says the CSS is a stylesheet of the HTML. . href points to the path of the CSS file. . The final result is shown below. . . 3.2. Backgrounds and Borders . Example: (from Part2_master.css) . . . background can be an url to an image, set no-repeat to avoid tiling. background-repeat can be repeat-x or repeat-y for x and y axis only. . For border, border-style and border-width are required attributes. Use one line to avoid 3 . border: orange 10px dashed; . Final result: . . 3.3. class and id: CSS Selector . This is the most important one for CSS. We can select by id or class. . Every HTML element can accept a class or id attribute. CSS can link to them by . . for class | # for id | . classs are for styling multiple different elements. . ids are for a single and unique element. . Example: from CSS Part3 . . . Later, Bootstap will define classes for us. . To summarize, CSS can style the HTML based on tags, classes and ids. . 3.4. Inspect Elements in Browser . In Chrome we can inspect the HTML and CSS in the developer tool. We can even edit it locally to see changes. For example, open Google and change it’s styling locally. . To go back to the original site, just hit refresh. . 3.5. Fonts . Not every font is available on each OS. Mac, Windows and Linux have different fonts. . Use Google Fonts API to change fonts. . Add a link to Google fonts API in the HTML | Add the name for the font-family from Google Fonts API. | You can get the link and name from the Google Fonts webpage. . Example: look at CSS Part5 files . 4. Bootstrap 4 . Bootstrap is a CSS framework originally developed at Twitter for internal use. It was open sourced in 2011 and became one of the most starred projects on Github. . What is Bootstrap? . Conceptually, Bootstrap is a really large CSS file + a really large JS file. . Check out the documentation and templates . This is a template for a dashboard. . . Key concepts: bootstrap components and classes . Linking Bootstrap | Containers | Jumbotrons | Buttons | . 4.1. Buttons . In the &lt;head&gt; section in the HTML, . Copy and paste the CSS link | . &lt;link rel=&quot;stylesheet&quot; href=&quot;https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css&quot; integrity=&quot;sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T&quot; crossorigin=&quot;anonymous&quot;&gt; . Copy and paste the jQuery link | . &lt;script src=&quot;https://code.jquery.com/jquery-3.3.1.slim.min.js&quot; integrity=&quot;sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt; &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js&quot; integrity=&quot;sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt; &lt;script src=&quot;https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js&quot; integrity=&quot;sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt; . Add containers next. The containers are responsive and can self-adjust the position based on browser size on different devices. . Then add a button. Go to Bootstrap -&gt; components -&gt; buttons, copy and paste the code there, mainly need the class names e.g. btn btn-primary. . &lt;button class=&quot;btn btn-success btn-lg active&quot; type=&quot;button&quot; name=&quot;button&quot;&gt;Button&lt;/button&gt; . Same for other components. Be comfortable searching the Component section and copy paste around. . . 4.1.1. Class jumbotron . A showcase message for the website. . Example: . &lt;!-- JumboTron --&gt; &lt;div class=&quot;container&quot;&gt; &lt;div class=&quot;jumbotron&quot;&gt; &lt;!-- &lt;div class=&quot;container&quot;&gt; --&gt; &lt;h1 class=&quot;display-3&quot;&gt;Hello, world!&lt;/h1&gt; &lt;p class=&quot;lead&quot;&gt;This is a simple hero unit, a simple jumbotron-style component for calling extra attention to featured content or information.&lt;/p&gt; &lt;hr class=&quot;my-2&quot;&gt; &lt;p&gt;It uses utility classes for typography and spacing to space content out within the larger container.&lt;/p&gt; &lt;p class=&quot;lead&quot;&gt; &lt;a class=&quot;btn btn-primary btn-lg &quot; href=&quot;#&quot; role=&quot;button&quot;&gt;Learn more&lt;/a&gt; &lt;/p&gt; &lt;!-- &lt;/div&gt; --&gt; &lt;/div&gt; &lt;/div&gt; . . 4.2. Forms . In &lt;head&gt; include . &lt;!-- Bootstrap CSS, JS, and jQuery --&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;https://stackpath.bootstrapcdn.com/bootstrap/4.1.1/css/bootstrap.min.css&quot; integrity=&quot;sha384-WskhaSGFgHYWDcbwN70/dfYBj47jz9qbsMId/iRN3ewGhXQFZCSftd1LZCfmhktB&quot; crossorigin=&quot;anonymous&quot;&gt; &lt;script src=&quot;https://code.jquery.com/jquery-3.3.1.slim.min.js&quot; integrity=&quot;sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt; &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js&quot; integrity=&quot;sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt; &lt;script src=&quot;https://stackpath.bootstrapcdn.com/bootstrap/4.1.1/js/bootstrap.min.js&quot; integrity=&quot;sha384-smHYKdLADwkXOn1EmN1qk/HfnUcbVRZyYmZ4qpPea6sjB/pTJ0euyQp0Mk8ck+5T&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt; . Bootstrap 4 makes forms look really well. . . All of these form components such as email submission, password, dropdown select, multiple select, text area, file upload, radio button, check button should use proper div, select and input with bootstrap class names. Refer to Part2_Forms.html for actual HTML code for each component. . 4.3. Navbar . HTML tag &lt;nav&gt; creates a navigation bar. Bootstrap classes can be added to it to add styling and functionality. It even makes it a dropdown menu on small screens of mobile devices. . Refer to Part3_Navbar.html for code examples. One thing to note is that we need jQuery uncompressed or minified, not slim or slim minified as in Bootstrap. Go to the jQuery website and get the link for minified . &lt;script src=&quot;https://code.jquery.com/jquery-3.4.1.min.js&quot; integrity=&quot;sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt; . The reason is that the slim version of jQuery from Bootstrap sometimes has problems for the collapse functionality for certain browsers. . Check the documentation for further customizations as needed. . The End . This is a quick crash course for HTML, CSS, and Bootstrap. . For further frontend knowledge, check out the courses below . The Complete Web Developer in 2019: Zero to Mastery by Andrei Neagoie | React - The Complete Guide (incl Hooks, React Router, Redux) by Maximilian Schwarzmüller | .",
            "url": "http://blog.logancyang.com/note/fullstack/flask/2019/07/08/flask-frontend.html",
            "relUrl": "/note/fullstack/flask/2019/07/08/flask-frontend.html",
            "date": " • Jul 8, 2019"
        }
        
    
  
    
        ,"post10": {
            "title": "Flask Full Stack Part 3: Flask and SQL Database Walkthrough",
            "content": "This is the note for the Flask course by Jose Portilla on Udemy. . Python and Flask can connect to a variety of SQL database engines, including PostgreSQL, MySQL, SQLite, and a lot more. . SQLite is a simple SQL database engine that comes with Flask, usually it can handle all the needs for a small application with daily hits around 100K. It can also handle more, it’s not a hard cap. . To connect Python code to SQL code, use ORM (Object Relational Mapper). The most common is SQLAlchemy. Flask-SQLAlchemy is an extension to connect Flask to SQLAlchemy. . pip install Flask-SQLAlchemy . Getting Started . We need the following steps . Set up SQLite database in a Flask app | Create a model in the Flask app | Perform basic CRUD on our model | . Here we will show the script for manual CRUD to help understand the concepts. In practice, Flask automates all these. . Use CLI tool to make database. Refer to BasicModelApp.py to check the model definition, and SetUpDatabase.py to see db creation script (usually db creation is through CLI). . A summary of model class definition: . define __tablename__ | define column variables as class variables such as id, name, age, etc. | define __init__(self, ...) method. Note that id is automatically set and it does not need to be in __init__() | define __repr__(self) method | (optional) if there is a relationship to another model/table, use db.relationship(&#39;&lt;another_model&gt;&#39;, backref=&#39;&lt;this_model&gt;&#39;, lazy=&#39;dynamic&#39;) | . To add a row to the db, . db.session.add(&lt;row_obj&gt;) db.session.commit() . For SQLite and SQLAlchemy, integer id gets added automatically and starts at 1. The .sqlite file is the database, it’s encoded and is not readable by a text editor. . CRUD example: . from BasicModelApp import db,Puppy ########################### ###### CREATE ############ ######################### my_puppy = Puppy(&#39;Rufus&#39;,5) db.session.add(my_puppy) db.session.commit() ########################### ###### READ ############## ######################### # Note lots of ORM filter options here. # filter(), filter_by(), limit(), order_by(), group_by() # Also lots of executor options # all(), first(), get(), count(), paginate() all_puppies = Puppy.query.all() # list of all puppies in table print(all_puppies) print(&#39; n&#39;) # Grab by id puppy_one = Puppy.query.get(1) print(puppy_one) print(puppy_one.age) print(&#39; n&#39;) # Filters puppy_sam = Puppy.query.filter_by(name=&#39;Sammy&#39;) # Returns list print(puppy_sam) print(&#39; n&#39;) ########################### ###### UPDATE ############ ######################### # Grab your data, then modify it, then save the changes. first_puppy = Puppy.query.get(1) first_puppy.age = 10 db.session.add(first_puppy) db.session.commit() ########################### ###### DELETE ############ ######################### second_pup = Puppy.query.get(2) db.session.delete(second_pup) db.session.commit() # Check for changes: all_puppies = Puppy.query.all() # list of all puppies in table print(all_puppies) . Flask Migrate: Database Migration . When there are new columns to be added, only updating the model.py file won’t update the database. We need database migration. . To install, . pip install Flask-Migrate . To import, . from flask_migrate import Migrate . To use Flask Migrate, the first thing is to set the FLASK_APP variable. On Linux/Mac it goes, . export FLASK_APP=&lt;myapp.py&gt; . then import Migrate from flask_migrate, and add the line below before the class definition code. . # Add on migration capabilities in order to run terminal commands Migrate(app,db) . Migration steps: . # Set up the migration directory flask db init # Add some changes to the model class, e.g. add a column # Set up the migration file. Similar to git commit flask db migrate -m &quot;&lt;some message&gt;&quot; # Update the database with the migration flask db upgrade . Flask Relationships . For multiple models (tables), we need “relationships” to link them together. Some concepts: . Primary Key: unique identifier for a row | Foreigh Key: the column that’s another table’s primary key | . The key is to define db.relationship(...) to point a class variable x in &lt;parent_table&gt; (column) to another table. x is defined as db.Column(db.&lt;var_type&gt;, db.ForeignKey(&#39;&lt;parent_table.column&gt;&#39;)) in the other table. &lt;parent_table&gt; does not reference to other tables, other tables reference to it by having a column of &lt;parent_table&gt;’s primary key as db.ForeignKey. . Code example below . class Puppy(db.Model): __tablename__ = &#39;puppies&#39; id = db.Column(db.Integer,primary_key = True) name = db.Column(db.Text) # This is a one-to-many relationship # A puppy can have many toys toys = db.relationship(&#39;Toy&#39;,backref=&#39;puppy&#39;,lazy=&#39;dynamic&#39;) # This is a one-to-one relationship # A puppy only has one owner, thus uselist is False. # Strong assumption of 1 dog per 1 owner and vice versa. owner = db.relationship(&#39;Owner&#39;,backref=&#39;puppy&#39;,uselist=False) def __init__(self,name): # Note how a puppy only needs to be initalized with a name! self.name = name def __repr__(self): if self.owner: return f&quot;Puppy name is {self.name} and owner is {self.owner.name}&quot; else: return f&quot;Puppy name is {self.name} and has no owner assigned yet.&quot; def report_toys(self): print(&quot;Here are my toys!&quot;) for toy in self.toys: print(toy.item_name) class Toy(db.Model): __tablename__ = &#39;toys&#39; id = db.Column(db.Integer,primary_key = True) item_name = db.Column(db.Text) # Connect the toy to the puppy that owns it. # We use puppies.id because __tablename__=&#39;puppies&#39; puppy_id = db.Column(db.Integer,db.ForeignKey(&#39;puppies.id&#39;)) def __init__(self,item_name,puppy_id): self.item_name = item_name self.puppy_id = puppy_id class Owner(db.Model): __tablename__ = &#39;owners&#39; id = db.Column(db.Integer,primary_key= True) name = db.Column(db.Text) # We use puppies.id because __tablename__=&#39;puppies&#39; puppy_id = db.Column(db.Integer,db.ForeignKey(&#39;puppies.id&#39;)) def __init__(self,name,puppy_id): self.name = name self.puppy_id = puppy_id . Note that one-to-one relationship has uselist=False. Default is uselist=True i.e. one-to-many, if not specified. . lazy=&#39;dynamic&#39; is a more advanced usage. Leave it as it for now. . . Schema: . Table puppies: id, name (parent table) . Table toys: id, item_name, puppy_id (foreign key, refers to puppies.id) . Table owners: id, name, puppy_id (foreign key, refers to puppies.id) . . Now we experiment with adding some records with relationships. . from models import db,Puppy,Owner,Toy # Create 2 puppies rufus = Puppy(&quot;Rufus&quot;) fido = Puppy(&quot;Fido&quot;) # Add puppies to database db.session.add_all([rufus,fido]) db.session.commit() # Check with a query, this prints out all the puppies! print(Puppy.query.all()) # Grab Rufus from database # Grab all puppies with the name &quot;Rufus&quot;, returns a list, so index [0] # Alternative is to use .first() instead of .all()[0] rufus = Puppy.query.filter_by(name=&#39;Rufus&#39;).all()[0] # Create an owner to Rufus # Owner __init__ takes in name and foreign key Puppy id which is unique jose = Owner(&quot;Jose&quot;,rufus.id) # Give some Toys to Rufus # Toy __init__ takes in item name and foreign key Puppy id which is unique toy1 = Toy(&#39;Chew Toy&#39;,rufus.id) toy2 = Toy(&quot;Ball&quot;,rufus.id) # Commit these changes to the database # Note that all_all can take different types of objects at once!! db.session.add_all([jose,toy1,toy2]) db.session.commit() # Let&#39;s now grab rufus again after these additions rufus = Puppy.query.filter_by(name=&#39;Rufus&#39;).first() print(rufus) # Output: # Puppy name is Rufus and owner is Jose # Show toys print(rufus.report_toys()) # Output: # Chew Toy # Ball # You can also delete things from the database: find_pup = Puppy.query.get(1) db.session.delete(find_pup) db.session.commit() # But note that if deleted, do not try to delete again, or it will error . Connect to Flask Template - Databases in Views . Refer to “03-Databases-in-Views” folder for a complete puppy adoption site example from scratch. . A view function is just a function with app.route in Flask, it responds to client requests. . First, create the site.py or app.py, then the necessary files such as forms.py and models.py. Next, create the templates folder and base.html in it, add necessary html files such as home.html, add.html, delete.html, etc. . Then add the model classes for the database same as in the last section. . The next step is to add @app.routes functions to the form view, such as . @app.route(&#39;/add&#39;, methods=[&#39;GET&#39;, &#39;POST&#39;]) def add_pup(): form = AddForm() if form.validate_on_submit(): name = form.name.data new_pup = Puppy(name) db.session.add(new_pup) db.session.commit() return redirect(url_for(&#39;list_pup&#39;)) return render_template(&#39;add.html&#39;, form=form) . and other views such as /list and /delete. Refer to example in the code folder. .",
            "url": "http://blog.logancyang.com/note/fullstack/flask/2019/07/08/flask-db.html",
            "relUrl": "/note/fullstack/flask/2019/07/08/flask-db.html",
            "date": " • Jul 8, 2019"
        }
        
    
  
    
        ,"post11": {
            "title": "How to Create Keyword List Entities in spaCy (v2.1)",
            "content": "Sometimes there is a need to create keyword entities from a list of known keywords, e.g. country names, species, brands, etc. The list can be large. This note shows how to create such entities in spaCy and make it work with a trained NER model. . Rule Based Matcher . PhraseMatcher is useful if you already have a large terminology list or gazetteer consisting of single or multi-token phrases that you want to find exact instances of in your data. As of spaCy v2.1.0, you can also match on the LOWER attribute for fast and case-insensitive matching. . Matcher is about individual tokens. For example, you can find a noun, followed by a verb with the lemma “love” or “like”, followed by an optional determiner and another token that’s at least ten characters long. . PhraseMatcher is what we need. . Say we have several brand names, . [u&quot;Armani&quot;, u&quot;Ralph Lauren&quot;, u&quot;Monique Lhuillier&quot;, u&quot;Norma Kamali&quot;] . Assume we have some text messages in which we find these brand names. We apply the trained NER model on these messages to make predictions. To make this case insensitive, use attr=&quot;LOWER&quot;, . from spacy.lang.en import English from spacy.matcher import PhraseMatcher nlp = English() matcher = PhraseMatcher(nlp.vocab, attr=&quot;LOWER&quot;) patterns = [ nlp.make_doc(name) for name in [u&quot;Armani&quot;, u&quot;Ralph Lauren&quot;, u&quot;Monique Lhuillier&quot;, u&quot;Norma Kamali&quot;] ] matcher.add(&quot;Brands&quot;, None, *patterns) doc = nlp(u&quot;armani and monique Lhuillier are both brands&quot;) for match_id, start, end in matcher(doc): print(&quot;Matched based on lowercase token text:&quot;, doc[start:end]) # output: # Matched based on lowercase token text: armani # Matched based on lowercase token text: monique Lhuillier . It can even match number entities by shape, attr=&quot;SHAPE&quot;, e.g. IP addresses. . Combine with Model Prediction: Use Entity Ruler and Pattern File (v2.1) . PhraseMatcher doesn’t address the need to combine rules with statistical models. The rules must have influence on the prediction process or they will have conflicts. . Citing the spaCy docs, “The entity ruler is designed to integrate with spaCy’s existing statistical models and enhance the named entity recognizer. If it’s added before the &quot;ner&quot; component, the entity recognizer will respect the existing entity spans and adjust its predictions around it. This can significantly improve accuracy in some cases. If it’s added after the &quot;ner&quot; component, the entity ruler will only add spans to the doc.ents if they don’t overlap with existing entities predicted by the model. To overwrite overlapping entities, you can set overwrite_ents=True on initialization.” . from spacy.lang.en import English from spacy.pipeline import EntityRuler # Before training nlp = English() &quot;&quot;&quot;This is the hard-coded ruler ruler = EntityRuler(nlp) patterns = [{&quot;label&quot;: &quot;ORG&quot;, &quot;pattern&quot;: &quot;Apple&quot;}, {&quot;label&quot;: &quot;GPE&quot;, &quot;pattern&quot;: [{&quot;lower&quot;: &quot;san&quot;}, {&quot;lower&quot;: &quot;francisco&quot;}]}] ruler.to_disk(&quot;./patterns.jsonl&quot;) ruler.add_patterns(patterns) &quot;&quot;&quot; # Loading ruler from jsonl file ruler = EntityRuler(nlp).from_disk(&quot;./patterns.jsonl&quot;) nlp.add_pipe(ruler) # Add NER training / transfer learning code here... # At prediction time doc = nlp(u&quot;Apple is opening its first big office in San Francisco.&quot;) print([(ent.text, ent.label_) for ent in doc.ents]) . Question: Since the pattern file is a list of patterns, it must be slow to go through the list every time to check whether something is a brand. What’s the solution? . Case Study: Brand Entity . Brand is an example where the keyword list / pattern file can be really large. There are already many labeled brand entities in the training data so the model may or may not find correct brand entities at prediction time. In the case of an incorrect prediction, how do we leverage the rule-based method to correct it? . Note that we prefer adding the EntityRuler before the &quot;ner&quot; component to let the model respect the keyword list and adjust its predictions. . Case 1: Predicted entity is not in the keyword list and has no word overlap with any item in the list. . In this case, it is either a wrong prediction or a new brand entity correctly predicted but is not in the training data. These cases need to be logged and checked by a human. If confirmed it IS a correct new brand entity, it should be added to the brand keyword list. . Case 2: Predicted entity is not in the keyword list BUT has overlap with one or more items in the list. . If EntityRuler is used, the model prediction should be able to find the complete brand name in the text, so any such overlap should be the case where only part of the brand name is there in the text but no complete name from the brand list is present. This is sometimes OK, people don’t necessarily call out the complete brand name but only refer to it with a short form. In other cases, this is a wrong prediction. Again, human check is preferred. . Case 3: Predicted entity is in the keyword list . This is the trivial case where the model is doing a perfect job. . (For more advanced usages involving dependency parsing, check here for examples. This is beyond the scope of this post about keyword list entities.) .",
            "url": "http://blog.logancyang.com/note/spacy/nlp/2019/06/07/keyword-entity.html",
            "relUrl": "/note/spacy/nlp/2019/06/07/keyword-entity.html",
            "date": " • Jun 7, 2019"
        }
        
    
  
    
        ,"post12": {
            "title": "How to setup local GPU server for fast.ai",
            "content": "Fast.ai is the best coder’s guide to practical deep learning. This is a guide to its environment setup on a Linux server with NVIDIA GPU. . Server setup: GPU driver and CUDA . If you have an Nvidia GPU on your Linux machine, great! Let’s install the necessary components for deep learning. . Preparation . Install some useful packages . sudo apt-get update sudo apt-get install aptitude freeglut3-dev g++-4.8 gcc-4.8 libglu1-mesa-dev libx11-dev libxi-dev libxmu-dev nvidia-modprobe python-dev python-pip python-virtualenv vim . Install CUDA . Download CUDA installation file: https://developer.nvidia.com/cuda-downloads . Choose Linux -&gt; x86_64 -&gt; Ubuntu -&gt; 14.04 -&gt; deb (local) -&gt; Download . Install CUDA in terminal (use the specific .deb file you’ve downloaded): . cd ~/Downloads sudo dpkg -i cuda-repo-ubuntu1404-8-0-local-ga2_8.0.61-1_amd64.deb sudo apt-get update sudo apt-get install cuda . Restart the computer to activate CUDA driver. Now your screen resolution should be automatically changed to highest resolution for the display! . Install cuDNN . The NVIDIA CUDA® Deep Neural Network library (cuDNN) is aGPU-accelerated library of primitives for deep neural networks with optimizations for convolutions etc. . Register an (free) acount on NVIDIA website and login to download the latest cuDNN library: https://developer.nvidia.com/cudnn . Choose the specific version of cuDNN (denpending on support of your prefered deep learning framework) . Choose Download cuDNN v5.1 (Jan 20, 2017), for CUDA 8.0 -&gt; cuDNN v5.1 Library for Linux . Install cuDNN (by copying files :) in terminal: . cd ~/Downloads tar xvf cudnn-8.0-linux-x64-v5.1.tgz cd cuda sudo cp lib64/* /usr/local/cuda/lib64/ sudo cp include/cudnn.h /usr/local/cuda/include/ sudo chmod a+r /usr/local/cuda/lib64/libcudnn* . Update your .bashrc . Add the following lines to your ~/.bashrc file (you can open it by gedit ~/.bashrc in terminal) . export PATH=/usr/local/cuda/bin:$PATH export MANPATH=/usr/local/cuda/man:$MANPATH export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH source ~/.bashrc . To check the installation, print some GPU and driver information by: . nvidia-smi nvcc --version . Note: Haven’t been using my gpu server for a while and when I started Ubuntu in Jan 2020, the NVidia driver stopped working. First of all, the display resolution was messed up and I had to perform actions from my other machine via ssh. I had to update the driver but there were some hoops to jump through. There were some broken dependencies and I needed . sudo aptitude install &lt;name_of_package_with_conflicts&gt; . to be able to run . sudo ubuntu-drivers autoinstall . reference: https://askubuntu.com/questions/1077493/unable-to-install-nvidia-drivers-on-ubuntu-18-04 . Server Setup: Conda . Install Anaconda . cd /tmp curl -O https://repo.anaconda.com/archive/Anaconda3-2019.03-Linux-x86_64.sh sha256sum Anaconda3-2019.03-Linux-x86_64.sh # output # 45c851b7497cc14d5ca060064394569f724b67d9b5f98a926ed49b834a6bb73a Anaconda3-2019.03-Linux-x86_64.sh bash Anaconda3-2019.03-Linux-x86_64.sh . You’ll receive the following output to review the license agreement by pressing ENTER until you reach the end. . # Output Welcome to Anaconda3 2019.03 In order to continue the installation process, please review the license agreement. Please, press ENTER to continue &gt;&gt;&gt; ... Do you approve the license terms? [yes|no] . When you get to the end of the license, type yes as long as you agree to the license to complete installation. . Once you agree to the license, you will be prompted to choose the location of the installation. You can press ENTER to accept the default location, or specify a different location. . Once installation is complete and ran conda init, the following code is added to .bashrc. If you use zsh, add it to .zshrc. . # &gt;&gt;&gt; conda initialize &gt;&gt;&gt; # !! Contents within this block are managed by &#39;conda init&#39; !! __conda_setup=&quot;$(&#39;/home/loganyc/anaconda3/bin/conda&#39; &#39;shell.bash&#39; &#39;hook&#39; 2&gt; /dev/null)&quot; if [ $? -eq 0 ]; then eval &quot;$__conda_setup&quot; else if [ -f &quot;/home/loganyc/anaconda3/etc/profile.d/conda.sh&quot; ]; then . &quot;/home/loganyc/anaconda3/etc/profile.d/conda.sh&quot; else export PATH=&quot;/home/loganyc/anaconda3/bin:$PATH&quot; fi fi unset __conda_setup # &lt;&lt;&lt; conda initialize &lt;&lt;&lt; . Use the conda command to test the installation and activation: . conda list . To setup conda environment, . conda create --name my_env python=3 conda activate my_env . To deactivate conda env . conda deactivate . (Note if you use zsh, there can be a (base) shown as the default conda environment.) . Server Setup: fast.ai . git clone https://github.com/fastai/course-v3 conda update conda conda install -c pytorch -c fastai fastai pytorch torchvision cuda92 jupyter cd course-v3/nbs/dl1 jupyter notebook . In your terminal, and you can access the notebook at localhost:8888. . If going to localhost:8888 doesn’t work, or asks for a password/token return to your terminal window and look for this message after you typed ‘jupyter notebook’: “Copy/paste this URL into your browser when you connect for the first time, to login with a token:” . Copy and paste that URL into your browser, and this should connect you to your jupyter notebook. . Go back to the first page to see how to use this jupyter notebook and run the jupyter notebook tutorial. Come back here once you’re finished and don’t forget to stop your instance with the next step. . If you have any problem while using the fastai library try running . conda install -c fastai fastai . Note that in Ubuntu terminal, use ctrl+ to stop the notebook server. . GPU vs. CPU for Deep Learning Test . Try running this inside a Jupyter Notebook: . Cell [1]: . import torch t_cpu = torch.rand(500,500,500) %timeit t_cpu @ t_cpu # Output # 785 ms ± 14.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) . Cell [2]: . t_gpu = torch.rand(500,500,500).cuda() %timeit t_gpu @ t_gpu # Output # 18.7 ms ± 376 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) . Set up SSH connection . Server side, install SSH server: . sudo apt-get install openssh-server . Edit SSH configuration to whitelist users: . sudo vim /etc/ssh/sshd_config . Change root login permission line to: PermitRootLogin no . Add allow users: AllowUsers &lt;your_username&gt; . Then restart SSH server: . sudo /etc/init.d/ssh restart sudo ufw allow 22 . Client side, to connect with the workstation, you need to firstly know the server’s IP (or hostname if it has one). Use ifconfig -a on the server to check IP address (look for that in eth0). . Client side (Mac OS), you need to whitelist the server IP in /etc/hosts: . sudo vim /etc/hosts . Add line &lt;server IP&gt; &lt;server hostname&gt; . Port Forwarding to Acess Jupyter Notebook (LAN) . Within the home network, access the Jupyter Notebook on the Linux GPU server from a client machine’s browser by running . ssh -fNL 8888:local:8888 &lt;username_on_server&gt;@&lt;server_ip&gt; . and go to localhost:8888/tree in your browser. . Setup Remote SSH Access (WAN/Internet) . In my case, my Ubuntu machine with GPU sits at home behind a Verizon Fios router. I can directly ssh into it in my home network (LAN) but doing so from outside requires several additional steps. . Configure Router for Port Forwarding . For Verison Fios routers, go to 192.168.1.1 to access the router setting. In my case, the username is admin and the password is printed on my router. . In the page, find port forwarding, set source port to be ANY, destination port to be your custom port, say 2222, and the port forward to is 22 which is the port on the box at which ssh is listening. Then click add. Done! . SSH from Remote . Now, to log onto the box from outside, run . ssh &lt;username&gt;@&lt;your_router_ip&gt; -p 2222 . To simplify this, add a blob in ~/.ssh/config. . Host deeplearningbox HostName &lt;router_public_ip&gt; User &lt;username_on_deeplearningbox&gt; . Now you can run ssh deeplearningbox -p 2222 . Setup Jupyter Notebook Server for Remote Access . With remote ssh setup, now we setup Jupyter Notebook. . jupyter notebook --generate-config . which will generate jupyter_notebook_config.py in ~/.jupyter. . Then generate certfile and key as: . $ cd ~/.jupyter $ openssl req -x509 -nodes -days 365 -newkey rsa:1024 -keyout mykey.key -out mycert.pem . Launch Python and run the following lines: . In [1]: from notebook.auth import passwd In [2]: passwd() Enter password: Verify password: Out[2]: &#39;sha1:67c9e60bb8b6:9ffede0825894254b2e042ea597d771089e11aed&#39; . After that, edit the jupyter_notebook_config.py as following: . c.NotebookApp.password = u&#39;sha1:67c9e60bb8b6:9ffede0825894254b2e042ea597d771089e11aed&#39; # Set options for certfile, ip, password, and toggle off browser auto-opening c.NotebookApp.certfile = u&#39;/absolute/path/to/your/certificate/mycert.pem&#39; c.NotebookApp.keyfile = u&#39;/absolute/path/to/your/certificate/mykey.key&#39; c.NotebookApp.ip = &#39;0.0.0.0&#39; c.NotebookApp.open_browser = False # It is a good idea to set a known, fixed port for server access c.NotebookApp.port = 8888 . Now, start jupyter notebook on the deep learning workstation and log in by . ssh -fNL 8888:localhost:8888 deeplearningbox -p 2222 . Note that visit https://localhost:8888 instead of http on the client computer. . Side Note: Github HTTPS . Since I have 2FA authentication for my github account, using https as remote url for my repo needs “personal access token” which serves as password when pushing. Refer to this for setup. . Using SSH instead of HTTPS is another option. But when the computer has two or more github account, setting up more key pairs and making things work is just too much. Since I prefer using my MBP (the machine with multiple Github accounts) over the Linux box for coding, sticking with HTTPS for the mac. . Side Note: SSH Port Forwarding Concepts . I’m a visual person. I found the answer here helpful for explaining different types of port forwarding. . References . https://course.fast.ai/start_aws.html | https://github.com/charlesq34/DIY-Deep-Learning-Workstation | https://unix.stackexchange.com/questions/115897/whats-ssh-port-forwarding-and-whats-the-difference-between-ssh-local-and-remot | https://superuser.com/questions/869417/port-forwarding-not-working-on-verizon-fios-router | https://fzhu.work/blog/python/remote-ipython-notebook-setup.html | https://tuatini.me/part-2-how-to-setup-your-own-environment-for-deep-learning-for-remote/ | https://help.github.com/en/articles/creating-a-personal-access-token-for-the-command-line | .",
            "url": "http://blog.logancyang.com/note/fastai/2019/05/27/fastai-gpu-setup.html",
            "relUrl": "/note/fastai/2019/05/27/fastai-gpu-setup.html",
            "date": " • May 27, 2019"
        }
        
    
  
    
        ,"post13": {
            "title": "Hinton Neural Network course",
            "content": "Regression . The word regression comes from “regression to the mean” like the parent-children height example. Later it actually means using functional form to approximate a bunch of data points. . i.i.d.: Independent and identically distributed, a fundamental assumption for the data. . Linear Regression | Polynomial Regression | Cross Validation Split the training set into training and validation set. K-fold cross validation. | To determine the complexity of the model (degree of polynomial in the case of polynomial regression), choose the complexity with the lowest validation error | . | . . Neural Networks . Perceptron: $ mathbf{wx} = y $, threshold $ y $ to get $ hat{y} $ If linearly separable, it will find a solution | If not, it won’t stop. But there’s no way to know when to stop and declare it’s not linearly separable. | . | Gradient Descent: $ mathbf{wx} = a $, activation. There’s no thresholding. | Perceptron vs. Gradient descent Perceptron: Δwi=η∗(y−y^)xi Delta w_i = eta * (y - hat y) x_iΔwi​=η∗(y−y^​)xi​ . | Gradient descent: Δwi=η∗(y−a)xi Delta w_i = eta * (y - a) x_iΔwi​=η∗(y−a)xi​ . | Difference: activation with or without thresholding. Gradient descent is more robust, it needs a differentiable loss. The 1-0 loss of perceptron is not differentiable. | To get a differentiable / softer thresholding function, we have sigmoid (literally means s-like). | . | Sigmoid unit in NN is just a softer thresholding version of Perceptron. | Back-propagation for error: computationally beneficial organization of the chain rule. | Local optima: For a single sigmoid unit, the error looks like a parabola because its quadratic. For many sigmoid units as in NN, there will be a lot of local optima. | Learning = optimization | Note that a 1-layer NN with sigmoid activation is equivalent to Logistic Regression! | NN complexity More nodes | More layers | Larger weights | . | Restriction bias: what it is that you are able to represent, the restriction on possible hypotheses functions Started out with Perceptron, linear | Then more perceptrons for more complex functions | Then use sigmoids instead of 1-0 hard thresholding | So NN doesn’t have much restriction. It can represent any boolean function (more units), any continuous function (one hidden layer), and any arbitrary function with discontinuities (more hidden layers). | Danger of overfitting: use certain network structures, and cross validation. | . | Preference bias: given two algorithm representations, why we prefer one over the other Initialize weights at small random values | Always prefer simpler models when the error is similar: Occam’s Razor | . | Summary Perceptrons: thresholding unit | Networks can produce any Boolean function. | Perceptron rule: finite time for linearly separable data | General differentiable rule - back propagation &amp; gradient descent | . | .",
            "url": "http://blog.logancyang.com/note/2017/01/22/regression-and-nn.html",
            "relUrl": "/note/2017/01/22/regression-and-nn.html",
            "date": " • Jan 22, 2017"
        }
        
    
  
    
        ,"post14": {
            "title": "Graph Search: Subsets, Permutations",
            "content": "Traits for using BFS: . 1. Shortest path in a simple graph: given a initial state, a final state, a transition rule between states, ask how many (the least #) transitions from init to final. 2. Graph traversal. Only visit each node once. . Traits for using DFS: . (DFS method: build search tree, check conditions for recursing down a branch) . 1. Enumerate subsets, permutations. 2. Find all possible solutions. . Generally, when we do a search to enumerate cases using DFS recursion, there are 3 steps we should keep in mind, . 1. Define the recursion. 2. Think about what to do in the base case. When should we return directly. 3. In general cases (other than base case), how to make the problem smaller and recurse down. . Subsets I &amp; II (DFS template) . The thinking is to categorize cases by different head items. Enumerate the head item of a case (path) in a for loop in this way: . 1. Append the item into the path. 2. DFS recurse down onto the next case (generally a smaller case, with advanced index and/or updated reference parameter). 3. Pop the item from the path, to iterate to a different head item on the next iteration. . For this specific Subsets problem, the base case is just adding the current path. For Subsets II, the only difference is that the input can have duplicates and we don’t want the result subsets to be duplicate sets. Since the input is sorted (or we sort it by ourselves before DFS), when we encounter a number which is equal to the previous number in the for loop, we continue. Because the same number is taking the same place as the previous one, the resulting subsets with either of them are the same sets. . Permutations I &amp; II (DFS template) . It’s quite similar to the Subsets problems. The thinking is also to categorize cases by different head items, and enumerate the head item of a case (path) in a for loop. The difference is that now we don’t want to keep track of the index as a parameter passed into DFS. Our base case is that when the path has the same length as the original input sequence, the current path is added. . The for loop is now as such: . 1. Append the item into the path. 2. DFS recurse down after appending the new head item. Avoid the same number by checking if it&#39;s already in path, if yes, continue. 3. Pop the item from the path, iterate to a different head item on the next iteration. . For permutations II where we allow duplicates in the input list, we must sort it first and then do DFS. In the results, duplicate permutations must be avoided, but how? We introduce a new list, visited. We only add continuous same numbers to path, meaning if the previous same number is not visited, we continue. Check the code for details. . Summary for Subsets and Permutations . This kind of problems is not easy to understand. Recursion tree diagrams can help to clarify, but also keep in mind the code templates: inside the for loop, check condition to recurse down, then append in path, DFS down with path (with appropriate update in some parameter), pop from path. . . Subsets . . class Solution: &quot;&quot;&quot; @param S: The set of numbers. @return: A list of lists. See example. &quot;&quot;&quot; def subsets(self, S): if S is None or len(S) == 0: return [] S.sort() self.results = [] self.DFS([], 0, S) return self.results def DFS(self, path, ind, S): # base case, add each path of each recursion (sorted as required) # must make new list, list(path). If not, # res (path) points to the obj passed in, which is empty at the beginning res = list(path) self.results.append(res) # i is the first item&#39;s index in a path for i in xrange(ind, len(S)): path.append(S[i]) self.DFS(path, i+1, S) path.pop() . . Permutations: . . class Solution: &quot;&quot;&quot; @param nums: A list of Integers. @return: A list of permutations. &quot;&quot;&quot; def permute(self, nums): if nums is None or len(nums) == 0: return [] self.results = [] self.DFS([], nums) return self.results def DFS(self, path, nums): # base case if len(path) == len(nums): # must make new list, list(path). If not, # it points to the obj passed in, which is empty at # the beginning self.results.append(list(path)) return for i in xrange(len(nums)): # check if the ith number is already in path if nums[i] in path: continue path.append(nums[i]) self.DFS(path, nums) path.pop() . Combination Sum is the Sum version of Subsets, with duplicates allowed. . . Palindrome Partitioning . We deem the cuts to be the member of a subset, and this problem becomes finding all subsets of valid cuts. If there are N cuts, we can choose whether to include each cut, so there are 2^N ways to cut our string. For O(2^N) problems, it’s usually a Subsets problem. . The thinking is that, we have a substring from start to i, s[start:i], called prefix. This is the next head item of the new node (path) in the DFS tree, we later append it to path, DFS down, and pop. But before that we should check if it is a valid palindrome. . For a fixed start, we loop through all substrings starting there, check if it satisfies the condition (palindrome in this case), if yes DFS down starting at i (the next char after s[old_start:i]). Again the template of DFS: . for i in range(old_start_ind, data.length): get next_head item check if next_head satisfies our condition if not, continue path.append(next_head) DFS(path, i or i+1, data) # i or i+1 greater than old_start_ind path.pop() . For &quot;aab&quot;, we have start = 0: |a|ab, |aa|b, |aab|; start = 1: a|a|b, a|ab|; start = 2: aa|b|; start = 3 == len, aab|, add one full path and return. start progresses by new recursion, i scans inside each recursion from start+1 to len+1. . This is the method to enumerate all substrings that satisfies some condition. . . Factor Combinations . For example, 8 -&gt; [[2, 2, 2], [2, 4]]. Do not include 1 or n as a factor, and each sublist should be ascending. . This is a typical DFS problem: list all solutions (combinations) or a certain decomposition problem, in this case, factorization. . class Solution: def factors(self, n): self.result = [] if n &lt;= 1: return self.result path = [] self.dfs(n, path) return self.result def dfs(self, n, path): # base case # len(path) == 1 is the case [n], which is not included # in this question if n == 1 and len(path) &gt; 1: self.result.append(list(path)) return # recursion # factor must include n itself # or when it&#39;s down to the last factor, it&#39;s not added for factor in xrange(2, n+1): # check if it&#39;s a factor of not if n % factor != 0: continue # ensure ascending order if path == [] or path[-1] &lt;= factor: path.append(factor) self.dfs(n/factor, path) path.pop() return . 3 points that need special attention in this particular problem, . For n &lt;= 3, result = [] | For the loop from 2 to n, must include n: for factor in xrange(2, n+1): .... Because if we don’t include n, the factors are not added to path. For example, | . If we use xrange(2, n)... Input: 8 Ex1: 1st level DFS: n = 8, factor = 2, dfs(n/2 = 4, [2]) 2nd level DFS: n = 4, factor = 2, dfs(n/2 = 2, [2, 2]) 3rd level DFS: n = 2, factor is in xrange(2, 2) which is nothing, abort Failed to add path [2, 2, 2] Ex2: 1st level DFS: n = 8, factor = 2, dfs(n/2 = 4, [2]) 2nd level DFS: n = 4, factor is in xrange(2, 4), factor cannot reach 4 Failed to add path [2, 4] . To ensure ascending order in paths, check for path == [] or path[-1] &lt;= factor before append(factor). | . (Note that Python has short circuit evaluation in the conditionals. For or it means that if path == [], the latter part(s) won’t be checked. So there won’t be a case where path[-1] does not exist in this expression.) .",
            "url": "http://blog.logancyang.com/note/algo/2015/10/03/graphsearch.html",
            "relUrl": "/note/algo/2015/10/03/graphsearch.html",
            "date": " • Oct 3, 2015"
        }
        
    
  
    
        ,"post15": {
            "title": "Handy Python Notes",
            "content": "Key Concepts . Immutable = hashable = can be element of a set (or key of dict) | mutable = unhashable = cannot be element of a set (or key of dict) . | immutables in Python: int, float, long, complex | str | tuple | bytes | frozenset | . | mutables byte array | list | set | dict | . | Do not modify an container while looping over it. Create a copy and loop on that. . | DO NOT ASSIGN STRING CHAR AS IN LIST!!! list(string), modify, and ‘’.join(list) to get string back | To sort a string: | . | . &gt;&gt;&gt; s = &quot;dcba&quot; &gt;&gt;&gt; sorted_s_list = sorted(s) [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;] &gt;&gt;&gt; sorted_s = &#39;&#39;.join(sorted(s)) &quot;abcd&quot; . An example of function: | . S = [9, 9, 8, 7, 6] # cannot swap S items without passing S in, if a and b are indices def swapWrong(a, b): X tmp = a a = b b = tmp &gt;&gt;&gt; swapWrong(S[0], S[3]) &gt;&gt;&gt; S [9, 9, 8, 7, 6] def swap(S, a, b): V tmp = S[a] S[b] = S[a] S[a] = tmp &gt;&gt;&gt; swap(S, 0, 3) &gt;&gt;&gt; S [7, 9, 8, 9, 6] . . Swap 2 Elements in a List . Usually in all languages it looks like, | . tmp = A[i] A[i] = A[j] A[j] = tmp . In Python it can be as simple as A[i], A[j] = A[j], A[i]! | . . The List Count Method . The list data structure in Python has a method list.count(element) which takes O(n) time. | Can be high dimensional list, e.g. board[:m][:n].count(element). Side note: set(2DList) =&gt; 1DSet | . | If we want all counts, using a dict as a histogram gives all counts in one pass. | . . Use Dict as a Histogram . Don’t forget to check if key exists. | . for item in list: if item not in map: map[item] = 0 map[item] += 1 . A better way, use dict.get(key, default=None). default – This is the Value to be returned in case key does not exist. | . | . d = {} for color in colors: d[color] = d.get(color, 0) + 1 . . Zip 2 Lists into a Dictionary . zip two lists into a list of tuples | dict the list of tuples into a dictionary: note, the tuples must have length 2, error if 3 | an easy way of constructing a reversed dictionary | . &gt;&gt;&gt; a = [1, 2, 3] &gt;&gt;&gt; b = [&#39;x&#39;, &#39;y&#39;, &#39;z&#39;] &gt;&gt;&gt; ab = zip(a, b) &gt;&gt;&gt; ab [(1, &#39;x&#39;), (2, &#39;y&#39;), (3, &#39;z&#39;)] &gt;&gt;&gt; dict_ab = dict(ab) &gt;&gt;&gt; dict_ab {1: &#39;x&#39;, 2: &#39;y&#39;, 3: &#39;z&#39;} &gt;&gt;&gt; dict_ba = {value: key for key, value in dict_ab.iteritems()} &gt;&gt;&gt; dict_ba {&#39;y&#39;: 2, &#39;x&#39;: 1, &#39;z&#39;: 3} . . Check if a Float is a Whole Number . Two methods: (my_float).is_integer(), or my_float % 1 == 0 | The second one works because in Python, 5.5 % 1 = 0.5, 5.0 % 1 = 0.0, and 0.0 == 0 is True. | . . Get the Index of the Min/Max Value in a List S . If there are duplicates, these methods return the first occurrence | . # print max(enumerate(S)) # returns a tuple (ind, val), key is ind  print max(enumerate(S), key = lambda S: S[1]) # faster  print S.index(max(S)) # easier to read . . Infinity . float(&#39;inf&#39;) is positive inf, float(&#39;-inf&#39;) is negative | Be careful not to multiply inf by 0, it yields nan | . . Import .py File as Module . In Python, modeling a clustering as a set of sets is impossible since the elements of a set must be immutable. | Import self-defined module: put the file.py file into the same directory, and do | import module (as alias) | . . Closure . Closure: http://en.wikipedia.org/wiki/Closure_%28computer_programming%29 | Pass function as parameter into another function. | . def function_a(): def function_b(): . . Lambda . runtime function with no name | often used in conjunction with filter(), map(), reduce() | . def f (x): return x**2 ... &gt;&gt;&gt; print f(8) 64 &gt;&gt;&gt; &gt;&gt;&gt; g = lambda x: x**2 &gt;&gt;&gt; print g(8) 64 . Or pass a small function as argument | . &gt;&gt;&gt; pairs = [(1, &#39;one&#39;), (2, &#39;two&#39;), (3, &#39;three&#39;), (4, &#39;four&#39;)] &gt;&gt;&gt; pairs.sort(key=lambda pair: pair[1]) &gt;&gt;&gt; pairs [(4, &#39;four&#39;), (1, &#39;one&#39;), (3, &#39;three&#39;), (2, &#39;two&#39;)] . . Initialize a 2D Array (Nested List) . USE LIST COMPREHENSION! | DO NOT USE [[None] * ncol] * nrow]! [anything] * const has sublists pointing to the same object. | . S = [[0 for ncol in xrange(5)] for nrow in xrange(3)] &gt;&gt;&gt; S [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]] . Similarly, to initialize a triangle where ncol &lt;= nrow: don’t forget the nrow+1 | . | . S = [[0 for ncol in xrange(nrow+1)] for nrow in xrange(3)] &gt;&gt;&gt; S [[0], [0, 0], [0, 0, 0]] . To initialize a contant 2D(or even higher dimensional) list with the shape of a given list “tri” | . tri = [ [2], [3,4], [6,5,7], [4,1,8,3] ] &gt;&gt;&gt; f = [[0 for item in tri[nrow]] for nrow in xrange(len(tri))] [[0], [0, 0], [0, 0, 0], [0, 0, 0, 0]] . To initialize a 2D list f1 where the value of the first row is 0 to j and the first col is 0 to i. The commas and colons should be removed, just to make it more readable. | . &gt;&gt;&gt; f1 = [[j if i == 0, else: i if j == 0, else: 0 for j in xrange(4)] for i in xrange(4)] [[0, 1, 2, 3], [1, 0, 0, 0], [2, 0, 0, 0], [3, 0, 0, 0]] . . Flatten a 2D Array . sublist in matrix, then, item in sublist. | Nested loops in List Comprehension: outer loop first, then inner loop. | flatten = [item for sublist in matrix for item in sublist] | . # equivalent to flatten = [] for sublist in matrix: for item in sublist: flatten.append(item) . . Find Min/Max in 2D Nested List . &gt;&gt;&gt; S = [[random.randint(1, 10) for ncol in xrange(nrow+1)] for nrow in xrange(3)] &gt;&gt;&gt; S [[3], [1, 2], [5, 9, 2]] &gt;&gt;&gt; min(min(sublist) for sublist in S) 1 . . To Check a List of Booleans to Yield a Boolean: all(), any() . list S = [True, True, True] &gt;&gt;&gt; all(S) is True True &gt;&gt;&gt; any(S) is False False . Beware, DO NOT use it like all([2, 2, 2]) == 2, all() and any() are for booleans inside. | . . If-Else in List Comprehension . To initialize a list f from list S, if S[i] == 0, f[i] = 1, else f[i] = 0. Here is a 2D example: | . &gt;&gt;&gt; f = [[1 if S[i][j] == 0 else 0 for j in xrange(len(S[0]))] for i in xrange(len(S))] . [expr if S[] == val else expr for i in iterable] . | What about elif?? . | . l = [1, 2, 3, 4, 5] for values in l: if values==1: print &#39;yes&#39; elif values==2: print &#39;no&#39; else: print &#39;idle&#39; &gt;&gt;&gt; [&#39;yes&#39; if v == 1 else &#39;no&#39; if v == 2 else &#39;idle&#39; for v in l] [&#39;yes&#39;, &#39;no&#39;, &#39;idle&#39;, &#39;idle&#39;, &#39;idle&#39;] . To initialize a 2D (4*4) list f where f[i][0] = i, f[0][j] = j, else 0: | . &gt;&gt;&gt; [[j if i == 0 else i if j == 0 else 0 for j in xrange(4)] for i in xrange(4)] [[0, 1, 2, 3], [1, 0, 0, 0], [2, 0, 0, 0], [3, 0, 0, 0]] . [expr if i == val else expr if j == val else expr for i in iterable] | very powerful pythonic list comprehension!! | . . Create Dict with Keys from a Set or List . # create a dict with keys from a set (or list), value 0  T = set([1, 2, 4])  dictT = dict((element, 0) for element in T)  print dictT # output: {1: 0, 2: 0, 4: 0} . . Using Lists as Stacks . It’s very easy to use lists as stacks with append() and pop(). Last in first out. | . &gt;&gt;&gt; stack = [3, 4, 5] &gt;&gt;&gt; stack.append(6) &gt;&gt;&gt; stack.append(7) &gt;&gt;&gt; stack [3, 4, 5, 6, 7] &gt;&gt;&gt; stack.pop() 7 &gt;&gt;&gt; stack [3, 4, 5, 6] &gt;&gt;&gt; stack.pop() 6 &gt;&gt;&gt; stack.pop() 5 &gt;&gt;&gt; stack [3, 4] . . Using Lists as Queues . We can use pop(0) and append() to implement the queue behavior but it’s not efficient. Because popping from the first position is slow, all elements must shift. Instead it’s better to use collections.deque and its append() and popleft() methods. | . &gt;&gt;&gt; from collections import deque &gt;&gt;&gt; queue = deque([&quot;Eric&quot;, &quot;John&quot;, &quot;Michael&quot;]) &gt;&gt;&gt; queue.append(&quot;Terry&quot;) # Terry arrives &gt;&gt;&gt; queue.append(&quot;Graham&quot;) # Graham arrives &gt;&gt;&gt; queue.popleft() # The first to arrive now leaves &#39;Eric&#39; &gt;&gt;&gt; queue.popleft() # The second to arrive now leaves &#39;John&#39; &gt;&gt;&gt; queue # Remaining queue in order of arrival deque([&#39;Michael&#39;, &#39;Terry&#39;, &#39;Graham&#39;]) . . A Peculiar Usage of Augment Assignment in Python . The augmented addition operator += behaves unexpectedly in the following case, | . &gt;&gt;&gt; lst = [] &gt;&gt;&gt; print lst + &quot;123&quot; TypeError: can only concatenate list (not &quot;str&quot;) to list &gt;&gt;&gt; lst += &quot;123&quot; &gt;&gt;&gt; print lst [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;] . What happened is that += calls iadd() method and try to modify the list in-place, while adding all the elements of the iterable on the right to the list. | . . Class Method vs. Static Method . @staticmethod function is nothing more than a function defined inside a class. It is callable without instantiating the class first. It’s definition is immutable via inheritance. . | @classmethod function also callable without instantiating the class, but its definition follows Sub class, not Parent class, via inheritance. That’s because the first argument for @classmethod function must always be cls (class). . | . To be continued… . . Some good resources: . The tutorial from Python documentation | Data Structures | I/O | Errors and Exceptions | Classes, Iterators and Generators | Standard Library | .",
            "url": "http://blog.logancyang.com/note/python/2015/07/21/handy-python-notes.html",
            "relUrl": "/note/python/2015/07/21/handy-python-notes.html",
            "date": " • Jul 21, 2015"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, I’m Logan (Chao) Yang, a machine learning engineer and creative coder. . My main interests are machine learning, physics simulation, math animation and music production. This is my fastpages blog for notes, code, Jupyter notebooks, and essays. . I share my sketches, simulations and videos at my personal website. . Feel free to connect with me on LinkedIn. . I also publish on Medium, and occasionally share new things on Twitter. .",
          "url": "http://blog.logancyang.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "http://blog.logancyang.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}