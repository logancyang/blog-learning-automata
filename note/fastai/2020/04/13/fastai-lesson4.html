<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>FastAI Lesson 4: NLP; Tabular Data; Collaborative Filtering and Embeddings | Learning Automata</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="FastAI Lesson 4: NLP; Tabular Data; Collaborative Filtering and Embeddings" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="fast.ai note series" />
<meta property="og:description" content="fast.ai note series" />
<link rel="canonical" href="http://blog.logancyang.com/note/fastai/2020/04/13/fastai-lesson4.html" />
<meta property="og:url" content="http://blog.logancyang.com/note/fastai/2020/04/13/fastai-lesson4.html" />
<meta property="og:site_name" content="Learning Automata" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-04-13T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"fast.ai note series","headline":"FastAI Lesson 4: NLP; Tabular Data; Collaborative Filtering and Embeddings","@type":"BlogPosting","dateModified":"2020-04-13T00:00:00-05:00","datePublished":"2020-04-13T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://blog.logancyang.com/note/fastai/2020/04/13/fastai-lesson4.html"},"url":"http://blog.logancyang.com/note/fastai/2020/04/13/fastai-lesson4.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://blog.logancyang.com/feed.xml" title="Learning Automata" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-164976898-2','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>FastAI Lesson 4: NLP; Tabular Data; Collaborative Filtering and Embeddings | Learning Automata</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="FastAI Lesson 4: NLP; Tabular Data; Collaborative Filtering and Embeddings" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="fast.ai note series" />
<meta property="og:description" content="fast.ai note series" />
<link rel="canonical" href="http://blog.logancyang.com/note/fastai/2020/04/13/fastai-lesson4.html" />
<meta property="og:url" content="http://blog.logancyang.com/note/fastai/2020/04/13/fastai-lesson4.html" />
<meta property="og:site_name" content="Learning Automata" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-04-13T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"fast.ai note series","headline":"FastAI Lesson 4: NLP; Tabular Data; Collaborative Filtering and Embeddings","@type":"BlogPosting","dateModified":"2020-04-13T00:00:00-05:00","datePublished":"2020-04-13T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://blog.logancyang.com/note/fastai/2020/04/13/fastai-lesson4.html"},"url":"http://blog.logancyang.com/note/fastai/2020/04/13/fastai-lesson4.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="http://blog.logancyang.com/feed.xml" title="Learning Automata" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-164976898-2','auto');ga('require','displayfeatures');ga('send','pageview');</script>


    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Learning Automata</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About Me</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">FastAI Lesson 4: NLP; Tabular Data; Collaborative Filtering and Embeddings</h1><p class="page-description">fast.ai note series</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-04-13T00:00:00-05:00" itemprop="datePublished">
        Apr 13, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      9 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#note">note</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#fastai">fastai</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#nlp-continued-from-lesson-3">NLP Continued from lesson 3</a></li>
<li class="toc-entry toc-h2"><a href="#tabular-data-using-deep-learning-embeddings-for-categorical-variables">Tabular Data using Deep Learning, Embeddings for Categorical Variables</a></li>
<li class="toc-entry toc-h2"><a href="#collaborative-filtering-and-embeddings">Collaborative Filtering and Embeddings</a></li>
</ul><h2 id="nlp-continued-from-lesson-3">
<a class="anchor" href="#nlp-continued-from-lesson-3" aria-hidden="true"><span class="octicon octicon-link"></span></a>NLP Continued from lesson 3</h2>

<p>2018 was the year that <strong>transfer learning</strong> started working well with NLP. You train a language model on a very large dataset to “learn to speak English”, and then do transfer learning for other purposes such as text classification, NER, etc.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Language model on all of Wikipedia (Wikitext 103, ~1B tokens)
    ==&gt;
    Fine-tune this language model using your target corpus
    (in this case, IMDb movie reviews)
        ==&gt;
        Extract the encoder from this fine tuned language model, and
        pair it with a classifier. Then fine-tune this model for the
        final classification task (in this case, sentiment analysis)
</code></pre></div></div>

<p>We would think that an LM trained on Wikipedia wouldn’t work well for slangs and informal language, but actually when it gets finetuned with your target corpus, it works.</p>

<p>For the imdb notebook, the sample data has columns <code class="highlighter-rouge">label, text, is_valid (in validation set)</code>.</p>

<p>For tokenization, most often a token is a word, sometimes it can be <code class="highlighter-rouge">'s</code> or a punctuation or symbol.</p>

<p>The full unique set of tokens is <code class="highlighter-rouge">vocabulary</code>. Here a limit of 60K tokens and a frequency threshold of 2 are applied to the vocab.</p>

<p><code class="highlighter-rouge">xxunk</code> means unknown token, it means that word was not common enough to be in the vocab.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Using the data block API is the better way to create the DataBunch
</span><span class="n">data</span> <span class="o">=</span> <span class="p">(</span><span class="n">TextList</span><span class="o">.</span><span class="n">from_csv</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s">'texts.csv'</span><span class="p">,</span> <span class="n">cols</span><span class="o">=</span><span class="s">'text'</span><span class="p">)</span>
                <span class="c1"># how to split out validation set based on bool column
</span>                <span class="o">.</span><span class="n">split_from_df</span><span class="p">(</span><span class="n">col</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
                <span class="c1"># which column is the label
</span>                <span class="o">.</span><span class="n">label_from_df</span><span class="p">(</span><span class="n">cols</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="o">.</span><span class="n">databunch</span><span class="p">())</span>
</code></pre></div></div>

<p>The reviews are in a training and test set following an imagenet structure. The only difference is that there is an <code class="highlighter-rouge">unsup</code> folder on top of <code class="highlighter-rouge">train</code> and <code class="highlighter-rouge">test</code> that contains the unlabelled data.</p>

<p>Training an LM on Wiki data takes 2-3 days on a decent GPU, no need to do that, just download the pretrained model.</p>

<p>Even if we have a large target corpus, we still prefer to start from the pretrained model on <a href="https://einstein.ai/research/blog/the-wikitext-long-term-dependency-language-modeling-dataset">wikitext-103</a>, there is no reason to start from random.</p>

<p>We are going to use that ‘knowledge’ of the English language to build our classifier, but first, like for computer vision, we need to fine-tune the pretrained model to our particular dataset. Because the English of the reviews left by people on IMDB isn’t the same as the English of wikipedia, we’ll need to adjust the parameters of our model by a little bit. Plus there might be some words that would be extremely common in the reviews dataset but would be barely present in wikipedia, and therefore might not be part of the vocabulary the model was trained on.</p>

<p>This is where the unlabelled data is going to be useful to us, as we can use it to fine-tune our model. Let’s create our data object with the data block API (next line takes a few minutes).</p>

<div class="Toast">
   <span class="Toast-icon"><svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>
   <span class="Toast-content">
        Trick: make sure to train the LM on <b>all of the data</b> including the test set, because it doesn't matter, there is no label for LM.
    </span>
</div>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code>           <span class="c1">#Inputs: all the text files in path
</span><span class="n">data_lm</span> <span class="o">=</span> <span class="p">(</span><span class="n">TextList</span><span class="o">.</span><span class="n">from_folder</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
           <span class="c1">#We may have other temp folders that contain text files so
</span>           <span class="c1"># we only keep what's in train and test
</span>            <span class="o">.</span><span class="n">filter_by_folder</span><span class="p">(</span><span class="n">include</span><span class="o">=</span><span class="p">[</span><span class="s">'train'</span><span class="p">,</span> <span class="s">'test'</span><span class="p">,</span> <span class="s">'unsup'</span><span class="p">])</span>
           <span class="c1">#We randomly split and keep 10% (10,000 reviews) for validation
</span>            <span class="o">.</span><span class="n">split_by_rand_pct</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
           <span class="c1">#We want to do a language model so we label accordingly
</span>            <span class="o">.</span><span class="n">label_for_lm</span><span class="p">()</span>
            <span class="o">.</span><span class="n">databunch</span><span class="p">(</span><span class="n">bs</span><span class="o">=</span><span class="n">bs</span><span class="p">))</span>
<span class="n">data_lm</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s">'data_lm.pkl'</span><span class="p">)</span>

<span class="n">data_lm</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s">'data_lm.pkl'</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="n">bs</span><span class="p">)</span>
<span class="n">data_lm</span><span class="o">.</span><span class="n">show_batch</span><span class="p">()</span>

<span class="c1"># This is an RNN
</span><span class="n">learn</span> <span class="o">=</span> <span class="n">language_model_learner</span><span class="p">(</span><span class="n">data_lm</span><span class="p">,</span> <span class="n">AWD_LSTM</span><span class="p">,</span> <span class="n">drop_mult</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="c1">#Downloading https://s3.amazonaws.com/fast-ai-modelzoo/wt103-fwd
</span></code></pre></div></div>

<p>It takes very long to even train one epoch on an average GPU. Mine took ~20min for 1 epoch before <code class="highlighter-rouge">unfreeze</code>. It can easily take overnight to train a good model.</p>

<p>For LM on Wikipedia, ~30% accuracy is quite good. For more specific documents like medical or legal, it can be higher.</p>

<p>After training the LM on the target corpus, we save the encoder</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">learn</span><span class="o">.</span><span class="n">save_encoder</span><span class="p">(</span><span class="s">'fine_tuned_enc'</span><span class="p">)</span>
</code></pre></div></div>

<p>Next, we can build the classifier.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code>            <span class="c1">#grab all the text files in path, MUST use the same vocab and order
</span><span class="n">data_clas</span> <span class="o">=</span> <span class="p">(</span><span class="n">TextList</span><span class="o">.</span><span class="n">from_folder</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">vocab</span><span class="o">=</span><span class="n">data_lm</span><span class="o">.</span><span class="n">vocab</span><span class="p">)</span>
             <span class="c1">#split by train and valid folder (that only keeps 'train' and
</span>             <span class="c1">#'test' so no need to filter)
</span>             <span class="o">.</span><span class="n">split_by_folder</span><span class="p">(</span><span class="n">valid</span><span class="o">=</span><span class="s">'test'</span><span class="p">)</span>
             <span class="c1">#label them all with their folders
</span>             <span class="o">.</span><span class="n">label_from_folder</span><span class="p">(</span><span class="n">classes</span><span class="o">=</span><span class="p">[</span><span class="s">'neg'</span><span class="p">,</span> <span class="s">'pos'</span><span class="p">])</span>
             <span class="o">.</span><span class="n">databunch</span><span class="p">(</span><span class="n">bs</span><span class="o">=</span><span class="n">bs</span><span class="p">))</span>

<span class="n">data_clas</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s">'data_clas.pkl'</span><span class="p">)</span>

<span class="n">learn</span> <span class="o">=</span> <span class="n">text_classifier_learner</span><span class="p">(</span><span class="n">data_clas</span><span class="p">,</span> <span class="n">AWD_LSTM</span><span class="p">,</span> <span class="n">drop_mult</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">learn</span><span class="o">.</span><span class="n">load_encoder</span><span class="p">(</span><span class="s">'fine_tuned_enc'</span><span class="p">)</span>

<span class="n">learn</span><span class="o">.</span><span class="n">lr_find</span><span class="p">()</span>
<span class="n">learn</span><span class="o">.</span><span class="n">recorder</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>

<span class="c1"># moms is the momentum for the optimizer
</span><span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2e-2</span><span class="p">,</span> <span class="n">moms</span><span class="o">=</span><span class="p">(</span><span class="mf">0.8</span><span class="p">,</span><span class="mf">0.7</span><span class="p">))</span>

<span class="n">learn</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s">'first'</span><span class="p">)</span>
<span class="n">learn</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'first'</span><span class="p">)</span>

<span class="c1"># NOTE: This only unfreezes the last 2 layers
</span><span class="n">learn</span><span class="o">.</span><span class="n">freeze_to</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># NOTE: The 2.6**4 stuff is called discriminative learning rate
# the range controls the lr for different layers since they learn
# best at different rate
</span><span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">slice</span><span class="p">(</span><span class="mf">1e-2</span><span class="o">/</span><span class="p">(</span><span class="mf">2.6</span><span class="o">**</span><span class="mi">4</span><span class="p">),</span><span class="mf">1e-2</span><span class="p">),</span> <span class="n">moms</span><span class="o">=</span><span class="p">(</span><span class="mf">0.8</span><span class="p">,</span><span class="mf">0.7</span><span class="p">))</span>

<span class="n">learn</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s">'second'</span><span class="p">)</span>
<span class="n">learn</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'second'</span><span class="p">)</span>

<span class="c1"># Unfreeze last 3 layers
</span><span class="n">learn</span><span class="o">.</span><span class="n">freeze_to</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span>
<span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">slice</span><span class="p">(</span><span class="mf">5e-3</span><span class="o">/</span><span class="p">(</span><span class="mf">2.6</span><span class="o">**</span><span class="mi">4</span><span class="p">),</span><span class="mf">5e-3</span><span class="p">),</span> <span class="n">moms</span><span class="o">=</span><span class="p">(</span><span class="mf">0.8</span><span class="p">,</span><span class="mf">0.7</span><span class="p">))</span>

<span class="n">learn</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s">'third'</span><span class="p">)</span>
<span class="n">learn</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'third'</span><span class="p">)</span>

<span class="c1"># Unfreeze whole model to finetune
</span><span class="n">learn</span><span class="o">.</span><span class="n">unfreeze</span><span class="p">()</span>
<span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="nb">slice</span><span class="p">(</span><span class="mf">1e-3</span><span class="o">/</span><span class="p">(</span><span class="mf">2.6</span><span class="o">**</span><span class="mi">4</span><span class="p">),</span><span class="mf">1e-3</span><span class="p">),</span> <span class="n">moms</span><span class="o">=</span><span class="p">(</span><span class="mf">0.8</span><span class="p">,</span><span class="mf">0.7</span><span class="p">))</span>

<span class="n">learn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="s">"I really loved that movie, it was awesome!"</span><span class="p">)</span>
</code></pre></div></div>

<p>NOTE: When creating the classifier learner, we MUST have the same <code class="highlighter-rouge">vocab</code> as the pretrained language model. The line</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data_clas</span> <span class="o">=</span> <span class="p">(</span><span class="n">TextList</span><span class="o">.</span><span class="n">from_folder</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">vocab</span><span class="o">=</span><span class="n">data_lm</span><span class="o">.</span><span class="n">vocab</span><span class="p">)</span>
    <span class="o">...</span>
</code></pre></div></div>

<p>is very important. Have the right <code class="highlighter-rouge">data_lm</code> in memory and create <code class="highlighter-rouge">data_clas</code> with its <code class="highlighter-rouge">vocab</code>, or there will be an error when loading the pretrained model via <code class="highlighter-rouge">learn.load_encoder('fine_tuned_enc')</code> and it will say</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Error(s) in loading state_dict for AWD_LSTM: size mismatch...
</code></pre></div></div>

<div class="Toast">
   <span class="Toast-icon"><svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>
   <span class="Toast-content">
        Trick: for text classification, <b>unfreezing one layer at a time and train some more</b> is an effective strategy.
    </span>
</div>

<div class="Toast">
   <span class="Toast-icon"><svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>
   <span class="Toast-content">
        Trick: Jeremy created a random forest to find best hyperparameter setting. The best number for discriminative learning rate is 2.6^4. This is similar to AutoML for hyperparam search.
    </span>
</div>

<p>For training Chinese language models, search the forum for more info.</p>

<h2 id="tabular-data-using-deep-learning-embeddings-for-categorical-variables">
<a class="anchor" href="#tabular-data-using-deep-learning-embeddings-for-categorical-variables" aria-hidden="true"><span class="octicon octicon-link"></span></a>Tabular Data using Deep Learning, Embeddings for Categorical Variables</h2>

<p><img src="/images/fastai/tabular_data_usecases.png" alt="alt text" title="Tabular Data Use Cases"></p>

<p>People were skeptical about using neural nets on tabular data, they often use logistic regression, random forest, gradient boosting machines to do it. In fact, NN is extremely useful for tabular data.</p>

<p>With NN, you don’t need to hand engineer features as much as before. It’s more accurate and requires less maintenance. Jemery used to use Random Forest 99% of the time for tabular data, now he uses NN 90% of the time.</p>

<p>Nobody else created a library for NN on tabular data, fastai has <code class="highlighter-rouge">fastai.tabular</code>. In the notebook <code class="highlighter-rouge">lesson4-tabular</code> there is a detailed example.</p>

<p>It assumes the data is in a pandas dataframe. Pandas can read from csv, relational db, Spark and Hadoop.</p>

<p>The independent variables (features) can be continuous or categorical. With NN, we use <strong>embeddings</strong> for categorical variables.</p>

<p>Instead of having “transform”s as in CV such as brightening, flipping, normalization etc., we have “processor”s for tabular data. The difference is that transforms are for data augmentation and are different each time, but processors are run once ahead of time.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dep_var</span> <span class="o">=</span> <span class="s">'salary'</span>
<span class="n">cat_names</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s">'workclass'</span><span class="p">,</span> <span class="s">'education'</span><span class="p">,</span> <span class="s">'marital-status'</span><span class="p">,</span> <span class="s">'occupation'</span><span class="p">,</span>
    <span class="s">'relationship'</span><span class="p">,</span> <span class="s">'race'</span>
<span class="p">]</span>
<span class="n">cont_names</span> <span class="o">=</span> <span class="p">[</span><span class="s">'age'</span><span class="p">,</span> <span class="s">'fnlwgt'</span><span class="p">,</span> <span class="s">'education-num'</span><span class="p">]</span>

<span class="c1"># 1. Deal with missing values in some way
# 2. Use pandas categorical variables
# 3. Normalize continuous variables by mean 0 and std 1
</span><span class="n">procs</span> <span class="o">=</span> <span class="p">[</span><span class="n">FillMissing</span><span class="p">,</span> <span class="n">Categorify</span><span class="p">,</span> <span class="n">Normalize</span><span class="p">]</span>

<span class="n">data</span> <span class="o">=</span> <span class="p">(</span><span class="n">TabularList</span><span class="o">.</span><span class="n">from_df</span><span class="p">(</span>
    <span class="n">df</span><span class="p">,</span> <span class="n">path</span><span class="o">=</span><span class="n">path</span><span class="p">,</span> <span class="n">cat_names</span><span class="o">=</span><span class="n">cat_names</span><span class="p">,</span> <span class="n">cont_names</span><span class="o">=</span><span class="n">cont_names</span><span class="p">,</span> <span class="n">procs</span><span class="o">=</span><span class="n">procs</span><span class="p">)</span>
    <span class="c1"># when split validation set, must have contiguous indices
</span>    <span class="c1"># think time periods, video frames, or other structure in data
</span>    <span class="o">.</span><span class="n">split_by_idx</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">800</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)))</span>
    <span class="o">.</span><span class="n">label_from_df</span><span class="p">(</span><span class="n">cols</span><span class="o">=</span><span class="n">dep_var</span><span class="p">)</span>
    <span class="o">.</span><span class="n">add_test</span><span class="p">(</span><span class="n">test</span><span class="p">)</span>
    <span class="o">.</span><span class="n">databunch</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">data</span><span class="o">.</span><span class="n">show_batch</span><span class="p">(</span><span class="n">rows</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">learn</span> <span class="o">=</span> <span class="n">tabular_learner</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="p">[</span><span class="mi">200</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span> <span class="n">metrics</span><span class="o">=</span><span class="n">accuracy</span><span class="p">)</span>
<span class="n">learn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">)</span>

<span class="c1"># Inference
</span><span class="n">row</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">learn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">row</span><span class="p">)</span>
</code></pre></div></div>

<p>For time series tabular data, you generally don’t use RNN for them. Instead, you can time box them into day_of_week, time_of_day, etc. and it will give you state of the art result.</p>

<h2 id="collaborative-filtering-and-embeddings">
<a class="anchor" href="#collaborative-filtering-and-embeddings" aria-hidden="true"><span class="octicon octicon-link"></span></a>Collaborative Filtering and Embeddings</h2>

<p>When you have data about who-bought-what, who-liked-what, you can have two columns like [userId, productId] in the most basic form. Other metadata can be added, like timestamp, review, etc.</p>

<p>This matrix is very sparse because most users didn’t buy most products / watched most movies.</p>

<p>In this example our data has <code class="highlighter-rouge">userId, movieId, rating, timestamp</code>.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ratings</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s">'ratings.csv'</span><span class="p">)</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">CollabDataBunch</span><span class="o">.</span><span class="n">from_df</span><span class="p">(</span><span class="n">ratings</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">y_range</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mf">5.5</span><span class="p">]</span>

<span class="n">learn</span> <span class="o">=</span> <span class="n">collab_learner</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">n_factors</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">y_range</span><span class="o">=</span><span class="n">y_range</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>For recommender systems, a big challenge is the Cold Start problem</strong>. It means that we particularly care about recommending <strong>new movies</strong> or recommend relevant movies to <strong>new users</strong> which we don’t have any data for. The solution is to have a second model on user or movie metadata to quatify the similarities.</p>

<p>Netflix fixed the cold start problem by UX. It asks a new user whether they like the movies they show as a survey. For new movies, they just need to let some hundreds of people watch it and rate them. It wasn’t quite a cold start problem for Netflix.</p>

<p><strong>But for selling products, you might not want people to look at your range of products. You could for example find the metadata of the users such as what geography they are from, their age, gender, and other features to predict whether they would like something.</strong></p>

<p>Collaborative filtering is specifically for when you already have some data about the preferences of the users.</p>

<p><strong>A user has an embedding vector. A movie has an embedding vector.</strong> A bias term needs to be added in the user embedding and can be interpretted as <em>the user’s tendency to like movies in general regardless of what movie</em>. Similarly, a bias term in the movie embedding is like <em>the likeability of a movie regardless of users</em>.</p>

<p>The target value is the rating in the range 0 to 5. We dot the user embedding and the movie (item) embedding along with the weights, and <em>pass it through a sigmoid</em> (and times 5) to get a numbder between 0 - 5. <em>Notice that this is actually a “logistic regression” (linear layer on inputs and a sigmoid) but with MSE loss and target variables as numbers between 0 - 5.</em></p>

<div class="Toast Toast--warning googoo">
   <span class="Toast-icon"><svg class="octicon octicon-alert" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 000 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 00.01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>
   <span class="Toast-content">Question: Sigmoid with MSE is interesting, is there a maximum likelihood explanation for this?</span>
</div>

<p>Note that this mapping from the product of embeddings to the range [0, 5] is still regression and not classification, so the loss used is MSE and not cross entropy.</p>

<p>Why pass through the sigmoid? It makes the model learn easier and let the weights converge to relevant results. It is very common to use sigmoid or softmax as the last layer to produce the output.</p>

<div class="Toast Toast--warning googoo">
   <span class="Toast-icon"><svg class="octicon octicon-alert octicon octicon-alert" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 000 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 00.01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>
   <span class="Toast-content">Question: it seems this is a sigmoid(linear model) with MSE, the optimization is nonconvex. How is it done?</span>
</div>


  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="logancyang/blog-learning-automata"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/note/fastai/2020/04/13/fastai-lesson4.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Notes, code and essays by Logan Yang.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/logancyang" title="logancyang"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/logancyang" title="logancyang"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
