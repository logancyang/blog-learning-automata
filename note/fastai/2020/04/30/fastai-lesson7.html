<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>FastAI Lesson 7: Resnets from scratch; U-net; Generative adversarial networks; RNNs | Learning Automata</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="FastAI Lesson 7: Resnets from scratch; U-net; Generative adversarial networks; RNNs" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="fast.ai note series" />
<meta property="og:description" content="fast.ai note series" />
<link rel="canonical" href="http://blog.logancyang.com/note/fastai/2020/04/30/fastai-lesson7.html" />
<meta property="og:url" content="http://blog.logancyang.com/note/fastai/2020/04/30/fastai-lesson7.html" />
<meta property="og:site_name" content="Learning Automata" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-04-30T00:00:00-05:00" />
<script type="application/ld+json">
{"headline":"FastAI Lesson 7: Resnets from scratch; U-net; Generative adversarial networks; RNNs","dateModified":"2020-04-30T00:00:00-05:00","description":"fast.ai note series","datePublished":"2020-04-30T00:00:00-05:00","@type":"BlogPosting","url":"http://blog.logancyang.com/note/fastai/2020/04/30/fastai-lesson7.html","mainEntityOfPage":{"@type":"WebPage","@id":"http://blog.logancyang.com/note/fastai/2020/04/30/fastai-lesson7.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://blog.logancyang.com/feed.xml" title="Learning Automata" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-164976898-2','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>FastAI Lesson 7: Resnets from scratch; U-net; Generative adversarial networks; RNNs | Learning Automata</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="FastAI Lesson 7: Resnets from scratch; U-net; Generative adversarial networks; RNNs" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="fast.ai note series" />
<meta property="og:description" content="fast.ai note series" />
<link rel="canonical" href="http://blog.logancyang.com/note/fastai/2020/04/30/fastai-lesson7.html" />
<meta property="og:url" content="http://blog.logancyang.com/note/fastai/2020/04/30/fastai-lesson7.html" />
<meta property="og:site_name" content="Learning Automata" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-04-30T00:00:00-05:00" />
<script type="application/ld+json">
{"headline":"FastAI Lesson 7: Resnets from scratch; U-net; Generative adversarial networks; RNNs","dateModified":"2020-04-30T00:00:00-05:00","description":"fast.ai note series","datePublished":"2020-04-30T00:00:00-05:00","@type":"BlogPosting","url":"http://blog.logancyang.com/note/fastai/2020/04/30/fastai-lesson7.html","mainEntityOfPage":{"@type":"WebPage","@id":"http://blog.logancyang.com/note/fastai/2020/04/30/fastai-lesson7.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="http://blog.logancyang.com/feed.xml" title="Learning Automata" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-164976898-2','auto');ga('require','displayfeatures');ga('send','pageview');</script>


    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Learning Automata</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About Me</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">FastAI Lesson 7: Resnets from scratch; U-net; Generative adversarial networks; RNNs</h1><p class="page-description">fast.ai note series</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-04-30T00:00:00-05:00" itemprop="datePublished">
        Apr 30, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      9 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#note">note</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#fastai">fastai</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#refactor">Refactor</a></li>
<li class="toc-entry toc-h2"><a href="#introduce-the-residual-block">Introduce the Residual Block</a></li>
<li class="toc-entry toc-h2"><a href="#u-net">U-Net</a></li>
<li class="toc-entry toc-h2"><a href="#image-restoration-with-u-net-and-gan">Image Restoration with U-Net and GAN</a>
<ul>
<li class="toc-entry toc-h3"><a href="#wgan">WGAN</a></li>
<li class="toc-entry toc-h3"><a href="#perceptual-loss-feature-loss">Perceptual Loss (Feature Loss)</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#recap">Recap</a></li>
<li class="toc-entry toc-h2"><a href="#recurrent-neural-network">Recurrent Neural Network</a></li>
<li class="toc-entry toc-h2"><a href="#homework">Homework</a></li>
<li class="toc-entry toc-h2"><a href="#answers-and-comments-from-jeremy">Answers and Comments from Jeremy</a></li>
</ul><p>Notebook: <code class="highlighter-rouge">lesson7-resnet-mnist</code></p>

<p>PyTorch puts channel at the 1st dimension by default. An image in MNIST is a (1,28, 28) rank 3 tensor.</p>

<p>In fastai, there is a difference between validation and test set. Test set doesn’t have label. Use validation set for model development. If you want to do inference on many things at a time and not one at a time, set the data as <code class="highlighter-rouge">test</code> instead of <code class="highlighter-rouge">valid</code>.</p>

<p>Initial steps:</p>

<ol>
  <li>Create ItemList from image folders</li>
  <li>Split into <code class="highlighter-rouge">train</code> and <code class="highlighter-rouge">valid</code>.</li>
</ol>

<div class="Toast">
   <span class="Toast-icon"><svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>
   <span class="Toast-content">
        Tip: for data augmentation, MNIST can't have much transformation, you can't flip it because it changes the meaning of the number, you can't zoom because it's low res. The only transform is to add random padding. <b>Do this transform on training set but not validation set</b>.
    </span>
</div>

<p>If not using a pretrained model, don’t pass in <code class="highlighter-rouge">stat</code> in <code class="highlighter-rouge">normalize()</code> for databunch, it grabs a subset of the data at random and figures out how to normalize.</p>

<p><code class="highlighter-rouge">plot_multi(_plot, nrow, ncol, figsize=())</code> is a fastai function that plots multiple examples in a grid. Define <code class="highlighter-rouge">_plot</code> first for what to show.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Showing a batch of data using the DataBlock API
</span><span class="n">xb</span><span class="p">,</span><span class="n">yb</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">one_batch</span><span class="p">()</span>
<span class="n">xb</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">yb</span><span class="o">.</span><span class="n">shape</span>

<span class="c1"># DataBunch has .show_batch()
</span><span class="n">data</span><span class="o">.</span><span class="n">show_batch</span><span class="p">(</span><span class="n">rows</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
</code></pre></div></div>

<p>Then we define the convolution function with fixed kernel size, stride and padding.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># ni is # input channels, nf is # output filters (kernels)
</span><span class="k">def</span> <span class="nf">conv</span><span class="p">(</span><span class="n">ni</span><span class="p">,</span><span class="n">nf</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">ni</span><span class="p">,</span> <span class="n">nf</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="c1"># 1 channel in, 8 channels out (picked by us), output size 8*14*14
</span>    <span class="n">conv</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">8</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="c1"># 8 channel in, 16 channels out (picked by us), output size 16*7*7
</span>    <span class="n">conv</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span> <span class="c1"># 7
</span>    <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">16</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="c1"># 16 channel in, 32 channels out (picked by us), output size 32*4*4
</span>    <span class="n">conv</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span> <span class="c1"># 4
</span>    <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">32</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="c1"># 32 channel in, 16 channels out (picked by us), output size 16*2*2
</span>    <span class="n">conv</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span> <span class="c1"># 2
</span>    <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">16</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="c1"># 16 channel in, 10 channels out (picked by us), output size 10*1*1
</span>    <span class="n">conv</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="c1"># 1
</span>    <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span>
    <span class="n">Flatten</span><span class="p">()</span>     <span class="c1"># remove (1,1) grid
</span><span class="p">)</span>
</code></pre></div></div>

<div class="Toast">
   <span class="Toast-icon"><svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>
   <span class="Toast-content">
        Trick: <code>Flatten()</code> gets rid of all the unit axes (the axes with 1s)! <code>(10, 1, 1)</code> becomes just <code>(10,)</code>, a flat vector of dim 10!
    </span>
</div>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># create learner
</span><span class="n">learn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(),</span> <span class="n">metrics</span><span class="o">=</span><span class="n">accuracy</span><span class="p">)</span>
<span class="c1"># print learner summary
</span><span class="k">print</span><span class="p">(</span><span class="n">learn</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
<span class="c1"># pop data onto GPU
</span><span class="n">xb</span> <span class="o">=</span> <span class="n">xb</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="c1"># check model output shape
</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div></div>

<p>This is a model we built from scratch with a simple CNN architecture, it takes 12s to train on my GPU and got 98.8% accuracy! Already super good.</p>

<h2 id="refactor">
<a class="anchor" href="#refactor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Refactor</h2>

<p>fastai has <code class="highlighter-rouge">conv_layer</code> so we can skip writing all the batch norm and relu’s.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">conv2</span><span class="p">(</span><span class="n">ni</span><span class="p">,</span><span class="n">nf</span><span class="p">):</span> <span class="k">return</span> <span class="n">conv_layer</span><span class="p">(</span><span class="n">ni</span><span class="p">,</span><span class="n">nf</span><span class="p">,</span><span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">conv2</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span>   <span class="c1"># 14
</span>    <span class="n">conv2</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span>  <span class="c1"># 7
</span>    <span class="n">conv2</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span> <span class="c1"># 4
</span>    <span class="n">conv2</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span> <span class="c1"># 2
</span>    <span class="n">conv2</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="c1"># 1
</span>    <span class="n">Flatten</span><span class="p">()</span>      <span class="c1"># remove (1,1) grid
</span><span class="p">)</span>

<span class="n">learn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(),</span> <span class="n">metrics</span><span class="o">=</span><span class="n">accuracy</span><span class="p">)</span>

<span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">max_lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</code></pre></div></div>

<p>It’s the same as previous code, just looks better. Train 10 epochs, we can get to 99%+ accuracy.</p>

<h2 id="introduce-the-residual-block">
<a class="anchor" href="#introduce-the-residual-block" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduce the Residual Block</h2>

<p>The residual block is a revolutionary technique in computer vision.</p>

<p>Kaiming He et. al. at Microsoft Research initially found that a 56-layer CNN was performing worse than a 20-layer CNN which made no sense. He created an architecture where a 56-layer CNN <em>contains</em> the 20-layer CNN, by adding some skip connections that skipped some conv layers. That way, it must be as least as good as the 20-layer CNN because the deeper CNN could just set the skipped conv layers to 0 and only keep the identity links.</p>

<p><img src="/images/fastai/res-block.png" alt="Residual Block" align="middle"></p>

<p>Instead of having</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>output = conv( conv (x) )
</code></pre></div></div>

<p>The residual block is</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>output = conv( conv (x) ) + x
</code></pre></div></div>

<p>The result was that he won ImageNet that year (2015).</p>

<div class="Toast">
   <span class="Toast-icon"><svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>
   <span class="Toast-content">
        Trick: if an NN or GAN doesn't work so well, try replacing the conv layers with residual blocks!
    </span>
</div>

<p>Check out the fantastic paper <a href="https://arxiv.org/abs/1712.09913">Visualizing the Loss Landscape</a>. This is 3 years later since ResNet, and people started to realize why it worked. With the skip connections, the loss landscape is much smoother.</p>

<p>The batch norm had the same story. This reminds us <em>innovation usually comes from intuition</em>. Intuition comes first, people realize what’s going on and why it works much later.</p>

<p><img src="/images/fastai/loss-landscape.png" alt="Loss Landscape" align="middle"></p>

<p>fastai has <code class="highlighter-rouge">res_block</code>. We add a <code class="highlighter-rouge">res_block</code> after every <code class="highlighter-rouge">conv2</code> layer from previous code, we get</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">conv2</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span>
    <span class="n">res_block</span><span class="p">(</span><span class="mi">8</span><span class="p">),</span>
    <span class="n">conv2</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span>
    <span class="n">res_block</span><span class="p">(</span><span class="mi">16</span><span class="p">),</span>
    <span class="n">conv2</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span>
    <span class="n">res_block</span><span class="p">(</span><span class="mi">32</span><span class="p">),</span>
    <span class="n">conv2</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span>
    <span class="n">res_block</span><span class="p">(</span><span class="mi">16</span><span class="p">),</span>
    <span class="n">conv2</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
    <span class="n">Flatten</span><span class="p">()</span>
<span class="p">)</span>
</code></pre></div></div>

<p>Further refactoring it,</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">conv_and_res</span><span class="p">(</span><span class="n">ni</span><span class="p">,</span><span class="n">nf</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">conv2</span><span class="p">(</span><span class="n">ni</span><span class="p">,</span> <span class="n">nf</span><span class="p">),</span> <span class="n">res_block</span><span class="p">(</span><span class="n">nf</span><span class="p">))</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">conv_and_res</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span>
    <span class="n">conv_and_res</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span>
    <span class="n">conv_and_res</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span>
    <span class="n">conv_and_res</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span>
    <span class="n">conv2</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
    <span class="n">Flatten</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">learn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(),</span> <span class="n">metrics</span><span class="o">=</span><span class="n">accuracy</span><span class="p">)</span>

<span class="n">learn</span><span class="o">.</span><span class="n">lr_find</span><span class="p">(</span><span class="n">end_lr</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">learn</span><span class="o">.</span><span class="n">recorder</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>

<span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="n">max_lr</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">learn</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</code></pre></div></div>

<div class="Toast">
   <span class="Toast-icon"><svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>
   <span class="Toast-content">
        Tip: when you try out new architectures, keep refactor the code and reuse more to <b>avoid mistakes</b>.
    </span>
</div>

<p>Resnet is quite good and can reach SOTA accuracy for a lot of tasks. More modern techniques such as group convolutions don’t train as fast.</p>

<p><em>DenseNet</em> is another architecture, its only difference from Resnet is that instead of a <code class="highlighter-rouge">x + conv(conv(x))</code>, it does <code class="highlighter-rouge">concat(x, conv(conv(x)))</code> (the channel gets a little bigger). It is called a <em>DenseBlock</em> instead of ResBlock. The paper of DenseNet seems complicated but it’s really very similar to Resnet.</p>

<p>DenseNet is very memory intensive because it maintains all previous features, BUT it has much fewer parameters. <strong>It works really well for small datasets</strong>.</p>

<h2 id="u-net">
<a class="anchor" href="#u-net" aria-hidden="true"><span class="octicon octicon-link"></span></a>U-Net</h2>

<p>Use resnet34 and half-stride. What half-stride is really doing is <em>nearest-neighbor interpolation</em> or a <em>bilinear interpolation</em> with stride 1, it up samples the patch and increases the size, as shown below.</p>

<p><img src="/images/fastai/conv-upsample.png" alt="Conv UpSample" align="middle"></p>

<p>Fantastic paper for convolution: <a href="https://arxiv.org/pdf/1603.07285.pdf">A Guide to Convolution Arithmetic for Deep Learning</a></p>

<p>Nowadays we use a pretrained resnet34 as the <em>encoder</em> in U-net.</p>

<div class="Toast">
   <span class="Toast-icon"><svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>
   <span class="Toast-content">
        Trick: if you see two convs in a row, probably should use a resnet block instead. A skip connection with "+" or "concat" usually works great.
    </span>
</div>

<p>U-net came before resnet and densenet but it had a lot of the similar ideas and worked great for segmentation tasks.</p>

<div class="Toast">
   <span class="Toast-icon"><svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>
   <span class="Toast-content">
        Tip: don't use U-net for classification, because you only need the down-sampling part, not the up-sampling part. Use U-net for generative purposes such as image segmentation because the output resolution is the same as input resolution.
    </span>
</div>

<h2 id="image-restoration-with-u-net-and-gan">
<a class="anchor" href="#image-restoration-with-u-net-and-gan" aria-hidden="true"><span class="octicon octicon-link"></span></a>Image Restoration with U-Net and GAN</h2>

<p>Notebook: superres-gan</p>

<p>We use the U-net architecture to train a super-resolution model. This is a model which can increase the resolution of a low-quality image. Our model won’t only increase resolution—it will also remove jpeg artifacts, and remove unwanted text watermarks.</p>

<p>In order to make our model produce high quality results, we need to create a custom loss function which incorporates <em>feature loss (also known as perceptual loss)</em>, along with <em>gram loss</em>. These techniques can be used for many other types of image generation task, such as image colorization.</p>

<p>Traditionally, the GAN is hard to train because the initial generator and critic are bad. Fastai uses pretrained generator and critic, so they are already pretty good. After that the training of GAN is much easier.</p>

<p><img src="/images/fastai/fastai-gan.png" alt="GAN" align="middle"></p>

<p>To train a fastai version GAN, we need two folders, one with high-res original images, one with generated images.</p>

<div class="Toast">
   <span class="Toast-icon"><svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>
   <span class="Toast-content">
        Trick: to free GPU memory without restarting notebook, run the code below.
    </span>
</div>

<p>To free GPU memory without restarting notebook, run</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">my_learner</span> <span class="o">=</span> <span class="bp">None</span>
<span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
</code></pre></div></div>

<p>Running <code class="highlighter-rouge">nvidia-smi</code> won’t show it freed because pytorch has pre-allocated cache, but it’s available.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># need to wrap the loss with AdaptiveLoss for GAN to work
# Will revisit in Part II
</span><span class="n">loss_critic</span> <span class="o">=</span> <span class="n">AdaptiveLoss</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">())</span>
<span class="c1"># Use gan_critic() and not resnet here
</span><span class="k">def</span> <span class="nf">create_critic_learner</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">metrics</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">Learner</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">gan_critic</span><span class="p">(),</span> <span class="n">metrics</span><span class="o">=</span><span class="n">metrics</span><span class="p">,</span> <span class="n">loss_func</span><span class="o">=</span><span class="n">loss_critic</span><span class="p">,</span> <span class="n">wd</span><span class="o">=</span><span class="n">wd</span><span class="p">)</span>
<span class="c1"># GAN version of accuracy: accuracy_thresh_expand
</span><span class="n">learn_critic</span> <span class="o">=</span> <span class="n">create_critic_learner</span><span class="p">(</span><span class="n">data_crit</span><span class="p">,</span> <span class="n">accuracy_thresh_expand</span><span class="p">)</span>

<span class="n">learn_critic</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">)</span>
<span class="n">learn_critic</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s">'critic-pre2'</span><span class="p">)</span>
</code></pre></div></div>

<div class="Toast">
   <span class="Toast-icon"><svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>
   <span class="Toast-content">
        Tip: for GAN, fastai's <code>GANLearner</code> figures out the back and forth training of the generator and the critic for us. Use the hyperparameters like this below.
    </span>
</div>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">switcher</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">AdaptiveGANSwitcher</span><span class="p">,</span> <span class="n">critic_thresh</span><span class="o">=</span><span class="mf">0.65</span><span class="p">)</span>
<span class="n">learn</span> <span class="o">=</span> <span class="n">GANLearner</span><span class="o">.</span><span class="n">from_learners</span><span class="p">(</span>
    <span class="n">learn_gen</span><span class="p">,</span> <span class="n">learn_crit</span><span class="p">,</span>
    <span class="n">weights_gen</span><span class="o">=</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span><span class="mf">50.</span><span class="p">),</span>
    <span class="n">show_img</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">switcher</span><span class="o">=</span><span class="n">switcher</span><span class="p">,</span>
    <span class="n">opt_func</span><span class="o">=</span><span class="n">partial</span><span class="p">(</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span><span class="mf">0.99</span><span class="p">)),</span>
    <span class="n">wd</span><span class="o">=</span><span class="n">wd</span>
<span class="p">)</span>

<span class="n">learn</span><span class="o">.</span><span class="n">callback_fns</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">GANDiscriminativeLR</span><span class="p">,</span> <span class="n">mult_lr</span><span class="o">=</span><span class="mf">5.</span><span class="p">))</span>

<span class="n">lr</span> <span class="o">=</span> <span class="mf">1e-4</span>
<span class="n">learn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="mi">40</span><span class="p">,</span><span class="n">lr</span><span class="p">)</span>

<span class="c1"># NOTE: the train_loss and gen_loss should stay around the same values
# because when the generator and critic both get better, the loss is relative.
# The only way to tell how it's doing is by looking at the image results
# Use show_img=True to check
</span></code></pre></div></div>

<h3 id="wgan">
<a class="anchor" href="#wgan" aria-hidden="true"><span class="octicon octicon-link"></span></a>WGAN</h3>

<p>Notebook <code class="highlighter-rouge">wgan</code> is briefly mentioned for the task of generating image from pure noise without pretraining. Jeremy mentioned it’s a relatively old approach and the task isn’t particularly useful, but it’s good research exercise.</p>

<h3 id="perceptual-loss-feature-loss">
<a class="anchor" href="#perceptual-loss-feature-loss" aria-hidden="true"><span class="octicon octicon-link"></span></a>Perceptual Loss (Feature Loss)</h3>

<p>Notebook: <code class="highlighter-rouge">lesson7-superres</code></p>

<p>Paper: <a href="https://arxiv.org/pdf/1603.08155.pdf">Perceptual Losses for Real-Time Style Transfer and Super-Resolution</a></p>

<p>Jeremy didn’t like the name “perceptual loss” and he named it “feature loss” in fastai library.</p>

<p>Convention: in U-net shaped architecture, the down-sampling part is called <em>encoder</em> and the up-sampling part is called <em>decoder</em>.</p>

<p>The paper’s idea is to compare the generated image with the target image using a new loss function, that is, the activation from a middle layer in an ImageNet pretrained VGG network. Use the two images and pass them through this network up to that layer and check the difference. The intuition for this is that, each pixel in that activation should be capturing some feature of ImageNet images, such as furriness, round shaped, has eyeballs, etc. If the two images agree on these features they should have small loss with this loss function.</p>

<p><img src="/images/fastai/feature-loss.png" alt="Feature Loss" align="middle"></p>

<p>With 1 GPU and 1-2hr time, we can generate medium res images from low res images, or high res from medium res using this approach.</p>

<p>A fastai student Jason in 2018 cohort created the famous <a href="https://github.com/jantic/DeOldify">deOldify</a> project. He crappified color images to black and white, and trained <em>a GAN with feature loss</em> to color 19th century images!</p>

<h2 id="recap">
<a class="anchor" href="#recap" aria-hidden="true"><span class="octicon octicon-link"></span></a>Recap</h2>

<p><img src="/images/fastai/recap.png" alt="recap" align="middle"></p>

<p>Watch the videos again and go through notebooks in detail to understand better.</p>

<h2 id="recurrent-neural-network">
<a class="anchor" href="#recurrent-neural-network" aria-hidden="true"><span class="octicon octicon-link"></span></a>Recurrent Neural Network</h2>

<p>Notebook: <code class="highlighter-rouge">lesson7-human-numbers</code></p>

<p>Toy example dataset with numbers in English, the task is to predict the next word – language model.</p>

<p><code class="highlighter-rouge">xxbos</code>: beginning of stream, meaning start of document.</p>

<p><code class="highlighter-rouge">data.bptt</code>: <code class="highlighter-rouge">bptt</code> is backprop thru time</p>

<p>Basic NN with 1 hidden layer:</p>

<p><img src="/images/fastai/basic-nn.png" alt="Basic NN diagram" align="middle"></p>

<p>One step toward RNN</p>

<p><img src="/images/fastai/basic-rnn.png" alt="To RNN" align="middle"></p>

<p>Basic RNN:</p>

<p><img src="/images/fastai/basic-rnnn.png" alt="Basic RNN" align="middle"></p>

<p>Refactor, make it a loop –&gt; RNN</p>

<p><img src="/images/fastai/rnn.png" alt="RNN" align="middle"></p>

<p>There is nothing new for an RNN, it is just a fully connected NN with maintained states.</p>

<p>What GRU or LSTM is basically doing is to determine how much of the green arrow to keep and how much of the brown arrow to keep. Will see more in Course Part II.</p>

<p><img src="/images/fastai/end.png" alt="The end" align="middle"></p>

<h2 id="homework">
<a class="anchor" href="#homework" aria-hidden="true"><span class="octicon octicon-link"></span></a>Homework</h2>

<ul>
  <li>Read papers and write blog posts in plain language.</li>
  <li>Build visualizations and apps. Just finish something.</li>
</ul>

<h2 id="answers-and-comments-from-jeremy">
<a class="anchor" href="#answers-and-comments-from-jeremy" aria-hidden="true"><span class="octicon octicon-link"></span></a>Answers and Comments from Jeremy</h2>

<ul>
  <li>Reinforcement learning is far from being useful for average real world problems. Transfer learning is the most promising.</li>
  <li>If you can build the notebooks using fastai from blank, it’s already really rare and very competent in DL.</li>
</ul>


  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="logancyang/blog-learning-automata"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/note/fastai/2020/04/30/fastai-lesson7.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Notes, code and essays by Logan Yang.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/logancyang" title="logancyang"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/logancyang" title="logancyang"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
